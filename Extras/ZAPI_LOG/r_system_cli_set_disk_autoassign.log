2023-01-02 05:36:04,613| INFO|16120|6061|logger.py|132|cluster.py|3351:patch_cluster_management_network| Cluster [ArunS_2node]: cluster management network information refreshed
2023-01-02 05:36:15,914|DEBUG|16120|0|coroutine.py|177|RESTAPI [controllers.job:get_job] executing
2023-01-02 05:36:15,915|DEBUG|16120|0|coroutine.py|190|Registered Myself: current tasks [{140396627353672: <CoroutineRecord (asynchronous.cluster_tasks:refresh_cluster) [ACTIVE]>, 140396634057992: <CoroutineRecord (controllers.job:get_job) [ACTIVE]>}]
2023-01-02 05:36:15,917| INFO|16120|6069|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/jobs/57d7a728-8a5f-11ed-8a75-000c297a9d59?fields=*,last_modified,message
2023-01-02 05:36:15,918| INFO|16120|6069|restapi.py|263|GET http://127.0.0.1:8080/api/v3/jobs/57d7a728-8a5f-11ed-8a75-000c297a9d59?fields=*,last_modified,message returned in 3ms with status HTTPStatus.OK (BA)
2023-01-02 05:37:09,116|DEBUG|16120|6061|logger.py|132|ClusterRestApis.py|979:r_system_cli_set_disk_autoassign| Cluster [ArunS_2node]: "PATCH https://10.232.27.158:443/api/private/cli/storage/disk/option" response: {
  "num_records": 0,
  "error": {
    "message": "Option \"-autoassign\" is not supported on this platform.",
    "code": "721026"
  }
}
2023-01-02 05:37:17,834|ERROR|16120|6061|logger.py|132|ClusterRestApis.py|994:r_system_cli_set_disk_autoassign| Cluster [ArunS_2node]: "PATCH https://10.232.27.158:443/api/private/cli/storage/disk/option" error: 'Response' object has no attribute 'error'
2023-01-02 05:37:21,592| INFO|16120|6061|logger.py|132|event.py|134:__init__| <Event(id=801, request_id='6061', time='2023-01-02 05:37:21.585674', evtype='ClusterIdentityRefreshFailed', category='cluster', level='Error', detail='InternalErrWithInfo: ONTAP Select Deploy Internal Error: 'The REST sent to set the storage disk autoassign option has failed.'.')>
2023-01-02 05:37:21,593|DEBUG|16120|6061|job.py|364|Updated <Job (CLUSTER_REFRESH) running [id:57d7a728-8a5f-11ed-8a75-000c297a9d59 rid:6061 mod:2023-01-02 05:37:21.593221]> [failed:True] InternalErrWithInfo: ONTAP Select Deploy Internal Error: 'The REST sent to set the storage disk autoassign option has failed.'.
2023-01-02 05:37:21,765| INFO|16120|6061|logger.py|132|ZAPI.py|241:invoke|
API: "system-cli"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-02 05:37:21,766|DEBUG|16120|6061|logger.py|132|ClusterZapis.py|930:z_system_cli_version| Cluster [ArunS_2node]: System CLI, version: 2 entries were acted on.

Node: ArunS_2node-01
NetApp Release devN_221108_0200: Tue Nov  8 02:42:19 EST 2022   <1>

Node: ArunS_2node-02
NetApp Release devN_221108_0200: Tue Nov  8 02:42:19 EST 2022   <1>
2023-01-02 05:37:21,812| INFO|16120|6061|logger.py|132|ZAPI.py|241:invoke|
API: "system-get-version"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-02 05:37:21,813|DEBUG|16120|6061|logger.py|132|ClusterZapis.py|846:z_system_get_version| Cluster [ArunS_2node]: system get version {'generation': '9', 'major': '13', 'minor': '0'}
2023-01-02 05:37:21,820|DEBUG|16120|6061|fault.py|147|Unable to find a registered fault for zapi.system_node_get_iter.
2023-01-02 05:37:21,921|DEBUG|16120|6061|logger.py|132|ZAPI.py|241:invoke|
API: "system-node-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-02 05:37:21,921|DEBUG|16120|6061|logger.py|132|ClusterZapis.py|245:z_system_node_get_iter| Cluster [ArunS_2node]: got system node get iter: [[{'cpu-busytime': '63739', 'env-failed-fan-count': '0', 'env-failed-fan-message': 'There are no failed fans.', 'env-failed-power-supply-count': '0', 'env-failed-power-supply-message': 'There are no failed power supplies.', 'env-over-temperature': 'false', 'is-all-flash-optimized': 'false', 'is-all-flash-select-optimized': 'false', 'is-capacity-optimized': 'false', 'is-cloud-optimized': 'false', 'is-diff-svcs': 'false', 'is-epsilon-node': 'false', 'is-node-cluster-eligible': 'true', 'is-node-healthy': 'true', 'is-perf-optimized': 'false', 'maximum-aggregate-size': '219902325555200', 'maximum-number-of-volumes': '1000', 'maximum-volume-size': '329853488332800', 'node': 'ArunS_2node-01', 'node-location': None, 'node-model': 'FDvM300', 'node-nvram-id': '2443885398', 'node-owner': None, 'node-serial-number': '99887766554433221186', 'node-storage-configuration': 'unknown', 'node-system-id': '2443885398', 'node-uptime': '1542751', 'node-uuid': 'b2c64bbe-63cf-11ed-bfb4-000c297a9d59', 'node-vendor': 'NetApp', 'nvram-battery-status': 'battery_ok', 'product-version': 'NetApp Release Lighthouse__9.13.0: Tue Nov 08 07:42:19 UTC 2022', 'sas2-sas3-mixed-stack-support': 'none', 'vmhost-info': {'vm-uuid': '421698eb-a3ce-06c1-bee5-35bd3223040b', 'vmhost-error': 'Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the "system node virtual-machine hypervisor modify-credentials" command.', 'vmhost-hardware-vendor': 'VMware, Inc.', 'vmhost-model': 'VMware Virtual Platform', 'vmhost-software-vendor': 'NetApp'}}, {'cpu-busytime': '63820', 'env-failed-fan-count': '0', 'env-failed-fan-message': 'There are no failed fans.', 'env-failed-power-supply-count': '0', 'env-failed-power-supply-message': 'There are no failed power supplies.', 'env-over-temperature': 'false', 'is-all-flash-optimized': 'false', 'is-all-flash-select-optimized': 'false', 'is-capacity-optimized': 'false', 'is-cloud-optimized': 'false', 'is-diff-svcs': 'false', 'is-epsilon-node': 'false', 'is-node-cluster-eligible': 'true', 'is-node-healthy': 'true', 'is-perf-optimized': 'false', 'maximum-aggregate-size': '219902325555200', 'maximum-number-of-volumes': '1000', 'maximum-volume-size': '329853488332800', 'node': 'ArunS_2node-02', 'node-location': None, 'node-model': 'FDvM300', 'node-nvram-id': '2443885326', 'node-owner': None, 'node-serial-number': '99887766554433221114', 'node-storage-configuration': 'unknown', 'node-system-id': '2443885326', 'node-uptime': '1542749', 'node-uuid': 'b2c71008-63cf-11ed-bfb4-000c297a9d59', 'node-vendor': 'NetApp', 'nvram-battery-status': 'battery_ok', 'product-version': 'NetApp Release Lighthouse__9.13.0: Tue Nov 08 07:42:19 UTC 2022', 'sas2-sas3-mixed-stack-support': 'none', 'vmhost-info': {'vm-uuid': '42166123-d608-4c8e-9088-9b5e7ac95b09', 'vmhost-error': 'Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the "system node virtual-machine hypervisor modify-credentials" command.', 'vmhost-hardware-vendor': 'VMware, Inc.', 'vmhost-model': 'VMware Virtual Platform', 'vmhost-software-vendor': 'NetApp'}}]]
2023-01-02 05:37:21,925| INFO|16120|6061|logger.py|132|node.py|3481:patch_node_identity| Node [ArunS_2node-01] Cluster [ArunS_2node]: node identity refreshed
2023-01-02 05:37:21,985|DEBUG|16120|6061|logger.py|132|ZAPI.py|241:invoke|
API: "net-interface-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-02 05:37:21,987| INFO|16120|6061|logger.py|132|node.py|3387:patch_node_management_network| Node [ArunS_2node-01] Cluster [ArunS_2node]: node management network information refreshed
2023-01-02 05:37:21,989|DEBUG|16120|6061|logger.py|132|client_api_helper.py|221:get_vm_info| get_vm_info executing
2023-01-02 05:37:22,230|DEBUG|16120|6061|rest.py|173|response body: {"cpus":"4","host_id":"sdot-smicro-009.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"cluster_uuid":"b2c5f3e4-63cf-11ed-bfb4-000c297a9d59","node_uuid":"b2c64bbe-63cf-11ed-bfb4-000c297a9d59"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"ArunS_2node-01","obj_id":"vm-8356","state":"poweredon","vm_uid":"5016cfce-cb7f-9695-d0a1-3e1f94727ec1"}
2023-01-02 05:37:22,231| INFO|16120|6061|logger.py|132|node.py|3569:refresh_vm_name| Node [ArunS_2node-01] Cluster [ArunS_2node]: Node vm_name information refresh successful.
2023-01-02 05:37:22,232|DEBUG|16120|6061|logger.py|132|client_api_helper.py|221:get_vm_info| get_vm_info executing
2023-01-02 05:37:22,467|DEBUG|16120|6061|rest.py|173|response body: {"cpus":"4","host_id":"sdot-smicro-009.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"cluster_uuid":"b2c5f3e4-63cf-11ed-bfb4-000c297a9d59","node_uuid":"b2c64bbe-63cf-11ed-bfb4-000c297a9d59"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"ArunS_2node-01","obj_id":"vm-8356","state":"poweredon","vm_uid":"5016cfce-cb7f-9695-d0a1-3e1f94727ec1"}
2023-01-02 05:37:22,468| INFO|16120|6061|logger.py|132|node.py|3602:refresh_state| Node [ArunS_2node-01] Cluster [ArunS_2node]: Node state information refresh successful.
2023-01-02 05:37:22,469|DEBUG|16120|6061|logger.py|132|client_api_helper.py|558:vm_get_vm_vnics| vm_get_vm_vnics executing
2023-01-02 05:37:22,730|DEBUG|16120|6061|rest.py|173|response body: [{"mac":"00:A0:B8:CB:A9:DA","model":"VirtualVmxnet3","name":"Network adapter 1","network_name":"ONTAP-MANAGEMENT"},{"mac":"00:A0:B8:CB:CA:72","model":"VirtualVmxnet3","name":"Network adapter 2","network_name":"ONTAP-EXTERNAL"},{"mac":"02:0C:B0:00:96:C9","model":"VirtualVmxnet3","name":"Network adapter 3","network_name":"ONTAP-INTERNAL"},{"mac":"02:0C:B0:00:97:43","model":"VirtualVmxnet3","name":"Network adapter 4","network_name":"ONTAP-INTERNAL"},{"mac":"02:0C:B0:00:97:52","model":"VirtualVmxnet3","name":"Network adapter 5","network_name":"ONTAP-INTERNAL"},{"mac":"02:0C:B0:00:96:07","model":"VirtualVmxnet3","name":"Network adapter 6","network_name":"ONTAP-INTERNAL"},{"mac":"00:A0:B8:CB:16:3F","model":"VirtualVmxnet3","name":"Network adapter 7","network_name":"ONTAP-EXTERNAL"}]
2023-01-02 05:37:22,737| INFO|16120|6061|logger.py|132|node.py|3668:refresh_network_info| Node [ArunS_2node-01] Cluster [ArunS_2node]: Network information refresh successful.
2023-01-02 05:37:22,738|DEBUG|16120|6061|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-01-02 05:37:22,969|DEBUG|16120|6061|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"ArunS_2node-01.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"ArunS_2node-01_1.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"ArunS_2node-01_2.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"ArunS_2node-01_3.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"ArunS_2node-01_4.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":69632},{"capacity":51200,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"1","unit":"0"},"name":"ArunS_2node-01_5.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":51200},{"capacity":51200,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"1","unit":"0"},"name":"ArunS_2node-01_6.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":51200},{"capacity":1024,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"1","unit":"0"},"name":"ArunS_2node-01_7.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":1024}]
2023-01-02 05:37:22,980| INFO|16120|6061|logger.py|132|node.py|3808:refresh_storage_pool_info| Node [ArunS_2node-01] Cluster [ArunS_2node]: Node storage pool information refresh successful.
2023-01-02 05:37:22,982|DEBUG|16120|6061|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-01-02 05:37:23,216|DEBUG|16120|6061|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"ArunS_2node-01.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"ArunS_2node-01_1.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"ArunS_2node-01_2.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"ArunS_2node-01_3.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"ArunS_2node-01_4.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":69632},{"capacity":51200,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"1","unit":"0"},"name":"ArunS_2node-01_5.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":51200},{"capacity":51200,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"1","unit":"0"},"name":"ArunS_2node-01_6.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":51200},{"capacity":1024,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"1","unit":"0"},"name":"ArunS_2node-01_7.vmdk","pool":"sDOT-sdot-smicro-009","total_diskspace":1024}]
