2023-01-16 14:47:49,910|DEBUG| 2687|12022|rest.py|173|response body: [{"allocation":3767592,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":25058513,"status":"Online"},{"allocation":1991372,"capacity":2072320,"device_type":"SSD","location_type":"external","name":"Datastore_006","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)"],"pool_type":"VMFS-6","pool_uid":"datastore-2417","provisioned_space":1993808,"status":"Online"},{"allocation":1139009,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_06","pdisks":["NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)"],"pool_type":"VMFS-6","pool_uid":"datastore-2422","provisioned_space":1645834,"status":"Online"},{"allocation":1457,"capacity":1048320,"device_type":"SSD","location_type":"external","name":"Datastoree_100","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a47)"],"pool_type":"VMFS-6","pool_uid":"datastore-2438","provisioned_space":1457,"status":"Online"}]
2023-01-16 14:47:49,925| INFO| 2687|12022|logger.py|132|vm_create_config.py|216:set_azcs_bootarg| Not setting azcs bootarg, requirements being:                                 HW-RAID configuration, ONTAP version >= 9.4, HDD Internal storage pool
2023-01-16 14:47:49,943|DEBUG| 2687|12022|logger.py|132|vm_create_config.py|199:__init__| VM Create JSON for node cluster-unassigned-test-02 is complete
2023-01-16 14:47:49,945|DEBUG| 2687|12022|logger.py|132|vm_create_config.py|200:__init__| <resources.node.vm_create_config.VmCreateCfg object at 0x7fbcf62a4710>
2023-01-16 14:47:49,962| INFO| 2687|12022|logger.py|132|cluster_tasks.py|245:create_cluster| Cluster [cluster-unassigned-test]: Cluster config generated successfully.
2023-01-16 14:47:49,978| INFO| 2687|12022|logger.py|132|cluster_tasks.py|265:create_cluster| Cluster [cluster-unassigned-test]: creating nodes ...
2023-01-16 14:47:50,002| INFO| 2687|12022|logger.py|132|cluster_tasks.py|665:_create_nodes| Cluster [cluster-unassigned-test]: node create tasks are still running ... (completed tasks [0] of [2])
2023-01-16 14:47:50,012| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=401, request_id='12022', time='2023-01-16 14:47:50.005127', evtype='ClusterNodesCreateInProgress', category='cluster', level='Info', detail='Creating cluster nodes. This operation may take up to two hours, depending on the response time of the virtualization environment.')>
2023-01-16 14:47:50,014|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:47:50.014218]> [failed:False] Creating cluster nodes. This operation may take up to two hours, depending on the response time of the virtualization environment.
2023-01-16 14:47:50,015|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>}]
2023-01-16 14:47:50,017|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>}]
2023-01-16 14:47:50,019|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>}]
2023-01-16 14:47:50,044|DEBUG| 2687|12022|logger.py|132|node.py|1902:vm_node_create| vm create json for node cluster-unassigned-test-01: {'CpuCount': 4, 'GuestConfig': {'BootArgs': ['wafl-enable-sidl?=true', 'bootarg.template.use_node_uuids=true'], 'CLUSTER_UUID': 'e0d06a86-95ab-11ed-bd72-000c29e96265', 'CLUSTER_NAME': 'cluster-unassigned-test', 'NODE_LOCATION': '-', 'NODE_UUID': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'NODE_MGMT_IP': '10.228.160.229', 'NODE_MGMT_NETMASK': '255.255.252.0', 'NODE_MGMT_GATEWAY': '10.228.160.1', 'CPU_RESERVATION': 9576, 'BUILTIN_LICENSES': 'MPUUAKLHSOVOGDAAAAAAAAAAAAAA,SCFVEKLHSOVOGDAAAAAAAAAAAAAA,KEQYBKLHSOVOGDAAAAAAAAAAAAAA,GIHGEKLHSOVOGDAAAAAAAAAAAAAA,EXCKFKLHSOVOGDAAAAAAAAAAAAAA,UNJRDKLHSOVOGDAAAAAAAAAAAAAA,MZFNRFANXEWOGDAAAAAAAAAAAAAA,YJSJBKLHSOVOGDAAAAAAAAAAAAAA', 'SERIAL_NUMBER': '99887766554433221149', 'ADMIN_PASSWORD': 'changeme123', 'CLUSTER_PORT_MTU': 9000, 'CLUSTER_CREATE': 'TRUE', 'CLUSTER_MGMT_IP': '10.228.161.159', 'CLUSTER_MGMT_NETMASK': '255.255.252.0', 'CLUSTER_MGMT_GATEWAY': '10.228.160.1', 'NODE_COUNT': 2, 'HA_PARTNER': 'cluster-unassigned-test-02', 'HA_PARTNER_IC_MAC': '02:08:B0:00:16:BF', 'ISCSI_XMEDIATOR_IP': '10.228.161.158', 'ISCSI_XMEDIATOR_TGT': 'iqn.2012-05.local:mailbox.target.select000003', 'NODE_UUIDS': 'e0d1009a-95ab-11ed-bd72-000c29e96265,e0d228bc-95ab-11ed-bd72-000c29e96265'}, 'MemsizeMB': 16384, 'Name': 'cluster-unassigned-test-01', 'NetworkInterfaces': [{'Network': 'ONTAP_Management', 'MACAddress': '00:A0:B8:8B:06:7E'}, {'Network': 'ONTAP_External', 'MACAddress': '00:A0:B8:8B:18:AA'}, {'Network': 'ONTAP_Internal', 'MACAddress': '02:08:B0:00:16:C9'}, {'Network': 'ONTAP_Internal', 'MACAddress': '02:08:B0:00:17:00'}, {'Network': 'ONTAP_Internal', 'MACAddress': '02:08:B0:00:16:06'}, {'Network': 'ONTAP_Internal', 'MACAddress': '02:08:B0:00:17:4D'}, {'Network': 'ONTAP_External', 'MACAddress': '00:A0:B8:8B:BD:E3'}], 'ONTAPImage': '/opt/netapp/images/DataONTAPv-devN_230101_0200-vidconsole-esx.ova', 'SerialPorts': [], 'Storage': {'BootDisk': {'Pool': 'Datastore_05'}, 'PropertyISO': {'Pool': 'Datastore_05'}, 'CoreDisk': {'Pool': 'Datastore_05', 'SizeMB': 122880}, 'NVLogDisk': {'Pool': 'Datastore_05', 'SizeMB': 4096, 'Type': 'vnvme'}, 'RootDisks': [{'Pool': 'Datastore_05', 'SizeMB': 69632}, {'Pool': 'Datastore_05', 'SizeMB': 69632}]}, 'TemplateRev': 2, 'host_disks': None, 'node_disks': None}
2023-01-16 14:47:50,046|DEBUG| 2687|12022|logger.py|132|node.py|1903:vm_node_create| node cluster-unassigned-test-01 to be created and started
2023-01-16 14:47:50,063|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|319:vm_create| vm_create executing
2023-01-16 14:47:50,074|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>, 140449557918584: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_task) [ACTIVE]>}]
2023-01-16 14:47:50,097|DEBUG| 2687|12022|logger.py|132|node.py|1902:vm_node_create| vm create json for node cluster-unassigned-test-02: {'CpuCount': 4, 'GuestConfig': {'BootArgs': ['wafl-enable-sidl?=true', 'bootarg.template.use_node_uuids=true'], 'CLUSTER_UUID': 'e0d06a86-95ab-11ed-bd72-000c29e96265', 'CLUSTER_NAME': 'cluster-unassigned-test', 'NODE_LOCATION': '-', 'NODE_UUID': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'NODE_MGMT_IP': '10.228.160.231', 'NODE_MGMT_NETMASK': '255.255.252.0', 'NODE_MGMT_GATEWAY': '10.228.160.1', 'CPU_RESERVATION': 9576, 'BUILTIN_LICENSES': 'MPUUAKLHSOVOGDAAAAAAAAAAAAAA,SCFVEKLHSOVOGDAAAAAAAAAAAAAA,KEQYBKLHSOVOGDAAAAAAAAAAAAAA,GIHGEKLHSOVOGDAAAAAAAAAAAAAA,EXCKFKLHSOVOGDAAAAAAAAAAAAAA,UNJRDKLHSOVOGDAAAAAAAAAAAAAA,MZFNRFANXEWOGDAAAAAAAAAAAAAA,YJSJBKLHSOVOGDAAAAAAAAAAAAAA', 'SERIAL_NUMBER': '99887766554433221189', 'ADMIN_PASSWORD': 'changeme123', 'CLUSTER_PORT_MTU': 9000, 'CLUSTER_CREATE': 'FALSE', 'HA_PARTNER': 'cluster-unassigned-test-01', 'HA_PARTNER_IC_MAC': '02:08:B0:00:17:4D', 'ISCSI_XMEDIATOR_IP': '10.228.161.158', 'ISCSI_XMEDIATOR_TGT': 'iqn.2012-05.local:mailbox.target.select000003', 'NODE_UUIDS': 'e0d1009a-95ab-11ed-bd72-000c29e96265,e0d228bc-95ab-11ed-bd72-000c29e96265'}, 'MemsizeMB': 16384, 'Name': 'cluster-unassigned-test-02', 'NetworkInterfaces': [{'Network': 'ONTAP_Management', 'MACAddress': '00:A0:B8:8B:A3:96'}, {'Network': 'ONTAP_External', 'MACAddress': '00:A0:B8:8B:23:A4'}, {'Network': 'ONTAP_Internal', 'MACAddress': '02:08:B0:00:17:7A'}, {'Network': 'ONTAP_Internal', 'MACAddress': '02:08:B0:00:17:31'}, {'Network': 'ONTAP_Internal', 'MACAddress': '02:08:B0:00:17:62'}, {'Network': 'ONTAP_Internal', 'MACAddress': '02:08:B0:00:16:BF'}, {'Network': 'ONTAP_External', 'MACAddress': '00:A0:B8:8B:DC:B7'}], 'ONTAPImage': '/opt/netapp/images/DataONTAPv-devN_230101_0200-vidconsole-esx.ova', 'SerialPorts': [], 'Storage': {'BootDisk': {'Pool': 'Datastore_06'}, 'PropertyISO': {'Pool': 'Datastore_06'}, 'CoreDisk': {'Pool': 'Datastore_06', 'SizeMB': 122880}, 'NVLogDisk': {'Pool': 'Datastore_06', 'SizeMB': 4096, 'Type': 'vnvme'}, 'RootDisks': [{'Pool': 'Datastore_06', 'SizeMB': 69632}, {'Pool': 'Datastore_06', 'SizeMB': 69632}]}, 'TemplateRev': 2, 'host_disks': None, 'node_disks': None}
2023-01-16 14:47:50,099|DEBUG| 2687|12022|logger.py|132|node.py|1903:vm_node_create| node cluster-unassigned-test-02 to be created and started
2023-01-16 14:47:50,115|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|319:vm_create| vm_create executing
2023-01-16 14:48:10,021| INFO| 2687|12022|logger.py|132|cluster_tasks.py|665:_create_nodes| Cluster [cluster-unassigned-test]: node create tasks are still running ... (completed tasks [0] of [2])
2023-01-16 14:48:30,024| INFO| 2687|12022|logger.py|132|cluster_tasks.py|665:_create_nodes| Cluster [cluster-unassigned-test]: node create tasks are still running ... (completed tasks [0] of [2])
2023-01-16 14:48:50,032| INFO| 2687|12022|logger.py|132|cluster_tasks.py|665:_create_nodes| Cluster [cluster-unassigned-test]: node create tasks are still running ... (completed tasks [0] of [2])
2023-01-16 14:49:05,957|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-005.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d1009a-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-01","obj_id":"vm-11017","state":"poweredoff","vm_uid":"5016042c-0798-9f35-3c46-fed02c5ee585"}
2023-01-16 14:49:05,982| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=402, request_id='12022', time='2023-01-16 14:49:05.971470', evtype='NodeCreateSuccessful', category='cluster', level='Info', detail='Node "cluster-unassigned-test-01" create successful.')>
2023-01-16 14:49:05,983|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:05.983830]> [failed:False] Node "cluster-unassigned-test-01" create successful.
2023-01-16 14:49:07,308|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-006.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d228bc-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-02","obj_id":"vm-11018","state":"poweredoff","vm_uid":"5016e28c-4aed-f343-96db-8ab9e93939ac"}
2023-01-16 14:49:07,327| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=403, request_id='12022', time='2023-01-16 14:49:07.319653', evtype='NodeCreateSuccessful', category='cluster', level='Info', detail='Node "cluster-unassigned-test-02" create successful.')>
2023-01-16 14:49:07,329|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:07.329377]> [failed:False] Node "cluster-unassigned-test-02" create successful.
2023-01-16 14:49:10,047| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=404, request_id='12022', time='2023-01-16 14:49:10.036981', evtype='ClusterNodesCreateCompleted', category='cluster', level='Info', detail='All nodes in cluster "cluster-unassigned-test" were successfully created.')>
2023-01-16 14:49:10,049|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:10.049036]> [failed:False] All nodes in cluster "cluster-unassigned-test" were successfully created.
2023-01-16 14:49:10,056| INFO| 2687|12022|logger.py|132|cluster_tasks.py|272:create_cluster| Cluster [cluster-unassigned-test]: creating disks ...
2023-01-16 14:49:10,083| INFO| 2687|12022|logger.py|132|cluster_tasks.py|722:_create_disks| Creating data disks for all nodes in the cluster...
2023-01-16 14:49:10,094| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=405, request_id='12022', time='2023-01-16 14:49:10.086033', evtype='ClusterNodeDiskCreateInProgress', category='cluster', level='Info', detail='Creating data disks. This operation may take as long as two hours depending on the amount of storage to be provisioned on each node.')>
2023-01-16 14:49:10,095|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:10.095509]> [failed:False] Creating data disks. This operation may take as long as two hours depending on the amount of storage to be provisioned on each node.
2023-01-16 14:49:10,097|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>}]
2023-01-16 14:49:10,098|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>}]
2023-01-16 14:49:10,100|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>}]
2023-01-16 14:49:10,151| INFO| 2687|12022|logger.py|132|node.py|1384:validate_pool| 100 GB was requested from pool "Datastore_05".
2023-01-16 14:49:10,155|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|283:get_storage_pool| get_storage_pool executing
2023-01-16 14:49:10,159|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449557918312: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>}]
2023-01-16 14:49:10,213| INFO| 2687|12022|logger.py|132|node.py|1384:validate_pool| 100 GB was requested from pool "Datastore_06".
2023-01-16 14:49:10,216|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|283:get_storage_pool| get_storage_pool executing
2023-01-16 14:49:11,460|DEBUG| 2687|12022|rest.py|173|response body: {"allocation":1290112,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_06","pdisks":["NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)"],"pool_type":"VMFS-6","pool_uid":"datastore-2422","provisioned_space":1938701,"status":"Online","vdisks":[]}
2023-01-16 14:49:11,463|DEBUG| 2687|12022|logger.py|132|host.py|1300:is_pools_type_valid| Supported pool type ['NFS', 'VMFS-5', 'VMFS-6', 'NAS', 'vsan', 'VVOL', 'NFS41']
2023-01-16 14:49:11,464| INFO| 2687|12022|logger.py|132|host.py|1283:validate_node_pool_config| Validating host "sdot-b200-006.gdl.englab.netapp.com" storage pool "Datastore_06" against Node "cluster-unassigned-test-02" pre-deployment config. Requested Capacity 100 GB, Available Capacity 2.77 TB, Pool Overhead 81.91 GB.
2023-01-16 14:49:11,485|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|283:get_storage_pool| get_storage_pool executing
2023-01-16 14:49:11,636|DEBUG| 2687|12022|rest.py|173|response body: {"allocation":1012685,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_05","pdisks":["NETAPP Fibre Channel Disk (naa.600a098038303042542b51447a38305a)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b50636a)"],"pool_type":"VMFS-6","pool_uid":"datastore-2421","provisioned_space":1598237,"status":"Online","vdisks":[]}
2023-01-16 14:49:11,639|DEBUG| 2687|12022|logger.py|132|host.py|1300:is_pools_type_valid| Supported pool type ['NFS', 'VMFS-5', 'VMFS-6', 'NAS', 'vsan', 'VVOL', 'NFS41']
2023-01-16 14:49:11,640| INFO| 2687|12022|logger.py|132|host.py|1283:validate_node_pool_config| Validating host "sdot-b200-005.gdl.englab.netapp.com" storage pool "Datastore_05" against Node "cluster-unassigned-test-01" pre-deployment config. Requested Capacity 100 GB, Available Capacity 3.03 TB, Pool Overhead 81.91 GB.
2023-01-16 14:49:11,660|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|283:get_storage_pool| get_storage_pool executing
2023-01-16 14:49:12,347|DEBUG| 2687|12022|rest.py|173|response body: {"allocation":1290112,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_06","pdisks":["NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)"],"pool_type":"VMFS-6","pool_uid":"datastore-2422","provisioned_space":1938701,"status":"Online","vdisks":[]}
2023-01-16 14:49:12,353|DEBUG| 2687|12022|logger.py|132|node.py|2169:get_max_disk_size_supported| Maximum disk size supported: 16777216
2023-01-16 14:49:12,365|DEBUG| 2687|12022|logger.py|132|node.py|2011:_get_disk_configs| Total available space for data disks creation is: 102400.0
2023-01-16 14:49:12,368| INFO| 2687|12022|logger.py|132|node.py|2238:vm_create_disks| Data disks set [{'pool': 'Datastore_06', 'capacity': 51200}, {'pool': 'Datastore_06', 'capacity': 51200}] for node [cluster-unassigned-test-02] in cluster [cluster-unassigned-test]
2023-01-16 14:49:12,369|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|262:get_host_pdisks| get_host_pdisks executing
2023-01-16 14:49:12,571|DEBUG| 2687|12022|rest.py|173|response body: {"allocation":1012685,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_05","pdisks":["NETAPP Fibre Channel Disk (naa.600a098038303042542b51447a38305a)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b50636a)"],"pool_type":"VMFS-6","pool_uid":"datastore-2421","provisioned_space":1598237,"status":"Online","vdisks":[]}
2023-01-16 14:49:12,578|DEBUG| 2687|12022|logger.py|132|node.py|2169:get_max_disk_size_supported| Maximum disk size supported: 16777216
2023-01-16 14:49:12,592|DEBUG| 2687|12022|logger.py|132|node.py|2011:_get_disk_configs| Total available space for data disks creation is: 102400.0
2023-01-16 14:49:12,593| INFO| 2687|12022|logger.py|132|node.py|2238:vm_create_disks| Data disks set [{'pool': 'Datastore_05', 'capacity': 51200}, {'pool': 'Datastore_05', 'capacity': 51200}] for node [cluster-unassigned-test-01] in cluster [cluster-unassigned-test]
2023-01-16 14:49:12,595|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|262:get_host_pdisks| get_host_pdisks executing
2023-01-16 14:49:15,102|DEBUG| 2687|12022|rest.py|173|response body: [{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b75","total_diskspace":1048593,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a47","total_diskspace":1048593,"used_by":"Datastoree_100"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656932","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a0980383036554e2b514565396e70","total_diskspace":3145728,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38302f","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}]
2023-01-16 14:49:15,106| INFO| 2687|12022|logger.py|132|node.py|2241:vm_create_disks| host_pdisks_info {'naa.600a09805176304d6d5d4b384b506b75': {'id': 'naa.600a09805176304d6d5d4b384b506b75', 'name': 'naa.600a09805176304d6d5d4b384b506b75', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_06', 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724e7a46': {'id': 'naa.600a098051763039375d4b39724e7a46', 'name': 'naa.600a098051763039375d4b39724e7a46', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_006', 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724e7a47': {'id': 'naa.600a098051763039375d4b39724e7a47', 'name': 'naa.600a098051763039375d4b39724e7a47', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastoree_100', 'passthrough_type': 'rdm'}, 'naa.600a098051763671763f4b395a656932': {'id': 'naa.600a098051763671763f4b395a656932', 'name': 'naa.600a098051763671763f4b395a656932', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 20480, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3545': {'id': 'naa.600a098051763039375d4b39724f3545', 'name': 'naa.600a098051763039375d4b39724f3545', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3547': {'id': 'naa.600a098051763039375d4b39724f3547', 'name': 'naa.600a098051763039375d4b39724f3547', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3548': {'id': 'naa.600a098051763039375d4b39724f3548', 'name': 'naa.600a098051763039375d4b39724f3548', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f354a': {'id': 'naa.600a098051763039375d4b39724f354a', 'name': 'naa.600a098051763039375d4b39724f354a', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a0980383036554e2b514565396e70': {'id': 'naa.600a0980383036554e2b514565396e70', 'name': 'naa.600a0980383036554e2b514565396e70', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 3145728, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_06', 'passthrough_type': 'rdm'}, 'naa.600a098038303042542b51447a38302f': {'id': 'naa.600a098038303042542b51447a38302f', 'name': 'naa.600a098038303042542b51447a38302f', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 10240, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f352d': {'id': 'naa.600a098051763039375d4b39724f352d', 'name': 'naa.600a098051763039375d4b39724f352d', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}} for node [cluster-unassigned-test-02] in cluster [cluster-unassigned-test]
2023-01-16 14:49:15,110|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|137:is_vm_on_host| is_vm_on_host executing
2023-01-16 14:49:15,112|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|173:vm_get| vm_get executing
2023-01-16 14:49:15,268|DEBUG| 2687|12022|rest.py|173|response body: [{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b74","total_diskspace":1048593,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656931","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b50636a","total_diskspace":1048593,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a383059","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38305a","total_diskspace":3145728,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}]
2023-01-16 14:49:15,271| INFO| 2687|12022|logger.py|132|node.py|2241:vm_create_disks| host_pdisks_info {'naa.600a09805176304d6d5d4b384b506b74': {'id': 'naa.600a09805176304d6d5d4b384b506b74', 'name': 'naa.600a09805176304d6d5d4b384b506b74', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724e7a46': {'id': 'naa.600a098051763039375d4b39724e7a46', 'name': 'naa.600a098051763039375d4b39724e7a46', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_006', 'passthrough_type': 'rdm'}, 'naa.600a098051763671763f4b395a656931': {'id': 'naa.600a098051763671763f4b395a656931', 'name': 'naa.600a098051763671763f4b395a656931', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 20480, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3545': {'id': 'naa.600a098051763039375d4b39724f3545', 'name': 'naa.600a098051763039375d4b39724f3545', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3547': {'id': 'naa.600a098051763039375d4b39724f3547', 'name': 'naa.600a098051763039375d4b39724f3547', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3548': {'id': 'naa.600a098051763039375d4b39724f3548', 'name': 'naa.600a098051763039375d4b39724f3548', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f354a': {'id': 'naa.600a098051763039375d4b39724f354a', 'name': 'naa.600a098051763039375d4b39724f354a', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a09805176304d6d5d4b384b50636a': {'id': 'naa.600a09805176304d6d5d4b384b50636a', 'name': 'naa.600a09805176304d6d5d4b384b50636a', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_05', 'passthrough_type': 'rdm'}, 'naa.600a098038303042542b51447a383059': {'id': 'naa.600a098038303042542b51447a383059', 'name': 'naa.600a098038303042542b51447a383059', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 10240, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098038303042542b51447a38305a': {'id': 'naa.600a098038303042542b51447a38305a', 'name': 'naa.600a098038303042542b51447a38305a', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 3145728, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_05', 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f352d': {'id': 'naa.600a098051763039375d4b39724f352d', 'name': 'naa.600a098051763039375d4b39724f352d', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}} for node [cluster-unassigned-test-01] in cluster [cluster-unassigned-test]
2023-01-16 14:49:15,274|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|137:is_vm_on_host| is_vm_on_host executing
2023-01-16 14:49:15,275|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|173:vm_get| vm_get executing
2023-01-16 14:49:15,399|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-006.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d228bc-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-02","obj_id":"vm-11018","state":"poweredoff","vm_uid":"5016e28c-4aed-f343-96db-8ab9e93939ac"}
2023-01-16 14:49:15,401|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|221:get_vm_info| get_vm_info executing
2023-01-16 14:49:15,559|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-005.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d1009a-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-01","obj_id":"vm-11017","state":"poweredoff","vm_uid":"5016042c-0798-9f35-3c46-fed02c5ee585"}
2023-01-16 14:49:15,562|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|221:get_vm_info| get_vm_info executing
2023-01-16 14:49:15,676|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-006.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d228bc-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-02","obj_id":"vm-11018","state":"poweredoff","vm_uid":"5016e28c-4aed-f343-96db-8ab9e93939ac"}
2023-01-16 14:49:15,678|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-01-16 14:49:15,819|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-005.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d1009a-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-01","obj_id":"vm-11017","state":"poweredoff","vm_uid":"5016042c-0798-9f35-3c46-fed02c5ee585"}
2023-01-16 14:49:15,822|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-01-16 14:49:15,924|DEBUG| 2687|12022|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02.vmdk","pool":"Datastore_06","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02_1.vmdk","pool":"Datastore_06","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"cluster-unassigned-test-02_2.vmdk","pool":"Datastore_06","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02_3.vmdk","pool":"Datastore_06","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02_4.vmdk","pool":"Datastore_06","total_diskspace":69632}]
2023-01-16 14:49:15,933| INFO| 2687|12022|logger.py|132|node.py|2246:vm_create_disks| Starting to create data disks for node [cluster-unassigned-test-02] in cluster [cluster-unassigned-test]
2023-01-16 14:49:15,945| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=406, request_id='12022', time='2023-01-16 14:49:15.937038', evtype='CreatingDataDisks', category='node', level='Info', detail='Creating data disks for node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:15,946|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:15.946374]> [failed:False] Creating data disks for node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".
2023-01-16 14:49:15,948|DEBUG| 2687|12022|logger.py|132|node.py|2253:vm_create_disks| Creating data disk No: 1, disk {'pool': 'Datastore_06', 'capacity': 51200} on host (sdot-b200-006.gdl.englab.netapp.com)
2023-01-16 14:49:15,949|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|379:vm_create_vdisk| vm_create_vdisk executing
2023-01-16 14:49:16,036|DEBUG| 2687|12022|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01.vmdk","pool":"Datastore_05","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01_1.vmdk","pool":"Datastore_05","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"cluster-unassigned-test-01_2.vmdk","pool":"Datastore_05","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01_3.vmdk","pool":"Datastore_05","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01_4.vmdk","pool":"Datastore_05","total_diskspace":69632}]
2023-01-16 14:49:16,044| INFO| 2687|12022|logger.py|132|node.py|2246:vm_create_disks| Starting to create data disks for node [cluster-unassigned-test-01] in cluster [cluster-unassigned-test]
2023-01-16 14:49:16,054| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=407, request_id='12022', time='2023-01-16 14:49:16.045832', evtype='CreatingDataDisks', category='node', level='Info', detail='Creating data disks for node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:16,055|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:16.054924]> [failed:False] Creating data disks for node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".
2023-01-16 14:49:16,055|DEBUG| 2687|12022|logger.py|132|node.py|2253:vm_create_disks| Creating data disk No: 1, disk {'pool': 'Datastore_05', 'capacity': 51200} on host (sdot-b200-005.gdl.englab.netapp.com)
2023-01-16 14:49:16,056|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|379:vm_create_vdisk| vm_create_vdisk executing
2023-01-16 14:49:18,021|DEBUG| 2687|12022|rest.py|173|response body: {"capacity":51200,"name":"cluster-unassigned-test-02_5.vmdk","pool":"Datastore_06"}
2023-01-16 14:49:18,081| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=408, request_id='12022', time='2023-01-16 14:49:18.073890', evtype='NodeDataDiskAdded', category='node', level='Info', detail='Data disk 1 (size: 51200MB) was created and added to node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:18,082|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:18.082741]> [failed:False] Data disk 1 (size: 51200MB) was created and added to node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".
2023-01-16 14:49:18,087|DEBUG| 2687|12022|logger.py|132|node.py|2253:vm_create_disks| Creating data disk No: 2, disk {'pool': 'Datastore_06', 'capacity': 51200} on host (sdot-b200-006.gdl.englab.netapp.com)
2023-01-16 14:49:18,090|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|379:vm_create_vdisk| vm_create_vdisk executing
2023-01-16 14:49:18,121|DEBUG| 2687|12022|rest.py|173|response body: {"capacity":51200,"name":"cluster-unassigned-test-01_5.vmdk","pool":"Datastore_05"}
2023-01-16 14:49:18,148| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=409, request_id='12022', time='2023-01-16 14:49:18.139837', evtype='NodeDataDiskAdded', category='node', level='Info', detail='Data disk 1 (size: 51200MB) was created and added to node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:18,149|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:18.149347]> [failed:False] Data disk 1 (size: 51200MB) was created and added to node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".
2023-01-16 14:49:18,153|DEBUG| 2687|12022|logger.py|132|node.py|2253:vm_create_disks| Creating data disk No: 2, disk {'pool': 'Datastore_05', 'capacity': 51200} on host (sdot-b200-005.gdl.englab.netapp.com)
2023-01-16 14:49:18,156|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|379:vm_create_vdisk| vm_create_vdisk executing
2023-01-16 14:49:19,810|DEBUG| 2687|12022|rest.py|173|response body: {"capacity":51200,"name":"cluster-unassigned-test-02_6.vmdk","pool":"Datastore_06"}
2023-01-16 14:49:19,836| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=410, request_id='12022', time='2023-01-16 14:49:19.828691', evtype='NodeDataDiskAdded', category='node', level='Info', detail='Data disk 2 (size: 51200MB) was created and added to node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:19,837|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:19.837461]> [failed:False] Data disk 2 (size: 51200MB) was created and added to node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".
2023-01-16 14:49:19,838|DEBUG| 2687|12022|logger.py|132|node.py|2283:vm_create_disks| New data disks in node (cluster-unassigned-test-02): []
2023-01-16 14:49:19,848| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=411, request_id='12022', time='2023-01-16 14:49:19.840385', evtype='NodeDiskCreateSuccessful', category='cluster', level='Info', detail='Data disks on node "cluster-unassigned-test-02" created successfully.')>
2023-01-16 14:49:19,848|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:19.848778]> [failed:False] Data disks on node "cluster-unassigned-test-02" created successfully.
2023-01-16 14:49:19,895|DEBUG| 2687|12022|rest.py|173|response body: {"capacity":51200,"name":"cluster-unassigned-test-01_6.vmdk","pool":"Datastore_05"}
2023-01-16 14:49:19,919| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=412, request_id='12022', time='2023-01-16 14:49:19.912444', evtype='NodeDataDiskAdded', category='node', level='Info', detail='Data disk 2 (size: 51200MB) was created and added to node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:19,920|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:19.920206]> [failed:False] Data disk 2 (size: 51200MB) was created and added to node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".
2023-01-16 14:49:19,921|DEBUG| 2687|12022|logger.py|132|node.py|2283:vm_create_disks| New data disks in node (cluster-unassigned-test-01): []
2023-01-16 14:49:19,930| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=413, request_id='12022', time='2023-01-16 14:49:19.922692', evtype='NodeDiskCreateSuccessful', category='cluster', level='Info', detail='Data disks on node "cluster-unassigned-test-01" created successfully.')>
2023-01-16 14:49:19,930|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:19.930751]> [failed:False] Data disks on node "cluster-unassigned-test-01" created successfully.
2023-01-16 14:49:30,118| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=414, request_id='12022', time='2023-01-16 14:49:30.106088', evtype='ClusterNodesDataDiskCreateCompleted', category='cluster', level='Info', detail='Data disks were successfully created and added to all nodes in the cluster "cluster-unassigned-test".')>
2023-01-16 14:49:30,119|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:30.119783]> [failed:False] Data disks were successfully created and added to all nodes in the cluster "cluster-unassigned-test".
2023-01-16 14:49:30,120| INFO| 2687|12022|logger.py|132|cluster_tasks.py|277:create_cluster| Cluster [cluster-unassigned-test]: starting nodes ...
2023-01-16 14:49:30,131| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=415, request_id='12022', time='2023-01-16 14:49:30.122698', evtype='NodesPowerOnInProgress', category='cluster', level='Info', detail='All nodes in cluster "cluster-unassigned-test" are being powered on.')>
2023-01-16 14:49:30,131|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:30.131875]> [failed:False] All nodes in cluster "cluster-unassigned-test" are being powered on.
2023-01-16 14:49:30,132| INFO| 2687|12022|logger.py|132|cluster_tasks.py|585:_start_nodes| Cluster [cluster-unassigned-test]: starting node cluster-unassigned-test-02
2023-01-16 14:49:30,133| INFO| 2687|12022|logger.py|132|cluster_tasks.py|591:_start_nodes| Starting nodes for all nodes in the cluster...
2023-01-16 14:49:30,134|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_start_task) [ACTIVE]>}]
2023-01-16 14:49:30,135|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_start_task) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.cluster_tasks:vm_start_task) [ACTIVE]>}]
2023-01-16 14:49:30,144|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|346:vm_start| vm_start executing
2023-01-16 14:49:34,838|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-006.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d228bc-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-02","obj_id":"vm-11018","state":"poweredon","vm_uid":"5016e28c-4aed-f343-96db-8ab9e93939ac"}
2023-01-16 14:49:34,940| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=416, request_id='12022', time='2023-01-16 14:49:34.932554', evtype='NodePoweredOn', category='cluster', level='Info', detail='Node "cluster-unassigned-test-02" is successfully powered on.')>
2023-01-16 14:49:34,941|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:34.940897]> [failed:False] Node "cluster-unassigned-test-02" is successfully powered on.
2023-01-16 14:49:34,947| INFO| 2687|12022|logger.py|132|cluster_tasks.py|602:_start_nodes| Cluster [cluster-unassigned-test]: waiting a bit (60 secs) before starting cluster create node ...
2023-01-16 14:50:35,010| INFO| 2687|12022|logger.py|132|cluster_tasks.py|605:_start_nodes| Cluster [cluster-unassigned-test]: starting node cluster-unassigned-test-01
2023-01-16 14:50:35,023|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|346:vm_start| vm_start executing
2023-01-16 14:50:35,060|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:50:35,061|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564633560: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:50:35,069| INFO| 2687|12034|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:50:35,666| INFO| 2687|12034|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 604ms with status HTTPStatus.OK (BA)
2023-01-16 14:50:40,999|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-005.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d1009a-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-01","obj_id":"vm-11017","state":"poweredon","vm_uid":"5016042c-0798-9f35-3c46-fed02c5ee585"}
2023-01-16 14:50:41,002|DEBUG| 2687|12022|logger.py|132|host.py|765:vm_start| vm_start response: {'host_id': 'sdot-b200-005.gdl.englab.netapp.com',
 'hw_version': 'vmx-14',
 'metadata': {'cluster_uuid': 'e0d06a86-95ab-11ed-bd72-000c29e96265',
              'node_uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265'},
 'mgmt_server_uid': '3025b4b7-b510-4852-98ed-38668b194b85',
 'name': 'cluster-unassigned-test-01',
 'obj_id': 'vm-11017',
 'state': 'poweredon',
 'vm_uid': '5016042c-0798-9f35-3c46-fed02c5ee585'}
2023-01-16 14:50:41,024| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=417, request_id='12022', time='2023-01-16 14:50:41.016512', evtype='AllNodesPoweredOn', category='cluster', level='Info', detail='All nodes in cluster "cluster-unassigned-test" are successfully powered on.')>
2023-01-16 14:50:41,025|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:50:41.024942]> [failed:False] All nodes in cluster "cluster-unassigned-test" are successfully powered on.
2023-01-16 14:50:41,025| INFO| 2687|12022|logger.py|132|cluster_tasks.py|280:create_cluster| Cluster [cluster-unassigned-test]: all nodes started, starting cluster deploy checks ...
2023-01-16 14:50:41,056| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=418, request_id='12022', time='2023-01-16 14:50:41.048934', evtype='NodeIpsPingableCheck', category='cluster', level='Info', detail='Checking if all nodes in the cluster are responding to ping requests.')>
2023-01-16 14:50:41,057|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:50:41.057130]> [failed:False] Checking if all nodes in the cluster are responding to ping requests.
2023-01-16 14:50:43,095|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:05,138|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:27,178|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:47,213|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.229
2023-01-16 14:51:47,223|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:47,242| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=419, request_id='12022', time='2023-01-16 14:51:47.226687', evtype='IpPingStatus', category='cluster', level='Info', detail='Ping succeeded on IPs: ['10.228.160.231', '10.228.160.229']. Waiting for the following IPs: [].')>
2023-01-16 14:51:47,243|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:51:47.243652]> [failed:False] Ping succeeded on IPs: ['10.228.160.231', '10.228.160.229']. Waiting for the following IPs: [].
2023-01-16 14:51:47,244| INFO| 2687|12022|logger.py|132|cluster.py|1868:check_cluster_status| Cluster [cluster-unassigned-test]: all nodes pingable!
2023-01-16 14:51:47,256| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=420, request_id='12022', time='2023-01-16 14:51:47.246407', evtype='AllNodeMgmtIpsPingable', category='cluster', level='Info', detail='All nodes in the cluster are responding to ping requests.')>
2023-01-16 14:51:47,257|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:51:47.257656]> [failed:False] All nodes in the cluster are responding to ping requests.
2023-01-16 14:52:17,316|WARNING| 2687|12022|logger.py|132|ZAPI.py|79:retry| Operation: invoke. Retries remaining: 178. Result: ConnectionRefusedError(111, 'Connection refused').
2023-01-16 14:52:37,339| INFO| 2687|12022|logger.py|132|ZAPI.py|116:retry| Retry invoke for operation invoke. Remaining retries: 178.
2023-01-16 14:52:37,342|WARNING| 2687|12022|logger.py|132|ZAPI.py|79:retry| Operation: invoke. Retries remaining: 177. Result: ConnectionRefusedError(111, 'Connection refused').
2023-01-16 14:52:57,365| INFO| 2687|12022|logger.py|132|ZAPI.py|116:retry| Retry invoke for operation invoke. Remaining retries: 177.
2023-01-16 14:52:57,381|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:52:57,382|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:52:57,389| INFO| 2687|12035|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:52:57,988| INFO| 2687|12035|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 606ms with status HTTPStatus.OK (BA)
2023-01-16 14:52:58,427|WARNING| 2687|12022|logger.py|132|ZAPI.py|79:retry| Operation: invoke. Retries remaining: 176. Result: ConnectionRefusedError(111, 'Connection refused').
2023-01-16 14:53:18,446| INFO| 2687|12022|logger.py|132|ZAPI.py|116:retry| Retry invoke for operation invoke. Remaining retries: 176.
2023-01-16 14:53:19,836|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:53:19,837|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Creating cluster', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:53:19,838| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Creating cluster
2023-01-16 14:53:39,855| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=421, request_id='12022', time='2023-01-16 14:53:39.841939', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Creating cluster].')>
2023-01-16 14:53:39,856|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:53:39.856233]> [failed:False] ONTAP cluster create in progress. Status: [Creating cluster].
2023-01-16 14:53:39,937|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:53:39,938|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Vserver Management', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:53:39,939| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Vserver Management
2023-01-16 14:53:59,983| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=422, request_id='12022', time='2023-01-16 14:53:59.962228', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Vserver Management].')>
2023-01-16 14:53:59,984|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:53:59.984693]> [failed:False] ONTAP cluster create in progress. Status: [Vserver Management].
2023-01-16 14:54:00,027|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:54:00,029|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Non shared cluster setup', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:54:00,031| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Non shared cluster setup
2023-01-16 14:54:20,068| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=423, request_id='12022', time='2023-01-16 14:54:20.055061', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Non shared cluster setup].')>
2023-01-16 14:54:20,069|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:54:20.069449]> [failed:False] ONTAP cluster create in progress. Status: [Non shared cluster setup].
2023-01-16 14:54:20,145|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:54:20,147|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Starting cluster support services', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:54:20,148| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Starting cluster support services
2023-01-16 14:54:40,166| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=424, request_id='12022', time='2023-01-16 14:54:40.157812', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Starting cluster support services].')>
2023-01-16 14:54:40,167|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:54:40.167536]> [failed:False] ONTAP cluster create in progress. Status: [Starting cluster support services].
2023-01-16 14:54:40,208|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:54:40,209|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Starting cluster support services', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:54:40,210| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Starting cluster support services
2023-01-16 14:55:00,291|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:00,292|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Cluster has been created.', 'is-complete': 'true', 'status': 'success'}]
2023-01-16 14:55:00,293| INFO| 2687|12022|logger.py|132|cluster.py|2090:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster create finished!
2023-01-16 14:55:00,334|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:00,335|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'failure-msg': None, 'job-id': '16', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'node_discovery'}]]
2023-01-16 14:55:00,336| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, Unknown: node_discovery
2023-01-16 14:55:00,345| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=425, request_id='12022', time='2023-01-16 14:55:00.337831', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, Unknown: node_discovery].')>
2023-01-16 14:55:00,346|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:55:00.346261]> [failed:False] ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, Unknown: node_discovery].
2023-01-16 14:55:20,412|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:20,413|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:55:20,414| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:55:20,424| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=426, request_id='12022', time='2023-01-16 14:55:20.416059', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup].')>
2023-01-16 14:55:20,425|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:55:20.425043]> [failed:False] ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup].
2023-01-16 14:55:40,470|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:55:40,471|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:55:40,476| INFO| 2687|12036|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:55:41,096| INFO| 2687|12036|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 624ms with status HTTPStatus.OK (BA)
2023-01-16 14:55:41,606|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:41,608|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:55:41,610| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:56:01,695|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:56:01,696|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:56:01,697| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:56:21,768|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:56:21,769|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:56:21,770| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:56:41,817|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:56:41,818|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:56:41,819| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:57:01,878|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:01,879|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:57:01,880| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:57:21,945|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:21,946|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'success'}]]
2023-01-16 14:57:21,947| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: success
2023-01-16 14:57:21,957| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=427, request_id='12022', time='2023-01-16 14:57:21.948687', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: success].')>
2023-01-16 14:57:21,958|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.958435]> [failed:False] ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: success].
2023-01-16 14:57:21,959| INFO| 2687|12022|logger.py|132|cluster.py|2150:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes complete
2023-01-16 14:57:21,969|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.161.159
2023-01-16 14:57:21,979| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=428, request_id='12022', time='2023-01-16 14:57:21.971790', evtype='IpPingStatus', category='cluster', level='Info', detail='Ping succeeded on IPs: ['10.228.161.159']. Waiting for the following IPs: [].')>
2023-01-16 14:57:21,980|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.980621]> [failed:False] Ping succeeded on IPs: ['10.228.161.159']. Waiting for the following IPs: [].
2023-01-16 14:57:21,981| INFO| 2687|12022|logger.py|132|cluster.py|1889:check_cluster_status| Cluster [cluster-unassigned-test]: cluster ip 10.228.161.159 is pingable!
2023-01-16 14:57:21,990| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=429, request_id='12022', time='2023-01-16 14:57:21.983041', evtype='ClusterMgmtIpPingable', category='cluster', level='Info', detail='Cluster is responding to ping requests on the cluster management IP address.')>
2023-01-16 14:57:21,990|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.990701]> [failed:False] Cluster is responding to ping requests on the cluster management IP address.
2023-01-16 14:57:21,991| INFO| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.991364]> [failed:False] Waiting for aggregates to resync.
2023-01-16 14:57:22,188|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:22,189| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:57:32,289|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:42,388|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:52,502|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:52,504| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:58:02,601|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:12,720|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:22,843|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:22,847| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:58:32,942|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:43,045|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:53,075|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:58:53,078|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:58:53,081| INFO| 2687|12037|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:58:53,806| INFO| 2687|12037|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 728ms with status HTTPStatus.OK (BA)
2023-01-16 14:58:54,388|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
2023-01-16 14:49:10,097|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>}]
2023-01-16 14:49:10,098|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>}]
2023-01-16 14:49:10,100|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>}]
2023-01-16 14:49:10,151| INFO| 2687|12022|logger.py|132|node.py|1384:validate_pool| 100 GB was requested from pool "Datastore_05".
2023-01-16 14:49:10,155|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|283:get_storage_pool| get_storage_pool executing
2023-01-16 14:49:10,159|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449564634104: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>, 140449557918312: <CoroutineRecord (asynchronous.cluster_tasks:vm_create_disk_task) [ACTIVE]>}]
2023-01-16 14:49:10,213| INFO| 2687|12022|logger.py|132|node.py|1384:validate_pool| 100 GB was requested from pool "Datastore_06".
2023-01-16 14:49:10,216|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|283:get_storage_pool| get_storage_pool executing
2023-01-16 14:49:11,460|DEBUG| 2687|12022|rest.py|173|response body: {"allocation":1290112,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_06","pdisks":["NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)"],"pool_type":"VMFS-6","pool_uid":"datastore-2422","provisioned_space":1938701,"status":"Online","vdisks":[]}
2023-01-16 14:49:11,463|DEBUG| 2687|12022|logger.py|132|host.py|1300:is_pools_type_valid| Supported pool type ['NFS', 'VMFS-5', 'VMFS-6', 'NAS', 'vsan', 'VVOL', 'NFS41']
2023-01-16 14:49:11,464| INFO| 2687|12022|logger.py|132|host.py|1283:validate_node_pool_config| Validating host "sdot-b200-006.gdl.englab.netapp.com" storage pool "Datastore_06" against Node "cluster-unassigned-test-02" pre-deployment config. Requested Capacity 100 GB, Available Capacity 2.77 TB, Pool Overhead 81.91 GB.
2023-01-16 14:49:11,485|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|283:get_storage_pool| get_storage_pool executing
2023-01-16 14:49:11,636|DEBUG| 2687|12022|rest.py|173|response body: {"allocation":1012685,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_05","pdisks":["NETAPP Fibre Channel Disk (naa.600a098038303042542b51447a38305a)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b50636a)"],"pool_type":"VMFS-6","pool_uid":"datastore-2421","provisioned_space":1598237,"status":"Online","vdisks":[]}
2023-01-16 14:49:11,639|DEBUG| 2687|12022|logger.py|132|host.py|1300:is_pools_type_valid| Supported pool type ['NFS', 'VMFS-5', 'VMFS-6', 'NAS', 'vsan', 'VVOL', 'NFS41']
2023-01-16 14:49:11,640| INFO| 2687|12022|logger.py|132|host.py|1283:validate_node_pool_config| Validating host "sdot-b200-005.gdl.englab.netapp.com" storage pool "Datastore_05" against Node "cluster-unassigned-test-01" pre-deployment config. Requested Capacity 100 GB, Available Capacity 3.03 TB, Pool Overhead 81.91 GB.
2023-01-16 14:49:11,660|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|283:get_storage_pool| get_storage_pool executing
2023-01-16 14:49:12,347|DEBUG| 2687|12022|rest.py|173|response body: {"allocation":1290112,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_06","pdisks":["NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)"],"pool_type":"VMFS-6","pool_uid":"datastore-2422","provisioned_space":1938701,"status":"Online","vdisks":[]}
2023-01-16 14:49:12,353|DEBUG| 2687|12022|logger.py|132|node.py|2169:get_max_disk_size_supported| Maximum disk size supported: 16777216
2023-01-16 14:49:12,365|DEBUG| 2687|12022|logger.py|132|node.py|2011:_get_disk_configs| Total available space for data disks creation is: 102400.0
2023-01-16 14:49:12,368| INFO| 2687|12022|logger.py|132|node.py|2238:vm_create_disks| Data disks set [{'pool': 'Datastore_06', 'capacity': 51200}, {'pool': 'Datastore_06', 'capacity': 51200}] for node [cluster-unassigned-test-02] in cluster [cluster-unassigned-test]
2023-01-16 14:49:12,369|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|262:get_host_pdisks| get_host_pdisks executing
2023-01-16 14:49:12,571|DEBUG| 2687|12022|rest.py|173|response body: {"allocation":1012685,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_05","pdisks":["NETAPP Fibre Channel Disk (naa.600a098038303042542b51447a38305a)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b50636a)"],"pool_type":"VMFS-6","pool_uid":"datastore-2421","provisioned_space":1598237,"status":"Online","vdisks":[]}
2023-01-16 14:49:12,578|DEBUG| 2687|12022|logger.py|132|node.py|2169:get_max_disk_size_supported| Maximum disk size supported: 16777216
2023-01-16 14:49:12,592|DEBUG| 2687|12022|logger.py|132|node.py|2011:_get_disk_configs| Total available space for data disks creation is: 102400.0
2023-01-16 14:49:12,593| INFO| 2687|12022|logger.py|132|node.py|2238:vm_create_disks| Data disks set [{'pool': 'Datastore_05', 'capacity': 51200}, {'pool': 'Datastore_05', 'capacity': 51200}] for node [cluster-unassigned-test-01] in cluster [cluster-unassigned-test]
2023-01-16 14:49:12,595|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|262:get_host_pdisks| get_host_pdisks executing
2023-01-16 14:49:15,102|DEBUG| 2687|12022|rest.py|173|response body: [{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b75","total_diskspace":1048593,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a47","total_diskspace":1048593,"used_by":"Datastoree_100"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656932","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a0980383036554e2b514565396e70","total_diskspace":3145728,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38302f","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}]
2023-01-16 14:49:15,106| INFO| 2687|12022|logger.py|132|node.py|2241:vm_create_disks| host_pdisks_info {'naa.600a09805176304d6d5d4b384b506b75': {'id': 'naa.600a09805176304d6d5d4b384b506b75', 'name': 'naa.600a09805176304d6d5d4b384b506b75', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_06', 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724e7a46': {'id': 'naa.600a098051763039375d4b39724e7a46', 'name': 'naa.600a098051763039375d4b39724e7a46', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_006', 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724e7a47': {'id': 'naa.600a098051763039375d4b39724e7a47', 'name': 'naa.600a098051763039375d4b39724e7a47', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastoree_100', 'passthrough_type': 'rdm'}, 'naa.600a098051763671763f4b395a656932': {'id': 'naa.600a098051763671763f4b395a656932', 'name': 'naa.600a098051763671763f4b395a656932', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 20480, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3545': {'id': 'naa.600a098051763039375d4b39724f3545', 'name': 'naa.600a098051763039375d4b39724f3545', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3547': {'id': 'naa.600a098051763039375d4b39724f3547', 'name': 'naa.600a098051763039375d4b39724f3547', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3548': {'id': 'naa.600a098051763039375d4b39724f3548', 'name': 'naa.600a098051763039375d4b39724f3548', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f354a': {'id': 'naa.600a098051763039375d4b39724f354a', 'name': 'naa.600a098051763039375d4b39724f354a', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a0980383036554e2b514565396e70': {'id': 'naa.600a0980383036554e2b514565396e70', 'name': 'naa.600a0980383036554e2b514565396e70', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 3145728, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_06', 'passthrough_type': 'rdm'}, 'naa.600a098038303042542b51447a38302f': {'id': 'naa.600a098038303042542b51447a38302f', 'name': 'naa.600a098038303042542b51447a38302f', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 10240, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f352d': {'id': 'naa.600a098051763039375d4b39724f352d', 'name': 'naa.600a098051763039375d4b39724f352d', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}} for node [cluster-unassigned-test-02] in cluster [cluster-unassigned-test]
2023-01-16 14:49:15,110|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|137:is_vm_on_host| is_vm_on_host executing
2023-01-16 14:49:15,112|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|173:vm_get| vm_get executing
2023-01-16 14:49:15,268|DEBUG| 2687|12022|rest.py|173|response body: [{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b74","total_diskspace":1048593,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656931","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b50636a","total_diskspace":1048593,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a383059","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38305a","total_diskspace":3145728,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}]
2023-01-16 14:49:15,271| INFO| 2687|12022|logger.py|132|node.py|2241:vm_create_disks| host_pdisks_info {'naa.600a09805176304d6d5d4b384b506b74': {'id': 'naa.600a09805176304d6d5d4b384b506b74', 'name': 'naa.600a09805176304d6d5d4b384b506b74', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724e7a46': {'id': 'naa.600a098051763039375d4b39724e7a46', 'name': 'naa.600a098051763039375d4b39724e7a46', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_006', 'passthrough_type': 'rdm'}, 'naa.600a098051763671763f4b395a656931': {'id': 'naa.600a098051763671763f4b395a656931', 'name': 'naa.600a098051763671763f4b395a656931', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 20480, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3545': {'id': 'naa.600a098051763039375d4b39724f3545', 'name': 'naa.600a098051763039375d4b39724f3545', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3547': {'id': 'naa.600a098051763039375d4b39724f3547', 'name': 'naa.600a098051763039375d4b39724f3547', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f3548': {'id': 'naa.600a098051763039375d4b39724f3548', 'name': 'naa.600a098051763039375d4b39724f3548', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f354a': {'id': 'naa.600a098051763039375d4b39724f354a', 'name': 'naa.600a098051763039375d4b39724f354a', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a09805176304d6d5d4b384b50636a': {'id': 'naa.600a09805176304d6d5d4b384b50636a', 'name': 'naa.600a09805176304d6d5d4b384b50636a', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 1048593, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_05', 'passthrough_type': 'rdm'}, 'naa.600a098038303042542b51447a383059': {'id': 'naa.600a098038303042542b51447a383059', 'name': 'naa.600a098038303042542b51447a383059', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 10240, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}, 'naa.600a098038303042542b51447a38305a': {'id': 'naa.600a098038303042542b51447a38305a', 'name': 'naa.600a098038303042542b51447a38305a', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 3145728, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': 'Datastore_05', 'passthrough_type': 'rdm'}, 'naa.600a098051763039375d4b39724f352d': {'id': 'naa.600a098051763039375d4b39724f352d', 'name': 'naa.600a098051763039375d4b39724f352d', 'logical_name': '', 'adapter': 'vmhba1', 'device_type': 'unknown', 'capacity': 512078, 'description': 'NETAPP Fibre Channel Disk ', 'used_by': None, 'passthrough_type': 'rdm'}} for node [cluster-unassigned-test-01] in cluster [cluster-unassigned-test]
2023-01-16 14:49:15,274|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|137:is_vm_on_host| is_vm_on_host executing
2023-01-16 14:49:15,275|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|173:vm_get| vm_get executing
2023-01-16 14:49:15,399|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-006.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d228bc-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-02","obj_id":"vm-11018","state":"poweredoff","vm_uid":"5016e28c-4aed-f343-96db-8ab9e93939ac"}
2023-01-16 14:49:15,401|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|221:get_vm_info| get_vm_info executing
2023-01-16 14:49:15,559|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-005.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d1009a-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-01","obj_id":"vm-11017","state":"poweredoff","vm_uid":"5016042c-0798-9f35-3c46-fed02c5ee585"}
2023-01-16 14:49:15,562|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|221:get_vm_info| get_vm_info executing
2023-01-16 14:49:15,676|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-006.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d228bc-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-02","obj_id":"vm-11018","state":"poweredoff","vm_uid":"5016e28c-4aed-f343-96db-8ab9e93939ac"}
2023-01-16 14:49:15,678|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-01-16 14:49:15,819|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-005.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d1009a-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-01","obj_id":"vm-11017","state":"poweredoff","vm_uid":"5016042c-0798-9f35-3c46-fed02c5ee585"}
2023-01-16 14:49:15,822|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-01-16 14:49:15,924|DEBUG| 2687|12022|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02.vmdk","pool":"Datastore_06","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02_1.vmdk","pool":"Datastore_06","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"cluster-unassigned-test-02_2.vmdk","pool":"Datastore_06","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02_3.vmdk","pool":"Datastore_06","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02_4.vmdk","pool":"Datastore_06","total_diskspace":69632}]
2023-01-16 14:49:15,933| INFO| 2687|12022|logger.py|132|node.py|2246:vm_create_disks| Starting to create data disks for node [cluster-unassigned-test-02] in cluster [cluster-unassigned-test]
2023-01-16 14:49:15,945| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=406, request_id='12022', time='2023-01-16 14:49:15.937038', evtype='CreatingDataDisks', category='node', level='Info', detail='Creating data disks for node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:15,946|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:15.946374]> [failed:False] Creating data disks for node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".
2023-01-16 14:49:15,948|DEBUG| 2687|12022|logger.py|132|node.py|2253:vm_create_disks| Creating data disk No: 1, disk {'pool': 'Datastore_06', 'capacity': 51200} on host (sdot-b200-006.gdl.englab.netapp.com)
2023-01-16 14:49:15,949|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|379:vm_create_vdisk| vm_create_vdisk executing
2023-01-16 14:49:16,036|DEBUG| 2687|12022|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01.vmdk","pool":"Datastore_05","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01_1.vmdk","pool":"Datastore_05","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"cluster-unassigned-test-01_2.vmdk","pool":"Datastore_05","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01_3.vmdk","pool":"Datastore_05","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01_4.vmdk","pool":"Datastore_05","total_diskspace":69632}]
2023-01-16 14:49:16,044| INFO| 2687|12022|logger.py|132|node.py|2246:vm_create_disks| Starting to create data disks for node [cluster-unassigned-test-01] in cluster [cluster-unassigned-test]
2023-01-16 14:49:16,054| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=407, request_id='12022', time='2023-01-16 14:49:16.045832', evtype='CreatingDataDisks', category='node', level='Info', detail='Creating data disks for node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:16,055|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:16.054924]> [failed:False] Creating data disks for node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".
2023-01-16 14:49:16,055|DEBUG| 2687|12022|logger.py|132|node.py|2253:vm_create_disks| Creating data disk No: 1, disk {'pool': 'Datastore_05', 'capacity': 51200} on host (sdot-b200-005.gdl.englab.netapp.com)
2023-01-16 14:49:16,056|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|379:vm_create_vdisk| vm_create_vdisk executing
2023-01-16 14:49:18,021|DEBUG| 2687|12022|rest.py|173|response body: {"capacity":51200,"name":"cluster-unassigned-test-02_5.vmdk","pool":"Datastore_06"}
2023-01-16 14:49:18,081| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=408, request_id='12022', time='2023-01-16 14:49:18.073890', evtype='NodeDataDiskAdded', category='node', level='Info', detail='Data disk 1 (size: 51200MB) was created and added to node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:18,082|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:18.082741]> [failed:False] Data disk 1 (size: 51200MB) was created and added to node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".
2023-01-16 14:49:18,087|DEBUG| 2687|12022|logger.py|132|node.py|2253:vm_create_disks| Creating data disk No: 2, disk {'pool': 'Datastore_06', 'capacity': 51200} on host (sdot-b200-006.gdl.englab.netapp.com)
2023-01-16 14:49:18,090|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|379:vm_create_vdisk| vm_create_vdisk executing
2023-01-16 14:49:18,121|DEBUG| 2687|12022|rest.py|173|response body: {"capacity":51200,"name":"cluster-unassigned-test-01_5.vmdk","pool":"Datastore_05"}
2023-01-16 14:49:18,148| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=409, request_id='12022', time='2023-01-16 14:49:18.139837', evtype='NodeDataDiskAdded', category='node', level='Info', detail='Data disk 1 (size: 51200MB) was created and added to node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:18,149|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:18.149347]> [failed:False] Data disk 1 (size: 51200MB) was created and added to node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".
2023-01-16 14:49:18,153|DEBUG| 2687|12022|logger.py|132|node.py|2253:vm_create_disks| Creating data disk No: 2, disk {'pool': 'Datastore_05', 'capacity': 51200} on host (sdot-b200-005.gdl.englab.netapp.com)
2023-01-16 14:49:18,156|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|379:vm_create_vdisk| vm_create_vdisk executing
2023-01-16 14:49:19,810|DEBUG| 2687|12022|rest.py|173|response body: {"capacity":51200,"name":"cluster-unassigned-test-02_6.vmdk","pool":"Datastore_06"}
2023-01-16 14:49:19,836| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=410, request_id='12022', time='2023-01-16 14:49:19.828691', evtype='NodeDataDiskAdded', category='node', level='Info', detail='Data disk 2 (size: 51200MB) was created and added to node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:19,837|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:19.837461]> [failed:False] Data disk 2 (size: 51200MB) was created and added to node "cluster-unassigned-test-02" in cluster "cluster-unassigned-test".
2023-01-16 14:49:19,838|DEBUG| 2687|12022|logger.py|132|node.py|2283:vm_create_disks| New data disks in node (cluster-unassigned-test-02): []
2023-01-16 14:49:19,848| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=411, request_id='12022', time='2023-01-16 14:49:19.840385', evtype='NodeDiskCreateSuccessful', category='cluster', level='Info', detail='Data disks on node "cluster-unassigned-test-02" created successfully.')>
2023-01-16 14:49:19,848|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:19.848778]> [failed:False] Data disks on node "cluster-unassigned-test-02" created successfully.
2023-01-16 14:49:19,895|DEBUG| 2687|12022|rest.py|173|response body: {"capacity":51200,"name":"cluster-unassigned-test-01_6.vmdk","pool":"Datastore_05"}
2023-01-16 14:49:19,919| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=412, request_id='12022', time='2023-01-16 14:49:19.912444', evtype='NodeDataDiskAdded', category='node', level='Info', detail='Data disk 2 (size: 51200MB) was created and added to node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".')>
2023-01-16 14:49:19,920|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:19.920206]> [failed:False] Data disk 2 (size: 51200MB) was created and added to node "cluster-unassigned-test-01" in cluster "cluster-unassigned-test".
2023-01-16 14:49:19,921|DEBUG| 2687|12022|logger.py|132|node.py|2283:vm_create_disks| New data disks in node (cluster-unassigned-test-01): []
2023-01-16 14:49:19,930| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=413, request_id='12022', time='2023-01-16 14:49:19.922692', evtype='NodeDiskCreateSuccessful', category='cluster', level='Info', detail='Data disks on node "cluster-unassigned-test-01" created successfully.')>
2023-01-16 14:49:19,930|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:19.930751]> [failed:False] Data disks on node "cluster-unassigned-test-01" created successfully.
2023-01-16 14:49:30,118| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=414, request_id='12022', time='2023-01-16 14:49:30.106088', evtype='ClusterNodesDataDiskCreateCompleted', category='cluster', level='Info', detail='Data disks were successfully created and added to all nodes in the cluster "cluster-unassigned-test".')>
2023-01-16 14:49:30,119|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:30.119783]> [failed:False] Data disks were successfully created and added to all nodes in the cluster "cluster-unassigned-test".
2023-01-16 14:49:30,120| INFO| 2687|12022|logger.py|132|cluster_tasks.py|277:create_cluster| Cluster [cluster-unassigned-test]: starting nodes ...
2023-01-16 14:49:30,131| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=415, request_id='12022', time='2023-01-16 14:49:30.122698', evtype='NodesPowerOnInProgress', category='cluster', level='Info', detail='All nodes in cluster "cluster-unassigned-test" are being powered on.')>
2023-01-16 14:49:30,131|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:30.131875]> [failed:False] All nodes in cluster "cluster-unassigned-test" are being powered on.
2023-01-16 14:49:30,132| INFO| 2687|12022|logger.py|132|cluster_tasks.py|585:_start_nodes| Cluster [cluster-unassigned-test]: starting node cluster-unassigned-test-02
2023-01-16 14:49:30,133| INFO| 2687|12022|logger.py|132|cluster_tasks.py|591:_start_nodes| Starting nodes for all nodes in the cluster...
2023-01-16 14:49:30,134|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_start_task) [ACTIVE]>}]
2023-01-16 14:49:30,135|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_start_task) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.cluster_tasks:vm_start_task) [ACTIVE]>}]
2023-01-16 14:49:30,144|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|346:vm_start| vm_start executing
2023-01-16 14:49:34,838|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-006.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d228bc-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-02","obj_id":"vm-11018","state":"poweredon","vm_uid":"5016e28c-4aed-f343-96db-8ab9e93939ac"}
2023-01-16 14:49:34,940| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=416, request_id='12022', time='2023-01-16 14:49:34.932554', evtype='NodePoweredOn', category='cluster', level='Info', detail='Node "cluster-unassigned-test-02" is successfully powered on.')>
2023-01-16 14:49:34,941|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:34.940897]> [failed:False] Node "cluster-unassigned-test-02" is successfully powered on.
2023-01-16 14:49:34,947| INFO| 2687|12022|logger.py|132|cluster_tasks.py|602:_start_nodes| Cluster [cluster-unassigned-test]: waiting a bit (60 secs) before starting cluster create node ...
2023-01-16 14:50:35,010| INFO| 2687|12022|logger.py|132|cluster_tasks.py|605:_start_nodes| Cluster [cluster-unassigned-test]: starting node cluster-unassigned-test-01
2023-01-16 14:50:35,023|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|346:vm_start| vm_start executing
2023-01-16 14:50:35,060|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:50:35,061|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564633560: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:50:35,069| INFO| 2687|12034|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:50:35,666| INFO| 2687|12034|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 604ms with status HTTPStatus.OK (BA)
2023-01-16 14:50:40,999|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-005.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d1009a-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-01","obj_id":"vm-11017","state":"poweredon","vm_uid":"5016042c-0798-9f35-3c46-fed02c5ee585"}
2023-01-16 14:50:41,002|DEBUG| 2687|12022|logger.py|132|host.py|765:vm_start| vm_start response: {'host_id': 'sdot-b200-005.gdl.englab.netapp.com',
 'hw_version': 'vmx-14',
 'metadata': {'cluster_uuid': 'e0d06a86-95ab-11ed-bd72-000c29e96265',
              'node_uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265'},
 'mgmt_server_uid': '3025b4b7-b510-4852-98ed-38668b194b85',
 'name': 'cluster-unassigned-test-01',
 'obj_id': 'vm-11017',
 'state': 'poweredon',
 'vm_uid': '5016042c-0798-9f35-3c46-fed02c5ee585'}
2023-01-16 14:50:41,024| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=417, request_id='12022', time='2023-01-16 14:50:41.016512', evtype='AllNodesPoweredOn', category='cluster', level='Info', detail='All nodes in cluster "cluster-unassigned-test" are successfully powered on.')>
2023-01-16 14:50:41,025|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:50:41.024942]> [failed:False] All nodes in cluster "cluster-unassigned-test" are successfully powered on.
2023-01-16 14:50:41,025| INFO| 2687|12022|logger.py|132|cluster_tasks.py|280:create_cluster| Cluster [cluster-unassigned-test]: all nodes started, starting cluster deploy checks ...
2023-01-16 14:50:41,056| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=418, request_id='12022', time='2023-01-16 14:50:41.048934', evtype='NodeIpsPingableCheck', category='cluster', level='Info', detail='Checking if all nodes in the cluster are responding to ping requests.')>
2023-01-16 14:50:41,057|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:50:41.057130]> [failed:False] Checking if all nodes in the cluster are responding to ping requests.
2023-01-16 14:50:43,095|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:05,138|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:27,178|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:47,213|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.229
2023-01-16 14:51:47,223|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:47,242| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=419, request_id='12022', time='2023-01-16 14:51:47.226687', evtype='IpPingStatus', category='cluster', level='Info', detail='Ping succeeded on IPs: ['10.228.160.231', '10.228.160.229']. Waiting for the following IPs: [].')>
2023-01-16 14:51:47,243|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:51:47.243652]> [failed:False] Ping succeeded on IPs: ['10.228.160.231', '10.228.160.229']. Waiting for the following IPs: [].
2023-01-16 14:51:47,244| INFO| 2687|12022|logger.py|132|cluster.py|1868:check_cluster_status| Cluster [cluster-unassigned-test]: all nodes pingable!
2023-01-16 14:51:47,256| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=420, request_id='12022', time='2023-01-16 14:51:47.246407', evtype='AllNodeMgmtIpsPingable', category='cluster', level='Info', detail='All nodes in the cluster are responding to ping requests.')>
2023-01-16 14:51:47,257|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:51:47.257656]> [failed:False] All nodes in the cluster are responding to ping requests.
2023-01-16 14:52:17,316|WARNING| 2687|12022|logger.py|132|ZAPI.py|79:retry| Operation: invoke. Retries remaining: 178. Result: ConnectionRefusedError(111, 'Connection refused').
2023-01-16 14:52:37,339| INFO| 2687|12022|logger.py|132|ZAPI.py|116:retry| Retry invoke for operation invoke. Remaining retries: 178.
2023-01-16 14:52:37,342|WARNING| 2687|12022|logger.py|132|ZAPI.py|79:retry| Operation: invoke. Retries remaining: 177. Result: ConnectionRefusedError(111, 'Connection refused').
2023-01-16 14:52:57,365| INFO| 2687|12022|logger.py|132|ZAPI.py|116:retry| Retry invoke for operation invoke. Remaining retries: 177.
2023-01-16 14:52:57,381|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:52:57,382|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:52:57,389| INFO| 2687|12035|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:52:57,988| INFO| 2687|12035|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 606ms with status HTTPStatus.OK (BA)
2023-01-16 14:52:58,427|WARNING| 2687|12022|logger.py|132|ZAPI.py|79:retry| Operation: invoke. Retries remaining: 176. Result: ConnectionRefusedError(111, 'Connection refused').
2023-01-16 14:53:18,446| INFO| 2687|12022|logger.py|132|ZAPI.py|116:retry| Retry invoke for operation invoke. Remaining retries: 176.
2023-01-16 14:53:19,836|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:53:19,837|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Creating cluster', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:53:19,838| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Creating cluster
2023-01-16 14:53:39,855| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=421, request_id='12022', time='2023-01-16 14:53:39.841939', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Creating cluster].')>
2023-01-16 14:53:39,856|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:53:39.856233]> [failed:False] ONTAP cluster create in progress. Status: [Creating cluster].
2023-01-16 14:53:39,937|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:53:39,938|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Vserver Management', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:53:39,939| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Vserver Management
2023-01-16 14:53:59,983| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=422, request_id='12022', time='2023-01-16 14:53:59.962228', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Vserver Management].')>
2023-01-16 14:53:59,984|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:53:59.984693]> [failed:False] ONTAP cluster create in progress. Status: [Vserver Management].
2023-01-16 14:54:00,027|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:54:00,029|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Non shared cluster setup', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:54:00,031| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Non shared cluster setup
2023-01-16 14:54:20,068| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=423, request_id='12022', time='2023-01-16 14:54:20.055061', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Non shared cluster setup].')>
2023-01-16 14:54:20,069|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:54:20.069449]> [failed:False] ONTAP cluster create in progress. Status: [Non shared cluster setup].
2023-01-16 14:54:20,145|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:54:20,147|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Starting cluster support services', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:54:20,148| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Starting cluster support services
2023-01-16 14:54:40,166| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=424, request_id='12022', time='2023-01-16 14:54:40.157812', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Starting cluster support services].')>
2023-01-16 14:54:40,167|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:54:40.167536]> [failed:False] ONTAP cluster create in progress. Status: [Starting cluster support services].
2023-01-16 14:54:40,208|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:54:40,209|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Starting cluster support services', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:54:40,210| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Starting cluster support services
2023-01-16 14:55:00,291|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:00,292|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Cluster has been created.', 'is-complete': 'true', 'status': 'success'}]
2023-01-16 14:55:00,293| INFO| 2687|12022|logger.py|132|cluster.py|2090:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster create finished!
2023-01-16 14:55:00,334|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:00,335|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'failure-msg': None, 'job-id': '16', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'node_discovery'}]]
2023-01-16 14:55:00,336| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, Unknown: node_discovery
2023-01-16 14:55:00,345| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=425, request_id='12022', time='2023-01-16 14:55:00.337831', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, Unknown: node_discovery].')>
2023-01-16 14:55:00,346|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:55:00.346261]> [failed:False] ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, Unknown: node_discovery].
2023-01-16 14:55:20,412|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:20,413|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:55:20,414| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:55:20,424| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=426, request_id='12022', time='2023-01-16 14:55:20.416059', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup].')>
2023-01-16 14:55:20,425|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:55:20.425043]> [failed:False] ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup].
2023-01-16 14:55:40,470|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:55:40,471|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:55:40,476| INFO| 2687|12036|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:55:41,096| INFO| 2687|12036|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 624ms with status HTTPStatus.OK (BA)
2023-01-16 14:55:41,606|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:41,608|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:55:41,610| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:56:01,695|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:56:01,696|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:56:01,697| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:56:21,768|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:56:21,769|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:56:21,770| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:56:41,817|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:56:41,818|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:56:41,819| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:57:01,878|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:01,879|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:57:01,880| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:57:21,945|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:21,946|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'success'}]]
2023-01-16 14:57:21,947| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: success
2023-01-16 14:57:21,957| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=427, request_id='12022', time='2023-01-16 14:57:21.948687', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: success].')>
2023-01-16 14:57:21,958|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.958435]> [failed:False] ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: success].
2023-01-16 14:57:21,959| INFO| 2687|12022|logger.py|132|cluster.py|2150:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes complete
2023-01-16 14:57:21,969|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.161.159
2023-01-16 14:57:21,979| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=428, request_id='12022', time='2023-01-16 14:57:21.971790', evtype='IpPingStatus', category='cluster', level='Info', detail='Ping succeeded on IPs: ['10.228.161.159']. Waiting for the following IPs: [].')>
2023-01-16 14:57:21,980|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.980621]> [failed:False] Ping succeeded on IPs: ['10.228.161.159']. Waiting for the following IPs: [].
2023-01-16 14:57:21,981| INFO| 2687|12022|logger.py|132|cluster.py|1889:check_cluster_status| Cluster [cluster-unassigned-test]: cluster ip 10.228.161.159 is pingable!
2023-01-16 14:57:21,990| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=429, request_id='12022', time='2023-01-16 14:57:21.983041', evtype='ClusterMgmtIpPingable', category='cluster', level='Info', detail='Cluster is responding to ping requests on the cluster management IP address.')>
2023-01-16 14:57:21,990|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.990701]> [failed:False] Cluster is responding to ping requests on the cluster management IP address.
2023-01-16 14:57:21,991| INFO| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.991364]> [failed:False] Waiting for aggregates to resync.
2023-01-16 14:57:22,188|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:22,189| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:57:32,289|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:42,388|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:52,502|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:52,504| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:58:02,601|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:12,720|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:22,843|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:22,847| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:58:32,942|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:43,045|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:53,075|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:58:53,078|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:58:53,081| INFO| 2687|12037|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:58:53,806| INFO| 2687|12037|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 728ms with status HTTPStatus.OK (BA)
2023-01-16 14:58:54,388|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:54,392| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:59:04,505|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:14,619|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:24,744|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:24,747| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:59:34,855|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:44,942|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:55,054|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:55,057| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 15:00:05,179|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:15,288|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:25,325|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.crontab:rotate_events) [ACTIVE]>}]
2023-01-16 15:00:25,428|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
2023-01-16 14:49:30,131|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:30.131875]> [failed:False] All nodes in cluster "cluster-unassigned-test" are being powered on.
2023-01-16 14:49:30,132| INFO| 2687|12022|logger.py|132|cluster_tasks.py|585:_start_nodes| Cluster [cluster-unassigned-test]: starting node cluster-unassigned-test-02
2023-01-16 14:49:30,133| INFO| 2687|12022|logger.py|132|cluster_tasks.py|591:_start_nodes| Starting nodes for all nodes in the cluster...
2023-01-16 14:49:30,134|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_start_task) [ACTIVE]>}]
2023-01-16 14:49:30,135|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (asynchronous.cluster_tasks:vm_start_task) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.cluster_tasks:vm_start_task) [ACTIVE]>}]
2023-01-16 14:49:30,144|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|346:vm_start| vm_start executing
2023-01-16 14:49:34,838|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-006.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d228bc-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-02","obj_id":"vm-11018","state":"poweredon","vm_uid":"5016e28c-4aed-f343-96db-8ab9e93939ac"}
2023-01-16 14:49:34,940| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=416, request_id='12022', time='2023-01-16 14:49:34.932554', evtype='NodePoweredOn', category='cluster', level='Info', detail='Node "cluster-unassigned-test-02" is successfully powered on.')>
2023-01-16 14:49:34,941|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:49:34.940897]> [failed:False] Node "cluster-unassigned-test-02" is successfully powered on.
2023-01-16 14:49:34,947| INFO| 2687|12022|logger.py|132|cluster_tasks.py|602:_start_nodes| Cluster [cluster-unassigned-test]: waiting a bit (60 secs) before starting cluster create node ...
2023-01-16 14:50:35,010| INFO| 2687|12022|logger.py|132|cluster_tasks.py|605:_start_nodes| Cluster [cluster-unassigned-test]: starting node cluster-unassigned-test-01
2023-01-16 14:50:35,023|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|346:vm_start| vm_start executing
2023-01-16 14:50:35,060|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:50:35,061|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564633560: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:50:35,069| INFO| 2687|12034|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:50:35,666| INFO| 2687|12034|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 604ms with status HTTPStatus.OK (BA)
2023-01-16 14:50:40,999|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-005.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d1009a-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-01","obj_id":"vm-11017","state":"poweredon","vm_uid":"5016042c-0798-9f35-3c46-fed02c5ee585"}
2023-01-16 14:50:41,002|DEBUG| 2687|12022|logger.py|132|host.py|765:vm_start| vm_start response: {'host_id': 'sdot-b200-005.gdl.englab.netapp.com',
 'hw_version': 'vmx-14',
 'metadata': {'cluster_uuid': 'e0d06a86-95ab-11ed-bd72-000c29e96265',
              'node_uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265'},
 'mgmt_server_uid': '3025b4b7-b510-4852-98ed-38668b194b85',
 'name': 'cluster-unassigned-test-01',
 'obj_id': 'vm-11017',
 'state': 'poweredon',
 'vm_uid': '5016042c-0798-9f35-3c46-fed02c5ee585'}
2023-01-16 14:50:41,024| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=417, request_id='12022', time='2023-01-16 14:50:41.016512', evtype='AllNodesPoweredOn', category='cluster', level='Info', detail='All nodes in cluster "cluster-unassigned-test" are successfully powered on.')>
2023-01-16 14:50:41,025|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:50:41.024942]> [failed:False] All nodes in cluster "cluster-unassigned-test" are successfully powered on.
2023-01-16 14:50:41,025| INFO| 2687|12022|logger.py|132|cluster_tasks.py|280:create_cluster| Cluster [cluster-unassigned-test]: all nodes started, starting cluster deploy checks ...
2023-01-16 14:50:41,056| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=418, request_id='12022', time='2023-01-16 14:50:41.048934', evtype='NodeIpsPingableCheck', category='cluster', level='Info', detail='Checking if all nodes in the cluster are responding to ping requests.')>
2023-01-16 14:50:41,057|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:50:41.057130]> [failed:False] Checking if all nodes in the cluster are responding to ping requests.
2023-01-16 14:50:43,095|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:05,138|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:27,178|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:47,213|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.229
2023-01-16 14:51:47,223|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.160.231
2023-01-16 14:51:47,242| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=419, request_id='12022', time='2023-01-16 14:51:47.226687', evtype='IpPingStatus', category='cluster', level='Info', detail='Ping succeeded on IPs: ['10.228.160.231', '10.228.160.229']. Waiting for the following IPs: [].')>
2023-01-16 14:51:47,243|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:51:47.243652]> [failed:False] Ping succeeded on IPs: ['10.228.160.231', '10.228.160.229']. Waiting for the following IPs: [].
2023-01-16 14:51:47,244| INFO| 2687|12022|logger.py|132|cluster.py|1868:check_cluster_status| Cluster [cluster-unassigned-test]: all nodes pingable!
2023-01-16 14:51:47,256| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=420, request_id='12022', time='2023-01-16 14:51:47.246407', evtype='AllNodeMgmtIpsPingable', category='cluster', level='Info', detail='All nodes in the cluster are responding to ping requests.')>
2023-01-16 14:51:47,257|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:51:47.257656]> [failed:False] All nodes in the cluster are responding to ping requests.
2023-01-16 14:52:17,316|WARNING| 2687|12022|logger.py|132|ZAPI.py|79:retry| Operation: invoke. Retries remaining: 178. Result: ConnectionRefusedError(111, 'Connection refused').
2023-01-16 14:52:37,339| INFO| 2687|12022|logger.py|132|ZAPI.py|116:retry| Retry invoke for operation invoke. Remaining retries: 178.
2023-01-16 14:52:37,342|WARNING| 2687|12022|logger.py|132|ZAPI.py|79:retry| Operation: invoke. Retries remaining: 177. Result: ConnectionRefusedError(111, 'Connection refused').
2023-01-16 14:52:57,365| INFO| 2687|12022|logger.py|132|ZAPI.py|116:retry| Retry invoke for operation invoke. Remaining retries: 177.
2023-01-16 14:52:57,381|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:52:57,382|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:52:57,389| INFO| 2687|12035|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:52:57,988| INFO| 2687|12035|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 606ms with status HTTPStatus.OK (BA)
2023-01-16 14:52:58,427|WARNING| 2687|12022|logger.py|132|ZAPI.py|79:retry| Operation: invoke. Retries remaining: 176. Result: ConnectionRefusedError(111, 'Connection refused').
2023-01-16 14:53:18,446| INFO| 2687|12022|logger.py|132|ZAPI.py|116:retry| Retry invoke for operation invoke. Remaining retries: 176.
2023-01-16 14:53:19,836|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:53:19,837|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Creating cluster', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:53:19,838| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Creating cluster
2023-01-16 14:53:39,855| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=421, request_id='12022', time='2023-01-16 14:53:39.841939', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Creating cluster].')>
2023-01-16 14:53:39,856|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:53:39.856233]> [failed:False] ONTAP cluster create in progress. Status: [Creating cluster].
2023-01-16 14:53:39,937|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:53:39,938|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Vserver Management', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:53:39,939| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Vserver Management
2023-01-16 14:53:59,983| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=422, request_id='12022', time='2023-01-16 14:53:59.962228', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Vserver Management].')>
2023-01-16 14:53:59,984|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:53:59.984693]> [failed:False] ONTAP cluster create in progress. Status: [Vserver Management].
2023-01-16 14:54:00,027|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:54:00,029|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Non shared cluster setup', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:54:00,031| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Non shared cluster setup
2023-01-16 14:54:20,068| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=423, request_id='12022', time='2023-01-16 14:54:20.055061', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Non shared cluster setup].')>
2023-01-16 14:54:20,069|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:54:20.069449]> [failed:False] ONTAP cluster create in progress. Status: [Non shared cluster setup].
2023-01-16 14:54:20,145|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:54:20,147|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Starting cluster support services', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:54:20,148| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Starting cluster support services
2023-01-16 14:54:40,166| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=424, request_id='12022', time='2023-01-16 14:54:40.157812', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Starting cluster support services].')>
2023-01-16 14:54:40,167|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:54:40.167536]> [failed:False] ONTAP cluster create in progress. Status: [Starting cluster support services].
2023-01-16 14:54:40,208|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:54:40,209|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Starting cluster support services', 'is-complete': 'false', 'status': 'ongoing'}]
2023-01-16 14:54:40,210| INFO| 2687|12022|logger.py|132|cluster.py|2094:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster not ready yet. current status: Starting cluster support services
2023-01-16 14:55:00,291|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-create-join-progress-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:00,292|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|267:z_get_cluster_create_progress| Cluster [cluster-unassigned-test]: cluster-create-join-progress: [{'current-status-message': 'Cluster has been created.', 'is-complete': 'true', 'status': 'success'}]
2023-01-16 14:55:00,293| INFO| 2687|12022|logger.py|132|cluster.py|2090:_ensure_cluster_created| Cluster [cluster-unassigned-test]: cluster create finished!
2023-01-16 14:55:00,334|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:00,335|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'failure-msg': None, 'job-id': '16', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'node_discovery'}]]
2023-01-16 14:55:00,336| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, Unknown: node_discovery
2023-01-16 14:55:00,345| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=425, request_id='12022', time='2023-01-16 14:55:00.337831', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, Unknown: node_discovery].')>
2023-01-16 14:55:00,346|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:55:00.346261]> [failed:False] ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, Unknown: node_discovery].
2023-01-16 14:55:20,412|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:20,413|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:55:20,414| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:55:20,424| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=426, request_id='12022', time='2023-01-16 14:55:20.416059', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup].')>
2023-01-16 14:55:20,425|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:55:20.425043]> [failed:False] ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup].
2023-01-16 14:55:40,470|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:55:40,471|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634376: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:55:40,476| INFO| 2687|12036|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:55:41,096| INFO| 2687|12036|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 624ms with status HTTPStatus.OK (BA)
2023-01-16 14:55:41,606|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:55:41,608|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:55:41,610| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:56:01,695|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:56:01,696|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:56:01,697| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:56:21,768|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:56:21,769|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:56:21,770| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:56:41,817|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:56:41,818|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:56:41,819| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:57:01,878|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:01,879|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'post_cluster_setup'}]]
2023-01-16 14:57:01,880| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: post_cluster_setup
2023-01-16 14:57:21,945|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-add-node-status-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:21,946|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|284:z_get_cluster_add_node_status| Cluster [cluster-unassigned-test]: cluster-add-node-status: [[{'cluster-ip': '169.254.22.201', 'failure-msg': None, 'node-name': 'cluster-unassigned-test-01', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'status': 'success'}, {'cluster-ip': '169.254.23.122', 'failure-msg': None, 'job-id': '16', 'node-name': 'cluster-unassigned-test-02', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'status': 'success'}]]
2023-01-16 14:57:21,947| INFO| 2687|12022|logger.py|132|cluster.py|2141:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes in progress: Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: success
2023-01-16 14:57:21,957| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=427, request_id='12022', time='2023-01-16 14:57:21.948687', evtype='ONTAPClusterCreateStatus', category='cluster', level='Info', detail='ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: success].')>
2023-01-16 14:57:21,958|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.958435]> [failed:False] ONTAP cluster create in progress. Status: [Adding Nodes - cluster-unassigned-test-01: success, cluster-unassigned-test-02: success].
2023-01-16 14:57:21,959| INFO| 2687|12022|logger.py|132|cluster.py|2150:_ensure_nodes_added| Cluster [cluster-unassigned-test]: cluster add nodes complete
2023-01-16 14:57:21,969|DEBUG| 2687|12022|logger.py|132|cluster.py|3315:_ips_pingable| Cluster [cluster-unassigned-test]: able to ping 10.228.161.159
2023-01-16 14:57:21,979| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=428, request_id='12022', time='2023-01-16 14:57:21.971790', evtype='IpPingStatus', category='cluster', level='Info', detail='Ping succeeded on IPs: ['10.228.161.159']. Waiting for the following IPs: [].')>
2023-01-16 14:57:21,980|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.980621]> [failed:False] Ping succeeded on IPs: ['10.228.161.159']. Waiting for the following IPs: [].
2023-01-16 14:57:21,981| INFO| 2687|12022|logger.py|132|cluster.py|1889:check_cluster_status| Cluster [cluster-unassigned-test]: cluster ip 10.228.161.159 is pingable!
2023-01-16 14:57:21,990| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=429, request_id='12022', time='2023-01-16 14:57:21.983041', evtype='ClusterMgmtIpPingable', category='cluster', level='Info', detail='Cluster is responding to ping requests on the cluster management IP address.')>
2023-01-16 14:57:21,990|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.990701]> [failed:False] Cluster is responding to ping requests on the cluster management IP address.
2023-01-16 14:57:21,991| INFO| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 14:57:21.991364]> [failed:False] Waiting for aggregates to resync.
2023-01-16 14:57:22,188|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:22,189| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:57:32,289|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:42,388|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:52,502|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:57:52,504| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:58:02,601|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:12,720|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:22,843|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:22,847| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:58:32,942|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:43,045|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:53,075|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.event:get_events] executing
2023-01-16 14:58:53,078|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449564634104: <CoroutineRecord (controllers.event:get_events) [ACTIVE]>}]
2023-01-16 14:58:53,081| INFO| 2687|12037|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc
2023-01-16 14:58:53,806| INFO| 2687|12037|restapi.py|263|GET http://127.0.0.1:8080/api/v3/events?fields=*&order_by=time+desc returned in 728ms with status HTTPStatus.OK (BA)
2023-01-16 14:58:54,388|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:58:54,392| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:59:04,505|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:14,619|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:24,744|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:24,747| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 14:59:34,855|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:44,942|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:55,054|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 14:59:55,057| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 15:00:05,179|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:15,288|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:25,325|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449557918040: <CoroutineRecord (asynchronous.crontab:rotate_events) [ACTIVE]>}]
2023-01-16 15:00:25,428|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:25,431| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirror resynchronizing': 2}
2023-01-16 15:00:35,527|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:45,629|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:55,743|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "aggr-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:55,745| INFO| 2687|12022|logger.py|132|cluster.py|2235:_get_aggr_resync_status| Cluster [cluster-unassigned-test]: resync status {'mirrored': 2}
2023-01-16 15:00:55,746| INFO| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:00:55.746902]> [failed:False] Aggregates finished resyncing in 213 seconds.
2023-01-16 15:00:55,994|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cf-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:55,995| INFO| 2687|12022|logger.py|132|cluster.py|2320:_handle_clusterwait_state| Cluster [cluster-unassigned-test]: "node_state" is connected
2023-01-16 15:00:55,995| INFO| 2687|12022|logger.py|132|cluster.py|2320:_handle_clusterwait_state| Cluster [cluster-unassigned-test]: "node_state" is connected
2023-01-16 15:00:55,996| INFO| 2687|12022|logger.py|132|cluster.py|2366:_check_sfo_enabled| Cluster [cluster-unassigned-test]: checking sfo enabled status
2023-01-16 15:00:56,063|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cf-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:56,065| INFO| 2687|12022|logger.py|132|cluster.py|2396:_is_sfo_enabled| Cluster [cluster-unassigned-test]: localnode: cluster-unassigned-test-01
2023-01-16 15:00:56,067| INFO| 2687|12022|logger.py|132|cluster.py|2397:_is_sfo_enabled| Cluster [cluster-unassigned-test]: partner_nodename: cluster-unassigned-test-02
2023-01-16 15:00:56,067| INFO| 2687|12022|logger.py|132|cluster.py|2406:_is_sfo_enabled| Cluster [cluster-unassigned-test]: ha_type is non_shared_storage
2023-01-16 15:00:56,068| INFO| 2687|12022|logger.py|132|cluster.py|2415:_is_sfo_enabled| Cluster [cluster-unassigned-test]: "mode" is ha
2023-01-16 15:00:56,069| INFO| 2687|12022|logger.py|132|cluster.py|2424:_is_sfo_enabled| Cluster [cluster-unassigned-test]: "node_state" is connected
2023-01-16 15:00:56,070| INFO| 2687|12022|logger.py|132|cluster.py|2434:_is_sfo_enabled| Cluster [cluster-unassigned-test]: "failover_enabled" is true
2023-01-16 15:00:56,071| INFO| 2687|12022|logger.py|132|cluster.py|2444:_is_sfo_enabled| Cluster [cluster-unassigned-test]: "auto_giveback_enabled" is true
2023-01-16 15:00:56,072| INFO| 2687|12022|logger.py|132|cluster.py|2396:_is_sfo_enabled| Cluster [cluster-unassigned-test]: localnode: cluster-unassigned-test-02
2023-01-16 15:00:56,072| INFO| 2687|12022|logger.py|132|cluster.py|2397:_is_sfo_enabled| Cluster [cluster-unassigned-test]: partner_nodename: cluster-unassigned-test-01
2023-01-16 15:00:56,073| INFO| 2687|12022|logger.py|132|cluster.py|2406:_is_sfo_enabled| Cluster [cluster-unassigned-test]: ha_type is non_shared_storage
2023-01-16 15:00:56,074| INFO| 2687|12022|logger.py|132|cluster.py|2415:_is_sfo_enabled| Cluster [cluster-unassigned-test]: "mode" is ha
2023-01-16 15:00:56,075| INFO| 2687|12022|logger.py|132|cluster.py|2424:_is_sfo_enabled| Cluster [cluster-unassigned-test]: "node_state" is connected
2023-01-16 15:00:56,076| INFO| 2687|12022|logger.py|132|cluster.py|2434:_is_sfo_enabled| Cluster [cluster-unassigned-test]: "failover_enabled" is true
2023-01-16 15:00:56,076| INFO| 2687|12022|logger.py|132|cluster.py|2444:_is_sfo_enabled| Cluster [cluster-unassigned-test]: "auto_giveback_enabled" is true
2023-01-16 15:00:56,083| INFO| 2687|12022|logger.py|132|cluster.py|1905:check_cluster_status| Cluster [cluster-unassigned-test]: storage failover is enabled!
2023-01-16 15:00:56,093| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=430, request_id='12022', time='2023-01-16 15:00:56.085599', evtype='StorageFailoverEnabled', category='cluster', level='Info', detail='Cluster storage HA failover is enabled.')>
2023-01-16 15:00:56,094|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:00:56.094088]> [failed:False] Cluster storage HA failover is enabled.
2023-01-16 15:00:56,134|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-identity-get"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:56,136|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|225:z_get_cluster_identity| Cluster [cluster-unassigned-test]: got cluster_identity: [{'cluster-contact': None, 'cluster-location': None, 'cluster-name': 'cluster-unassigned-test', 'cluster-serial-number': '1-80-000011', 'cluster-uuid': 'e0d06a86-95ab-11ed-bd72-000c29e96265', 'rdb-uuid': '7f82cc2f-95ad-11ed-a014-00a0b88b067e'}]
2023-01-16 15:00:56,138| INFO| 2687|12022|logger.py|132|cluster.py|3365:_verify_cluster_identity| Cluster [cluster-unassigned-test]: cluster identity matched
2023-01-16 15:00:56,158|DEBUG| 2687|12022|fault.py|147|Unable to find a registered fault for zapi.system_node_get_iter.
2023-01-16 15:00:56,250|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "system-node-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:56,251|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|245:z_system_node_get_iter| Cluster [cluster-unassigned-test]: got system node get iter: [[{'cpu-busytime': '222', 'env-failed-fan-count': '0', 'env-failed-fan-message': 'There are no failed fans.', 'env-failed-power-supply-count': '0', 'env-failed-power-supply-message': 'There are no failed power supplies.', 'env-over-temperature': 'false', 'is-all-flash-optimized': 'false', 'is-all-flash-select-optimized': 'false', 'is-capacity-optimized': 'false', 'is-cloud-optimized': 'false', 'is-diff-svcs': 'false', 'is-epsilon-node': 'false', 'is-node-cluster-eligible': 'true', 'is-node-healthy': 'true', 'is-perf-optimized': 'false', 'maximum-aggregate-size': '219902325555200', 'maximum-number-of-volumes': '1000', 'maximum-volume-size': '329853488332800', 'node': 'cluster-unassigned-test-01', 'node-location': None, 'node-model': 'FDvM300', 'node-nvram-id': '2443885361', 'node-owner': None, 'node-serial-number': '99887766554433221149', 'node-storage-configuration': 'unknown', 'node-system-id': '2443885361', 'node-uptime': '586', 'node-uuid': 'e0d1009a-95ab-11ed-bd72-000c29e96265', 'node-vendor': 'NetApp', 'nvram-battery-status': 'battery_ok', 'product-version': 'NetApp Release Lighthouse__9.13.1: Sun Jan 01 08:57:59 UTC 2023', 'sas2-sas3-mixed-stack-support': 'none', 'vmhost-info': {'vm-uuid': '42166372-6b26-c162-38a4-f6a25f9266ef', 'vmhost-error': 'Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the "system node virtual-machine hypervisor modify-credentials" command.', 'vmhost-hardware-vendor': 'VMware, Inc.', 'vmhost-model': 'VMware Virtual Platform', 'vmhost-software-vendor': 'NetApp'}}, {'cpu-busytime': '226', 'env-failed-fan-count': '0', 'env-failed-fan-message': 'There are no failed fans.', 'env-failed-power-supply-count': '0', 'env-failed-power-supply-message': 'There are no failed power supplies.', 'env-over-temperature': 'false', 'is-all-flash-optimized': 'false', 'is-all-flash-select-optimized': 'false', 'is-capacity-optimized': 'false', 'is-cloud-optimized': 'false', 'is-diff-svcs': 'false', 'is-epsilon-node': 'false', 'is-node-cluster-eligible': 'true', 'is-node-healthy': 'true', 'is-perf-optimized': 'false', 'maximum-aggregate-size': '219902325555200', 'maximum-number-of-volumes': '1000', 'maximum-volume-size': '329853488332800', 'node': 'cluster-unassigned-test-02', 'node-location': None, 'node-model': 'FDvM300', 'node-nvram-id': '2443885401', 'node-owner': None, 'node-serial-number': '99887766554433221189', 'node-storage-configuration': 'unknown', 'node-system-id': '2443885401', 'node-uptime': '653', 'node-uuid': 'e0d228bc-95ab-11ed-bd72-000c29e96265', 'node-vendor': 'NetApp', 'nvram-battery-status': 'battery_ok', 'product-version': 'NetApp Release Lighthouse__9.13.1: Sun Jan 01 08:57:59 UTC 2023', 'sas2-sas3-mixed-stack-support': 'none', 'vmhost-info': {'vm-uuid': '421679e3-db61-326e-a782-4f88bb087e40', 'vmhost-error': 'Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the "system node virtual-machine hypervisor modify-credentials" command.', 'vmhost-hardware-vendor': 'VMware, Inc.', 'vmhost-model': 'VMware Virtual Platform', 'vmhost-software-vendor': 'NetApp'}}]]
2023-01-16 15:00:56,279| INFO| 2687|12022|logger.py|132|cluster.py|2617:_configure_disks| Cluster [cluster-unassigned-test]: configuring unassigned disks on nodes (cluster-unassigned-test-01 and cluster-unassigned-test-02)
2023-01-16 15:00:56,383|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "storage-disk-get-iter"
User: "admin
2023-01-16 15:00:56,383|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "storage-disk-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:56,439|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "storage-disk-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:56,441| INFO| 2687|12022|logger.py|132|cluster.py|2818:_assign_disks| Cluster [cluster-unassigned-test]: unassigned disks local data on node (cluster-unassigned-test-01): [[{'disk-inventory-info': {'bytes-per-sector': '512', 'capacity-sectors': '104857592', 'checksum-compatibility': 'advanced_zoned', 'disk-class': 'solid-state', 'disk-cluster-name': 'NET-1.2', 'disk-type': 'SSD', 'disk-uid': '4E455441:50502020:30303030:30303030:55434741:6B334E67:69347333:00000000:00000000:00000000', 'firmware-revision': '0001', 'grown-defect-list-count': '0', 'health-monitor-time-interval': '0', 'import-in-progress': 'false', 'is-dynamically-qualified': 'false', 'is-foreign': 'false', 'is-multidisk-carrier': 'false', 'is-shared': 'false', 'location': 'cluster-unassigned-test-01', 'location-id': '2443885361', 'model': 'PHA-DISK', 'reservation-key': '0x0', 'reservation-type': 'none', 'right-size-sectors': '104844800', 'serial-number': '00000000UCGAk3Ngi4s3', 'vendor': 'NETAPP', 'virtual-machine-disk-info': {'vmhost-error': 'Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the "system node virtual-machine hypervisor modify-credentials" command.'}, 'vmdisk-target-address': 'Scsi bus=0, target=1, lun=0'}, 'disk-metrocluster-info': {'is-local-attach': 'true'}, 'disk-name': 'NET-1.2', 'disk-ownership-info': {'disk-uid': '4E455441:50502020:30303030:30303030:55434741:6B334E67:69347333:00000000:00000000:00000000', 'is-failed': 'false', 'reserved-by-node-id': '0'}, 'disk-paths': {'disk-path-info': [{'array-name': 'NETAPP_PHA_1', 'disk-name': 'cluster-unassigned-test-01:0b.8', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:55434741:6B334E67:69347333:00000000:00000000:00000000', 'initiator-io-kbps': '25899', 'initiator-iops': '295', 'initiator-lun-in-use-count': '0', 'initiator-port': '0b', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-01', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '25899', 'target-iops': '295', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb31000000', 'target-wwpn': '530191aabb310000', 'tpgn': '68', 'vmdisk-device-id': '8'}, {'array-name': 'NETAPP_PHA_1', 'disk-name': 'cluster-unassigned-test-02:0d.15', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:55434741:6B334E67:69347333:00000000:00000000:00000000', 'initiator-io-kbps': '28987', 'initiator-iops': '478', 'initiator-lun-in-use-count': '0', 'initiator-port': '0d', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-02', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '14677', 'target-iops': '238', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb31000000', 'target-wwpn': '530291aabb310000', 'tpgn': '27', 'vmdisk-device-id': '15'}, {'array-name': 'NETAPP_PHA_1', 'disk-name': 'cluster-unassigned-test-02:0d.20', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:55434741:6B334E67:69347333:00000000:00000000:00000000', 'initiator-io-kbps': '28987', 'initiator-iops': '478', 'initiator-lun-in-use-count': '0', 'initiator-port': '0d', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-02', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '14310', 'target-iops': '239', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb31000000', 'target-wwpn': '530891aabb310000', 'tpgn': '78', 'vmdisk-device-id': None}]}, 'disk-raid-info': {'container-type': 'unassigned', 'disk-outage-info': {'is-in-fdr': 'false', 'reason': 'unassigned'}, 'disk-uid': '4E455441:50502020:30303030:30303030:55434741:6B334E67:69347333:00000000:00000000:00000000', 'physical-blocks': '13107199', 'position': 'present', 'standard-disk-type': 'ssd'}, 'disk-stats-info': {'average-latency': '2344531894', 'bytes-per-sector': '512', 'disk-io-kbps': '0', 'disk-iops': '0', 'disk-uid': '4E455441:50502020:30303030:30303030:55434741:6B334E67:69347333:00000000:00000000:00000000', 'path-error-count': '0', 'power-on-time-interval': '0', 'sectors-read': '0', 'sectors-written': '0'}, 'disk-uid': '4E455441:50502020:30303030:30303030:55434741:6B334E67:69347333:00000000:00000000:00000000'}, {'disk-inventory-info': {'bytes-per-sector': '512', 'capacity-sectors': '104857592', 'checksum-compatibility': 'advanced_zoned', 'disk-class': 'solid-state', 'disk-cluster-name': 'NET-1.3', 'disk-type': 'SSD', 'disk-uid': '4E455441:50502020:30303030:30303030:7A33576D:566F7672:4E363434:00000000:00000000:00000000', 'firmware-revision': '0001', 'grown-defect-list-count': '0', 'health-monitor-time-interval': '0', 'import-in-progress': 'false', 'is-dynamically-qualified': 'false', 'is-foreign': 'false', 'is-multidisk-carrier': 'false', 'is-shared': 'false', 'location': 'cluster-unassigned-test-01', 'location-id': '2443885361', 'model': 'PHA-DISK', 'reservation-key': '0x0', 'reservation-type': 'none', 'right-size-sectors': '104844800', 'serial-number': '00000000z3WmVovrN644', 'vendor': 'NETAPP', 'virtual-machine-disk-info': {'vmhost-error': 'Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the "system node virtual-machine hypervisor modify-credentials" command.'}, 'vmdisk-target-address': 'Scsi bus=1, target=1, lun=0'}, 'disk-metrocluster-info': {'is-local-attach': 'true'}, 'disk-name': 'NET-1.3', 'disk-ownership-info': {'disk-uid': '4E455441:50502020:30303030:30303030:7A33576D:566F7672:4E363434:00000000:00000000:00000000', 'is-failed': 'false', 'reserved-by-node-id': '0'}, 'disk-paths': {'disk-path-info': [{'array-name': 'NETAPP_PHA_1', 'disk-name': 'cluster-unassigned-test-01:0b.9', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:7A33576D:566F7672:4E363434:00000000:00000000:00000000', 'initiator-io-kbps': '25899', 'initiator-iops': '295', 'initiator-lun-in-use-count': '0', 'initiator-port': '0b', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-01', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '25899', 'target-iops': '295', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb31000000', 'target-wwpn': '530191aabb310000', 'tpgn': '68', 'vmdisk-device-id': '9'}, {'array-name': 'NETAPP_PHA_1', 'disk-name': 'cluster-unassigned-test-02:0d.16', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:7A33576D:566F7672:4E363434:00000000:00000000:00000000', 'initiator-io-kbps': '28987', 'initiator-iops': '478', 'initiator-lun-in-use-count': '0', 'initiator-port': '0d', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-02', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '14677', 'target-iops': '238', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb31000000', 'target-wwpn': '530291aabb310000', 'tpgn': '27', 'vmdisk-device-id': '16'}, {'array-name': 'NETAPP_PHA_1', 'disk-name': 'cluster-unassigned-test-02:0d.21', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:7A33576D:566F7672:4E363434:00000000:00000000:00000000', 'initiator-io-kbps': '28987', 'initiator-iops': '478', 'initiator-lun-in-use-count': '0', 'initiator-port': '0d', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-02', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '14310', 'target-iops': '239', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb31000000', 'target-wwpn': '530891aabb310000', 'tpgn': '78', 'vmdisk-device-id': None}]}, 'disk-raid-info': {'container-type': 'unassigned', 'disk-outage-info': {'is-in-fdr': 'false', 'reason': 'unassigned'}, 'disk-uid': '4E455441:50502020:30303030:30303030:7A33576D:566F7672:4E363434:00000000:00000000:00000000', 'physical-blocks': '13107199', 'position': 'present', 'standard-disk-type': 'ssd'}, 'disk-stats-info': {'average-latency': '2344531894', 'bytes-per-sector': '512', 'disk-io-kbps': '0', 'disk-iops': '0', 'disk-uid': '4E455441:50502020:30303030:30303030:7A33576D:566F7672:4E363434:00000000:00000000:00000000', 'path-error-count': '0', 'power-on-time-interval': '0', 'sectors-read': '0', 'sectors-written': '0'}, 'disk-uid': '4E455441:50502020:30303030:30303030:7A33576D:566F7672:4E363434:00000000:00000000:00000000'}]]
2023-01-16 15:00:56,442| INFO| 2687|12022|logger.py|132|cluster.py|2820:_assign_disks| Cluster [cluster-unassigned-test]: unassigned disks partner data on node (cluster-unassigned-test-01): [[{'disk-inventory-info': {'bytes-per-sector': '512', 'capacity-sectors': '104857592', 'checksum-compatibility': 'advanced_zoned', 'disk-class': 'solid-state', 'disk-cluster-name': 'NET-3.2', 'disk-type': 'SSD', 'disk-uid': '4E455441:50502020:30303030:30303030:6B323967:434D7334:61346734:00000000:00000000:00000000', 'firmware-revision': '0001', 'grown-defect-list-count': '0', 'health-monitor-time-interval': '0', 'import-in-progress': 'false', 'is-dynamically-qualified': 'false', 'is-foreign': 'false', 'is-multidisk-carrier': 'false', 'is-shared': 'false', 'location': 'cluster-unassigned-test-02', 'location-id': '2443885401', 'model': 'PHA-DISK', 'reservation-key': '0x0', 'reservation-type': 'none', 'right-size-sectors': '104844800', 'serial-number': '00000000k29gCMs4a4g4', 'vendor': 'NETAPP', 'virtual-machine-disk-info': {'vmhost-error': 'Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the "system node virtual-machine hypervisor modify-credentials" command.'}, 'vmdisk-target-address': 'Scsi bus=1, target=1, lun=0'}, 'disk-metrocluster-info': {'is-local-attach': 'true'}, 'disk-name': 'NET-3.2', 'disk-ownership-info': {'disk-uid': '4E455441:50502020:30303030:30303030:6B323967:434D7334:61346734:00000000:00000000:00000000', 'is-failed': 'false', 'reserved-by-node-id': '0'}, 'disk-paths': {'disk-path-info': [{'array-name': 'NETAPP_PHA_3', 'disk-name': 'cluster-unassigned-test-01:0d.19', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:6B323967:434D7334:61346734:00000000:00000000:00000000', 'initiator-io-kbps': '26280', 'initiator-iops': '435', 'initiator-lun-in-use-count': '0', 'initiator-port': '0d', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-01', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '13192', 'target-iops': '217', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb59000000', 'target-wwpn': '530891aabb590000', 'tpgn': '52', 'vmdisk-device-id': '19'}, {'array-name': 'NETAPP_PHA_3', 'disk-name': 'cluster-unassigned-test-01:0d.20', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:6B323967:434D7334:61346734:00000000:00000000:00000000', 'initiator-io-kbps': '26280', 'initiator-iops': '435', 'initiator-lun-in-use-count': '0', 'initiator-port': '0d', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-01', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '13088', 'target-iops': '217', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb59000000', 'target-wwpn': '530291aabb590000', 'tpgn': '1', 'vmdisk-device-id': '9'}, {'array-name': 'NETAPP_PHA_3', 'disk-name': 'cluster-unassigned-test-02:0b.9', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:6B323967:434D7334:61346734:00000000:00000000:00000000', 'initiator-io-kbps': '28479', 'initiator-iops': '323', 'initiator-lun-in-use-count': '0', 'initiator-port': '0b', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-02', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '28479', 'target-iops': '323', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb59000000', 'target-wwpn': '530191aabb590000', 'tpgn': '42', 'vmdisk-device-id': None}]}, 'disk-raid-info': {'container-type': 'unassigned', 'disk-outage-info': {'is-in-fdr': 'false', 'reason': 'unassigned'}, 'disk-uid': '4E455441:50502020:30303030:30303030:6B323967:434D7334:61346734:00000000:00000000:00000000', 'physical-blocks': '13107199', 'position': 'present', 'standard-disk-type': 'ssd'}, 'disk-stats-info': {'average-latency': '2344531894', 'bytes-per-sector': '512', 'disk-io-kbps': '0', 'disk-iops': '0', 'disk-uid': '4E455441:50502020:30303030:30303030:6B323967:434D7334:61346734:00000000:00000000:00000000', 'path-error-count': '0', 'power-on-time-interval': '0', 'sectors-read': '0', 'sectors-written': '0'}, 'disk-uid': '4E455441:50502020:30303030:30303030:6B323967:434D7334:61346734:00000000:00000000:00000000'}, {'disk-inventory-info': {'bytes-per-sector': '512', 'capacity-sectors': '104857592', 'checksum-compatibility': 'advanced_zoned', 'disk-class': 'solid-state', 'disk-cluster-name': 'NET-3.3', 'disk-type': 'SSD', 'disk-uid': '4E455441:50502020:30303030:30303030:7A43486F:654C4B73:496F3833:00000000:00000000:00000000', 'firmware-revision': '0001', 'grown-defect-list-count': '0', 'health-monitor-time-interval': '0', 'import-in-progress': 'false', 'is-dynamically-qualified': 'false', 'is-foreign': 'false', 'is-multidisk-carrier': 'false', 'is-shared': 'false', 'location': 'cluster-unassigned-test-02', 'location-id': '2443885401', 'model': 'PHA-DISK', 'reservation-key': '0x0', 'reservation-type': 'none', 'right-size-sectors': '104844800', 'serial-number': '00000000zCHoeLKsIo83', 'vendor': 'NETAPP', 'virtual-machine-disk-info': {'vmhost-error': 'Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the "system node virtual-machine hypervisor modify-credentials" command.'}, 'vmdisk-target-address': 'Scsi bus=0, target=1, lun=0'}, 'disk-metrocluster-info': {'is-local-attach': 'true'}, 'disk-name': 'NET-3.3', 'disk-ownership-info': {'disk-uid': '4E455441:50502020:30303030:30303030:7A43486F:654C4B73:496F3833:00000000:00000000:00000000', 'is-failed': 'false', 'reserved-by-node-id': '0'}, 'disk-paths': {'disk-path-info': [{'array-name': 'NETAPP_PHA_3', 'disk-name': 'cluster-unassigned-test-01:0d.18', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:7A43486F:654C4B73:496F3833:00000000:00000000:00000000', 'initiator-io-kbps': '26280', 'initiator-iops': '435', 'initiator-lun-in-use-count': '0', 'initiator-port': '0d', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-01', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '13192', 'target-iops': '217', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb59000000', 'target-wwpn': '530891aabb590000', 'tpgn': '52', 'vmdisk-device-id': '18'}, {'array-name': 'NETAPP_PHA_3', 'disk-name': 'cluster-unassigned-test-01:0d.17', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:7A43486F:654C4B73:496F3833:00000000:00000000:00000000', 'initiator-io-kbps': '26280', 'initiator-iops': '435', 'initiator-lun-in-use-count': '0', 'initiator-port': '0d', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-01', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '13088', 'target-iops': '217', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb59000000', 'target-wwpn': '530291aabb590000', 'tpgn': '1', 'vmdisk-device-id': '8'}, {'array-name': 'NETAPP_PHA_3', 'disk-name': 'cluster-unassigned-test-02:0b.8', 'disk-port': 'A', 'disk-port-name': 'SA:A', 'disk-uid': '4E455441:50502020:30303030:30303030:7A43486F:654C4B73:496F3833:00000000:00000000:00000000', 'initiator-io-kbps': '28479', 'initiator-iops': '323', 'initiator-lun-in-use-count': '0', 'initiator-port': '0b', 'initiator-port-speed': '-', 'initiator-side-switch-port': 'N/A', 'lun-io-kbps': '0', 'lun-iops': '0', 'lun-number': '0', 'lun-path-use-state': 'INU', 'node': 'cluster-unassigned-test-02', 'path-io-kbps': '0', 'path-iops': '0', 'path-link-errors': '0', 'path-lun-in-use-count': '0', 'path-quality': '0', 'preferred-target-port': 'false', 'target-io-kbps': '28479', 'target-iops': '323', 'target-iqn': 'N/A', 'target-lun-in-use-count': '0', 'target-port-access-state': 'AO', 'target-side-switch-port': 'N/A', 'target-wwnn': '5391aabb59000000', 'target-wwpn': '530191aabb590000', 'tpgn': '42', 'vmdisk-device-id': None}]}, 'disk-raid-info': {'container-type': 'unassigned', 'disk-outage-info': {'is-in-fdr': 'false', 'reason': 'unassigned'}, 'disk-uid': '4E455441:50502020:30303030:30303030:7A43486F:654C4B73:496F3833:00000000:00000000:00000000', 'physical-blocks': '13107199', 'position': 'present', 'standard-disk-type': 'ssd'}, 'disk-stats-info': {'average-latency': '2344531894', 'bytes-per-sector': '512', 'disk-io-kbps': '0', 'disk-iops': '0', 'disk-uid': '4E455441:50502020:30303030:30303030:7A43486F:654C4B73:496F3833:00000000:00000000:00000000', 'path-error-count': '0', 'power-on-time-interval': '0', 'sectors-read': '0', 'sectors-written': '0'}, 'disk-uid': '4E455441:50502020:30303030:30303030:7A43486F:654C4B73:496F3833:00000000:00000000:00000000'}]]
2023-01-16 15:00:56,444| INFO| 2687|12022|logger.py|132|cluster.py|2822:_assign_disks| Cluster [cluster-unassigned-test]: number of unassigned disks on node (cluster-unassigned-test-01): [2]
2023-01-16 15:00:56,446| INFO| 2687|12022|logger.py|132|cluster.py|2824:_assign_disks| Cluster [cluster-unassigned-test]: number of unassigned disks on node (cluster-unassigned-test-02): [2]
2023-01-16 15:00:56,603| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "disk-sanown-assign"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:56,604| INFO| 2687|12022|logger.py|132|ClusterZapis.py|135:z_disk_assign| Cluster [cluster-unassigned-test]: Success assigning disk(NET-1.2) to node(cluster-unassigned-test-01) on pool (0)
2023-01-16 15:00:56,769| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "disk-sanown-assign"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:56,771| INFO| 2687|12022|logger.py|132|ClusterZapis.py|135:z_disk_assign| Cluster [cluster-unassigned-test]: Success assigning disk(NET-3.2) to node(cluster-unassigned-test-01) on pool (1)
2023-01-16 15:00:56,934| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "disk-sanown-assign"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:56,935| INFO| 2687|12022|logger.py|132|ClusterZapis.py|135:z_disk_assign| Cluster [cluster-unassigned-test]: Success assigning disk(NET-1.3) to node(cluster-unassigned-test-02) on pool (1)
2023-01-16 15:00:57,088| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "disk-sanown-assign"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:57,090| INFO| 2687|12022|logger.py|132|ClusterZapis.py|135:z_disk_assign| Cluster [cluster-unassigned-test]: Success assigning disk(NET-3.3) to node(cluster-unassigned-test-02) on pool (0)
2023-01-16 15:00:57,107| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=431, request_id='12022', time='2023-01-16 15:00:57.093844', evtype='ClusterCreateDiskAssignmentCompleted', category='cluster', level='Info', detail='ONTAP disk assignments completed successfully.')>
2023-01-16 15:00:57,109|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:00:57.109706]> [failed:False] ONTAP disk assignments completed successfully.
2023-01-16 15:00:57,122| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=432, request_id='12022', time='2023-01-16 15:00:57.114007', evtype='ONTAPClusterCreateCompleted', category='cluster', level='Info', detail='ONTAP cluster create complete. All nodes successfully joined cluster "cluster-unassigned-test".')>
2023-01-16 15:00:57,123|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:00:57.123761]> [failed:False] ONTAP cluster create complete. All nodes successfully joined cluster "cluster-unassigned-test".
2023-01-16 15:00:57,125| INFO| 2687|12022|logger.py|132|cluster_tasks.py|288:create_cluster| Cluster [cluster-unassigned-test]: checking for any nodes that moved to a new host...
2023-01-16 15:00:57,152|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|137:is_vm_on_host| is_vm_on_host executing
2023-01-16 15:00:57,154|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|173:vm_get| vm_get executing
2023-01-16 15:00:57,464|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-005.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d1009a-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-01","obj_id":"vm-11017","state":"poweredon","vm_uid":"5016042c-0798-9f35-3c46-fed02c5ee585"}
2023-01-16 15:00:57,491|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|137:is_vm_on_host| is_vm_on_host executing
2023-01-16 15:00:57,492|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|173:vm_get| vm_get executing
2023-01-16 15:00:57,805|DEBUG| 2687|12022|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-006.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"e0d06a86-95ab-11ed-bd72-000c29e96265","node_uuid":"e0d228bc-95ab-11ed-bd72-000c29e96265"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"cluster-unassigned-test-02","obj_id":"vm-11018","state":"poweredon","vm_uid":"5016e28c-4aed-f343-96db-8ab9e93939ac"}
2023-01-16 15:00:57,808| INFO| 2687|12022|logger.py|132|cluster.py|2925:configure_new_cluster| Cluster [cluster-unassigned-test]: setting up new cluster ...
2023-01-16 15:00:57,822| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=433, request_id='12022', time='2023-01-16 15:00:57.811916', evtype='ONTAPClusterSetupInProgress', category='cluster', level='Info', detail='New cluster "cluster-unassigned-test" setup in progress.')>
2023-01-16 15:00:57,823|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:00:57.823455]> [failed:False] New cluster "cluster-unassigned-test" setup in progress.
2023-01-16 15:00:57,840|DEBUG| 2687|12022|fault.py|147|Unable to find a registered fault for zapi.net_dns_create.
2023-01-16 15:00:57,963| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "net-dns-create"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:57,965| INFO| 2687|12022|logger.py|132|cluster.py|2898:_set_dns_config| Cluster [cluster-unassigned-test]: completed set dns info (['10.224.223.135'], ['gdl.englab.netapp.com'])
2023-01-16 15:00:57,967|DEBUG| 2687|12022|logger.py|132|cluster.py|2933:configure_new_cluster| DNS setup succeeded
2023-01-16 15:00:57,992|DEBUG| 2687|12022|fault.py|147|Unable to find a registered fault for zapi.ntp_server_create.
2023-01-16 15:00:58,054| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "ntp-server-create"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:58,055| INFO| 2687|12022|logger.py|132|cluster.py|2916:_set_ntp_config| Cluster [cluster-unassigned-test]: set ntp server (10.60.248.183)
2023-01-16 15:00:58,056|DEBUG| 2687|12022|logger.py|132|cluster.py|2945:configure_new_cluster| NTP setup succeeded
2023-01-16 15:00:58,066|DEBUG| 2687|12022|logger.py|132|cluster.py|2958:configure_new_cluster| Split broadcast domain config skipped
2023-01-16 15:00:58,361|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "license-v2-capacity-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:58,608|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "license-v2-capacity-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:58,855| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "cluster-application-record-create"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:59,752|DEBUG| 2687|12022|logger.py|132|ASUP.py|50:get_asup_config| asup config to send to ontap: ({'asup_enabled': 'true', 'local_collection': 'true', 'destination_url': 'https://testbed.netapp.com/put/AsupPut', 'proxy_url': ''})
2023-01-16 15:00:59,763|DEBUG| 2687|12022|fault.py|147|Unable to find a registered fault for zapi.autosupport_config_modify.
2023-01-16 15:00:59,816| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "autosupport-config-modify"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:00:59,831|DEBUG| 2687|12022|fault.py|147|Unable to find a registered fault for zapi.autosupport_config_modify.
2023-01-16 15:01:00,001| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "autosupport-config-modify"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:01:00,003| INFO| 2687|12022|logger.py|132|ClusterZapis.py|439:z_set_asup_config| Cluster [cluster-unassigned-test]: asup config set on all nodes in cluster
2023-01-16 15:01:00,119| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "autosupport-invoke-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:01:00,121| INFO| 2687|12022|logger.py|132|ClusterZapis.py|463:z_trigger_ontap_asup| Cluster [cluster-unassigned-test]: autosupport_invoke zapi output: ([[{'node-name': 'cluster-unassigned-test-01', 'seq-num': '2'}, {'node-name': 'cluster-unassigned-test-02', 'seq-num': '2'}], []])
2023-01-16 15:01:00,133| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=434, request_id='12022', time='2023-01-16 15:01:00.125265', evtype='ClusterONTAPAsupSent', category='cluster', level='Info', detail='Cluster ONTAP AutoSupport request triggered.')>
2023-01-16 15:01:00,134|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:00.134269]> [failed:False] Cluster ONTAP AutoSupport request triggered.
2023-01-16 15:01:00,196| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=435, request_id='12022', time='2023-01-16 15:01:00.188568', evtype='ClusterOFFTAPAsupSent', category='cluster', level='Info', detail='Cluster ONTAP Select Deploy AutoSupport request triggered, seq_num:7.')>
2023-01-16 15:01:00,196|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:00.196786]> [failed:False] Cluster ONTAP Select Deploy AutoSupport request triggered, seq_num:7.
2023-01-16 15:01:00,223|DEBUG| 2687|12022|coroutine.py|190|Registered Myself: current tasks [{140449564633016: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>}]
2023-01-16 15:01:00,320|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:00.320068]> [failed:False] Host "sdot-b200-005.gdl.englab.netapp.com" loading cache
2023-01-16 15:01:00,518| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "system-cli"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:01:00,519|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|908:z_system_cli_sidl_enable_at_aggr_create_on| Cluster [cluster-unassigned-test]: System CLI, SIDL enable at aggregate creation is ON
2023-01-16 15:01:00,532| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=436, request_id='12022', time='2023-01-16 15:01:00.522178', evtype='SIDLAggrCreateEnabledOnNode', category='node', level='Info', detail='SIDL has been enabled on node "cluster-unassigned-test-01".')>
2023-01-16 15:01:00,533|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:00.533101]> [failed:False] SIDL has been enabled on node "cluster-unassigned-test-01".
2023-01-16 15:01:00,673| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "system-cli"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:01:00,674|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|908:z_system_cli_sidl_enable_at_aggr_create_on| Cluster [cluster-unassigned-test]: System CLI, SIDL enable at aggregate creation is ON
2023-01-16 15:01:00,683| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=437, request_id='12022', time='2023-01-16 15:01:00.676161', evtype='SIDLAggrCreateEnabledOnNode', category='node', level='Info', detail='SIDL has been enabled on node "cluster-unassigned-test-02".')>
2023-01-16 15:01:00,684|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:00.684250]> [failed:False] SIDL has been enabled on node "cluster-unassigned-test-02".
2023-01-16 15:01:00,701|DEBUG| 2687|12022|fault.py|147|Unable to find a registered fault for zapi.security_login_modify_password.
2023-01-16 15:01:00,778| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "security-login-modify-password"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:01:00,780| INFO| 2687|12022|logger.py|132|cluster.py|3236:_set_new_password| Cluster [cluster-unassigned-test]: set admin_password initiated
2023-01-16 15:01:00,829| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "system-get-version"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:01:00,831|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|846:z_system_get_version| Cluster [cluster-unassigned-test]: system get version {'generation': '9', 'major': '13', 'minor': '1'}
2023-01-16 15:01:00,843| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=438, request_id='12022', time='2023-01-16 15:01:00.835396', evtype='ONTAPAdminPasswordResetCompleted', category='cluster', level='Info', detail='ONTAP default password reset complete.')>
2023-01-16 15:01:00,844|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:00.844501]> [failed:False] ONTAP default password reset complete.
2023-01-16 15:01:00,858| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=439, request_id='12022', time='2023-01-16 15:01:00.850923', evtype='ONTAPClusterSetupCompleted', category='cluster', level='Info', detail='New cluster "cluster-unassigned-test" setup completed successfully.')>
2023-01-16 15:01:00,858|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:00.858807]> [failed:False] New cluster "cluster-unassigned-test" setup completed successfully.
2023-01-16 15:01:00,859| INFO| 2687|12022|logger.py|132|cluster.py|3045:configure_new_cluster| Cluster [cluster-unassigned-test]: finished setup of new cluster, returning True
2023-01-16 15:01:00,860| INFO| 2687|12022|logger.py|132|cluster_tasks.py|298:create_cluster| Cluster [cluster-unassigned-test]: cluster created and setup successfully
2023-01-16 15:01:00,884| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=440, request_id='12022', time='2023-01-16 15:01:00.877534', evtype='FetchingONTAPDiskNames', category='node', level='Info', detail='Fetching ONTAP disk names for node "cluster-unassigned-test-01"')>
2023-01-16 15:01:00,885|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:00.885475]> [failed:False] Fetching ONTAP disk names for node "cluster-unassigned-test-01"
2023-01-16 15:01:00,894|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-01-16 15:01:01,182|DEBUG| 2687|12022|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01.vmdk","pool":"Datastore_05","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01_1.vmdk","pool":"Datastore_05","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"cluster-unassigned-test-01_2.vmdk","pool":"Datastore_05","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01_3.vmdk","pool":"Datastore_05","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-01_4.vmdk","pool":"Datastore_05","total_diskspace":69632},{"capacity":51200,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"1","unit":"0"},"name":"cluster-unassigned-test-01_5.vmdk","pool":"Datastore_05","total_diskspace":51200},{"capacity":51200,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"1","unit":"0"},"name":"cluster-unassigned-test-01_6.vmdk","pool":"Datastore_05","total_diskspace":51200}]
2023-01-16 15:01:01,185|DEBUG| 2687|12022|logger.py|132|node.py|5310:get_ontap_disk_names| Disks to find: ['cluster-unassigned-test-01_5.vmdk', 'cluster-unassigned-test-01_6.vmdk']
2023-01-16 15:01:01,267|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "storage-disk-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:01:01,268|DEBUG| 2687|12022|logger.py|132|node.py|5316:get_ontap_disk_names| unsupported_disks list: []
2023-01-16 15:01:01,269|DEBUG| 2687|12022|logger.py|132|node.py|5319:get_ontap_disk_names| Disks to be attached_after_checking ['cluster-unassigned-test-01_5.vmdk', 'cluster-unassigned-test-01_6.vmdk']
2023-01-16 15:01:01,270|DEBUG| 2687|12022|logger.py|132|node.py|5227:_get_ontap_disk_info| vmdisk-target-addr is unknown
2023-01-16 15:01:01,272|DEBUG| 2687|12022|logger.py|132|node.py|5264:_get_ontap_disk_info| Matched disk NET-1.3 with diskpathname 0b.9
2023-01-16 15:01:01,273|DEBUG| 2687|12022|logger.py|132|node.py|5227:_get_ontap_disk_info| vmdisk-target-addr is unknown
2023-01-16 15:01:01,274|DEBUG| 2687|12022|logger.py|132|node.py|5264:_get_ontap_disk_info| Matched disk NET-1.2 with diskpathname 0b.8
2023-01-16 15:01:01,275|DEBUG| 2687|12022|logger.py|132|node.py|5352:set_data_disk_ontap_disk_names| ONTAP disks: {'cluster-unassigned-test-01_5.vmdk': {'ontap_name': 'NET-1.2', 'found': True, 'more_disk_info': {'diskpath_name': '0b.8', 'container_type': 'spare', 'capacity': 53687087104}}, 'cluster-unassigned-test-01_6.vmdk': {'ontap_name': 'NET-1.3', 'found': True, 'more_disk_info': {'diskpath_name': '0b.9', 'container_type': 'spare', 'capacity': 53687087104}}}
2023-01-16 15:01:01,297| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=441, request_id='12022', time='2023-01-16 15:01:01.290251', evtype='FetchingONTAPDiskNames', category='node', level='Info', detail='Fetching ONTAP disk names for node "cluster-unassigned-test-02"')>
2023-01-16 15:01:01,297|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:01.297830]> [failed:False] Fetching ONTAP disk names for node "cluster-unassigned-test-02"
2023-01-16 15:01:01,305|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-01-16 15:01:01,555|DEBUG| 2687|12022|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02.vmdk","pool":"Datastore_06","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02_1.vmdk","pool":"Datastore_06","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"cluster-unassigned-test-02_2.vmdk","pool":"Datastore_06","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02_3.vmdk","pool":"Datastore_06","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"cluster-unassigned-test-02_4.vmdk","pool":"Datastore_06","total_diskspace":69632},{"capacity":51200,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"1","unit":"0"},"name":"cluster-unassigned-test-02_5.vmdk","pool":"Datastore_06","total_diskspace":51200},{"capacity":51200,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"1","unit":"0"},"name":"cluster-unassigned-test-02_6.vmdk","pool":"Datastore_06","total_diskspace":51200}]
2023-01-16 15:01:01,558|DEBUG| 2687|12022|logger.py|132|node.py|5310:get_ontap_disk_names| Disks to find: ['cluster-unassigned-test-02_5.vmdk', 'cluster-unassigned-test-02_6.vmdk']
2023-01-16 15:01:01,637|DEBUG| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "storage-disk-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:01:01,638|DEBUG| 2687|12022|logger.py|132|node.py|5316:get_ontap_disk_names| unsupported_disks list: []
2023-01-16 15:01:01,639|DEBUG| 2687|12022|logger.py|132|node.py|5319:get_ontap_disk_names| Disks to be attached_after_checking ['cluster-unassigned-test-02_5.vmdk', 'cluster-unassigned-test-02_6.vmdk']
2023-01-16 15:01:01,640|DEBUG| 2687|12022|logger.py|132|node.py|5227:_get_ontap_disk_info| vmdisk-target-addr is unknown
2023-01-16 15:01:01,642|DEBUG| 2687|12022|logger.py|132|node.py|5264:_get_ontap_disk_info| Matched disk NET-3.2 with diskpathname 0b.9
2023-01-16 15:01:01,642|DEBUG| 2687|12022|logger.py|132|node.py|5227:_get_ontap_disk_info| vmdisk-target-addr is unknown
2023-01-16 15:01:01,644|DEBUG| 2687|12022|logger.py|132|node.py|5264:_get_ontap_disk_info| Matched disk NET-3.3 with diskpathname 0b.8
2023-01-16 15:01:01,645|DEBUG| 2687|12022|logger.py|132|node.py|5352:set_data_disk_ontap_disk_names| ONTAP disks: {'cluster-unassigned-test-02_5.vmdk': {'ontap_name': 'NET-3.3', 'found': True, 'more_disk_info': {'diskpath_name': '0b.8', 'container_type': 'spare', 'capacity': 53687087104}}, 'cluster-unassigned-test-02_6.vmdk': {'ontap_name': 'NET-3.2', 'found': True, 'more_disk_info': {'diskpath_name': '0b.9', 'container_type': 'spare', 'capacity': 53687087104}}}
2023-01-16 15:01:01,689| INFO| 2687|12022|logger.py|132|ZAPI.py|241:invoke|
API: "system-get-version"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-01-16 15:01:01,691|DEBUG| 2687|12022|logger.py|132|ClusterZapis.py|846:z_system_get_version| Cluster [cluster-unassigned-test]: system get version {'generation': '9', 'major': '13', 'minor': '1'}
2023-01-16 15:01:01,702| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=442, request_id='12022', time='2023-01-16 15:01:01.694952', evtype='ClusterVersionRefreshed', category='cluster', level='Info', detail='ONTAP version changed from "devN_230101_0200-vidconsole" to "9.13.1-vidconsole".')>
2023-01-16 15:01:01,703|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:01.703631]> [failed:False] ONTAP version changed from "devN_230101_0200-vidconsole" to "9.13.1-vidconsole".
2023-01-16 15:01:01,724| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=443, request_id='12022', time='2023-01-16 15:01:01.711515', evtype='ClusterDeployCompleted', category='cluster', level='Info', detail='Cluster "cluster-unassigned-test" is deployed and ready for use.')>
2023-01-16 15:01:01,724|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:01.724787]> [failed:False] Cluster "cluster-unassigned-test" is deployed and ready for use.
2023-01-16 15:01:07,981|DEBUG| 2687|12022|rest.py|173|response body: {"available_pdisks":[{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b74","total_diskspace":1048593,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656931","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b50636a","total_diskspace":1048593,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a383059","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38305a","total_diskspace":3145728,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}],"available_pools":[{"allocation":3767592,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":25058513,"status":"Online"},{"allocation":1991372,"capacity":2072320,"device_type":"SSD","location_type":"external","name":"Datastore_006","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)"],"pool_type":"VMFS-6","pool_uid":"datastore-2417","provisioned_space":1993808,"status":"Online"},{"allocation":1115085,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_05","pdisks":["NETAPP Fibre Channel Disk (naa.600a098038303042542b51447a38305a)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b50636a)"],"pool_type":"VMFS-6","pool_uid":"datastore-2421","provisioned_space":1684248,"status":"Online"}],"available_portgroups":[{"name":"VM Network","network_uid":"network-15","vlanid":"0","vswitch":"vSwitch0"},{"name":"ONTAP_Internal","network_uid":"network-40","vlanid":"0","vswitch":"vSwitch1"},{"name":"ONTAP_External","network_uid":"network-41","vlanid":"2480","vswitch":"vSwitch1"},{"name":"ONTAP_Management","network_uid":"network-39","vlanid":"0","vswitch":"vSwitch1"}],"available_vms":[],"available_vswitches":[{"associated_adapter":["vmnic0"],"available_ports":7916,"mtu":"1500","name":"vSwitch0","portgroups":["VM Network","Management Network"],"total_ports":7936,"type":"Standard vSwitch"},{"associated_adapter":["vmnic2","vmnic3"],"available_ports":7916,"mtu":"9000","name":"vSwitch1","portgroups":["ONTAP_Internal","ONTAP_External","ONTAP_Management"],"total_ports":7936,"type":"Standard vSwitch"}],"host_configuration":{"host_hardware_config":{"cores_in_use":3,"cpu_model":"Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz","cpu_speed_mhz":2394,"hardware_vendor":"Cisco Systems Inc","mem_size_mb":130960,"mem_usage_mb":27614,"num_cpu_cores":20,"num_hbas":2,"num_nics":4,"number_cpus":40,"product_version":"VMware ESXi 7.0.0"},"hypervisor_info":{"esx":{"ESX_license":"VMware vSphere 7 Enterprise Plus","SDK_version":"6.7.0","api_version":"7.0.0.0","bios_release_date":"2018-07-11T00:00:00Z","bios_version":"B200M4.4.0.1c.0.0711181418","firewall_defaults_in":"blocked","firewall_defaults_out":"blocked","host":"sdot-b200-005.gdl.englab.netapp.com","hyperthreading":"active","hypervisor":"VMware ESXi 7.0.0 build-15843807","hypervisor_build":"15843807","hypervisor_name":"VMware ESXi","hypervisor_version":"7.0.0","model":"UCSB-B200-M4","moid":"host-2412:3025b4b7-b510-4852-98ed-38668b194b85","power_mgmt_policy":"Balanced","power_mgmt_technology":"ACPI P-states","reboot_required":"No","server":"sdot-b200-010a.gdl.englab.netapp.com","user":"administrator@vsphere.local","uuid":"00000000-0000-0000-0000-0025b5270050"}},"network_adapters":[{"associated_vswitch":"vSwitch0","enabled":true,"mac":"00:25:b5:27:00:51","name":"vmnic0","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"None","enabled":true,"mac":"00:25:b5:27:00:52","name":"vmnic1","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"vSwitch1","enabled":true,"mac":"00:25:b5:27:00:53","name":"vmnic2","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"vSwitch1","enabled":true,"mac":"00:25:b5:27:00:54","name":"vmnic3","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"}],"storage_adapters":[{"description":"(0000:08:00.0) Cisco UCS VIC Fnic Controller","logical_name":"vmhba0","product":"Cisco UCS VIC Fnic Controller"},{"description":"(0000:0f:00.0) Cisco UCS VIC Fnic Controller","logical_name":"vmhba1","product":"Cisco UCS VIC Fnic Controller"}]}}
2023-01-16 15:01:07,993|DEBUG| 2687|12022|logger.py|132|host.py|645:get_host_support| {'storage_pools': [{'allocation': 3767592, 'capacity': 16934502, 'device_type': 'N/A', 'location_type': 'external', 'name': 'sdot_nfs_ds1', 'pdisks': ['N/A'], 'pool_type': 'NFS', 'pool_uid': 'datastore-2250', 'provisioned_space': 25058513, 'status': 'Online'}, {'allocation': 1991372, 'capacity': 2072320, 'device_type': 'SSD', 'location_type': 'external', 'name': 'Datastore_006', 'pdisks': ['NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)'], 'pool_type': 'VMFS-6', 'pool_uid': 'datastore-2417', 'provisioned_space': 1993808, 'status': 'Online'}, {'allocation': 1115085, 'capacity': 4193792, 'device_type': 'SSD', 'location_type': 'external', 'name': 'Datastore_05', 'pdisks': ['NETAPP Fibre Channel Disk (naa.600a098038303042542b51447a38305a)', 'NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b50636a)'], 'pool_type': 'VMFS-6', 'pool_uid': 'datastore-2421', 'provisioned_space': 1684248, 'status': 'Online'}], 'storage_pdisks': [{'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a09805176304d6d5d4b384b506b74', 'total_diskspace': 1048593, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724e7a46', 'total_diskspace': 1048593, 'used_by': 'Datastore_006'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763671763f4b395a656931', 'total_diskspace': 20480, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f3545', 'total_diskspace': 512078, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f3547', 'total_diskspace': 512078, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f3548', 'total_diskspace': 512078, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f354a', 'total_diskspace': 512078, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a09805176304d6d5d4b384b50636a', 'total_diskspace': 1048593, 'used_by': 'Datastore_05'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098038303042542b51447a383059', 'total_diskspace': 10240, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098038303042542b51447a38305a', 'total_diskspace': 3145728, 'used_by': 'Datastore_05'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f352d', 'total_diskspace': 512078, 'used_by': ''}], 'network_portgroups': [{'name': 'VM Network', 'network_uid': 'network-15', 'vlanid': '0', 'vswitch': 'vSwitch0'}, {'name': 'ONTAP_Internal', 'network_uid': 'network-40', 'vlanid': '0', 'vswitch': 'vSwitch1'}, {'name': 'ONTAP_External', 'network_uid': 'network-41', 'vlanid': '2480', 'vswitch': 'vSwitch1'}, {'name': 'ONTAP_Management', 'network_uid': 'network-39', 'vlanid': '0', 'vswitch': 'vSwitch1'}], 'vswitch_config': [{'associated_adapter': ['vmnic0'], 'available_ports': 7916, 'mtu': '1500', 'name': 'vSwitch0', 'portgroups': ['VM Network', 'Management Network'], 'total_ports': 7936, 'type': 'Standard vSwitch'}, {'associated_adapter': ['vmnic2', 'vmnic3'], 'available_ports': 7916, 'mtu': '9000', 'name': 'vSwitch1', 'portgroups': ['ONTAP_Internal', 'ONTAP_External', 'ONTAP_Management'], 'total_ports': 7936, 'type': 'Standard vSwitch'}], 'physical_config': {'network_adapters': [{'associated_vswitch': 'vSwitch0', 'enabled': True, 'mac': '00:25:b5:27:00:51', 'name': 'vmnic0', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'None', 'enabled': True, 'mac': '00:25:b5:27:00:52', 'name': 'vmnic1', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'vSwitch1', 'enabled': True, 'mac': '00:25:b5:27:00:53', 'name': 'vmnic2', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'vSwitch1', 'enabled': True, 'mac': '00:25:b5:27:00:54', 'name': 'vmnic3', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}], 'storage_adapters': [{'description': '(0000:08:00.0) Cisco UCS VIC Fnic Controller', 'logical_name': 'vmhba0', 'product': 'Cisco UCS VIC Fnic Controller'}, {'description': '(0000:0f:00.0) Cisco UCS VIC Fnic Controller', 'logical_name': 'vmhba1', 'product': 'Cisco UCS VIC Fnic Controller'}], 'host_hardware_config': {'cores_in_use': 3, 'cpu_model': 'Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz', 'cpu_speed_mhz': 2394, 'hardware_vendor': 'Cisco Systems Inc', 'mem_size_mb': 130960, 'mem_usage_mb': 27614, 'num_cpu_cores': 20, 'num_hbas': 2, 'num_nics': 4, 'number_cpus': 40, 'product_version': 'VMware ESXi 7.0.0'}, 'host_software_config': {}, 'hypervisor_config': {'esx': {'esx_license': 'VMware vSphere 7 Enterprise Plus', 'sdk_version': '6.7.0', 'api_version': '7.0.0.0', 'bios_release_date': '2018-07-11T00:00:00Z', 'bios_version': 'B200M4.4.0.1c.0.0711181418', 'firewall_defaults_in': 'blocked', 'firewall_defaults_out': 'blocked', 'host': 'sdot-b200-005.gdl.englab.netapp.com', 'hyperthreading': 'active', 'hypervisor': 'VMware ESXi 7.0.0 build-15843807', 'hypervisor_build': '15843807', 'hypervisor_name': 'VMware ESXi', 'hypervisor_version': '7.0.0', 'model': 'UCSB-B200-M4', 'moid': 'host-2412:3025b4b7-b510-4852-98ed-38668b194b85', 'power_mgmt_policy': 'Balanced', 'power_mgmt_technology': 'ACPI P-states', 'reboot_required': 'No', 'server': 'sdot-b200-010a.gdl.englab.netapp.com', 'user': 'administrator@vsphere.local', 'uuid': '00000000-0000-0000-0000-0025b5270050'}}}}
2023-01-16 15:01:08,030|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) success [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:08.030315]> [failed:False] Host "sdot-b200-006.gdl.englab.netapp.com" loading cache
2023-01-16 15:01:09,816|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.cluster:get_clusters] executing
2023-01-16 15:01:09,817|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449564633016: <CoroutineRecord (controllers.cluster:get_clusters) [ACTIVE]>}]
2023-01-16 15:01:09,824| INFO| 2687|12038|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/clusters?fields=uuid,name,num_nodes,state,gateway,ip,nodes,mediator_iscsi_target,netmask,state,ontap_image_version
2023-01-16 15:01:09,979|DEBUG| 2687|12038|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1372","write_io_kb":"2052"},{"ip":"10.228.160.231","read_io_kb":"1194","write_io_kb":"1696"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:05 2023","lun0_last_modify":"Mon Jan 16 15:01:09 2023","lun1_last_access":"Mon Jan 16 15:01:08 2023","lun1_last_modify":"Mon Jan 16 15:01:09 2023"}]})
2023-01-16 15:01:10,031|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:10,083|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:10,135|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:10,188|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:10,273|DEBUG| 2687|12038|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1372","write_io_kb":"2052"},{"ip":"10.228.160.231","read_io_kb":"1194","write_io_kb":"1696"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:05 2023","lun0_last_modify":"Mon Jan 16 15:01:09 2023","lun1_last_access":"Mon Jan 16 15:01:08 2023","lun1_last_modify":"Mon Jan 16 15:01:09 2023"}]})
2023-01-16 15:01:10,326|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:10,378|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:10,430|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:10,482|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:10,565|DEBUG| 2687|12038|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1372","write_io_kb":"2052"},{"ip":"10.228.160.231","read_io_kb":"1194","write_io_kb":"1696"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:05 2023","lun0_last_modify":"Mon Jan 16 15:01:09 2023","lun1_last_access":"Mon Jan 16 15:01:08 2023","lun1_last_modify":"Mon Jan 16 15:01:09 2023"}]})
2023-01-16 15:01:10,617|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:10,675|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:10,729|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:10,784|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:10,867|DEBUG| 2687|12038|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1380","write_io_kb":"2052"},{"ip":"10.228.160.231","read_io_kb":"1194","write_io_kb":"1704"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:10 2023","lun0_last_modify":"Mon Jan 16 15:01:10 2023","lun1_last_access":"Mon Jan 16 15:01:08 2023","lun1_last_modify":"Mon Jan 16 15:01:09 2023"}]})
2023-01-16 15:01:10,920|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:10,975|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:11,031|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:11,086|DEBUG| 2687|12038|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:11,096| INFO| 2687|12038|restapi.py|263|GET http://127.0.0.1:8080/api/v3/clusters?fields=uuid,name,num_nodes,state,gateway,ip,nodes,mediator_iscsi_target,netmask,state,ontap_image_version returned in 1278ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:12,712|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.cluster:get_cluster] executing
2023-01-16 15:01:12,713|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449557918040: <CoroutineRecord (controllers.cluster:get_cluster) [ACTIVE]>}]
2023-01-16 15:01:12,718| INFO| 2687|12039|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/clusters/e0d06a86-95ab-11ed-bd72-000c29e96265?fields=uuid,name,num_nodes,state,last_refresh,mtu,ntp_servers,eval_days,ontap_image_version,ontap_version,ip,gateway,netmask,eval,nodes.host.*,mediator_iscsi_target.name,mediator_iscsi_target.mediator.ip,is_host_update_in_progress,dns_info.*,nodes.serial_number,nodes.*,nodes.networks.*,nodes.networks.vswitch.network_adapters,nodes.networks.vswitch.type,nodes.storage.pools.*,nodes.license.type,nodes.license.state,nodes.license.pool_name,nodes.license.capacity,nodes.host.hypervisor_type,nodes.storage.disks.*,nodes.host.hypervisor_config.esx.moid,nodes.vm.*,nodes.networks.network.moid
2023-01-16 15:01:13,147|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.job:get_jobs] executing
2023-01-16 15:01:13,149|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449557918040: <CoroutineRecord (controllers.cluster:get_cluster) [ACTIVE]>, 140449564633016: <CoroutineRecord (controllers.job:get_jobs) [ACTIVE]>}]
2023-01-16 15:01:13,153| INFO| 2687|12040|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/jobs?fields=id,message,name,owner,request_id&state=running%7Cqueued
2023-01-16 15:01:13,157| INFO| 2687|12040|restapi.py|263|GET http://127.0.0.1:8080/api/v3/jobs?fields=id,message,name,owner,request_id&state=running%7Cqueued returned in 8ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:13,194|DEBUG| 2687|12039|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1380","write_io_kb":"2060"},{"ip":"10.228.160.231","read_io_kb":"1198","write_io_kb":"1712"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:10 2023","lun0_last_modify":"Mon Jan 16 15:01:13 2023","lun1_last_access":"Mon Jan 16 15:01:11 2023","lun1_last_modify":"Mon Jan 16 15:01:12 2023"}]})
2023-01-16 15:01:13,241|DEBUG| 2687|12039|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:13,294|DEBUG| 2687|12039|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:13,345|DEBUG| 2687|12039|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:13,398|DEBUG| 2687|12039|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:13,435| INFO| 2687|12039|restapi.py|263|GET http://127.0.0.1:8080/api/v3/clusters/e0d06a86-95ab-11ed-bd72-000c29e96265?fields=uuid,name,num_nodes,state,last_refresh,mtu,ntp_servers,eval_days,ontap_image_version,ontap_version,ip,gateway,netmask,eval,nodes.host.*,mediator_iscsi_target.name,mediator_iscsi_target.mediator.ip,is_host_update_in_progress,dns_info.*,nodes.serial_number,nodes.*,nodes.networks.*,nodes.networks.vswitch.network_adapters,nodes.networks.vswitch.type,nodes.storage.pools.*,nodes.license.type,nodes.license.state,nodes.license.pool_name,nodes.license.capacity,nodes.host.hypervisor_type,nodes.storage.disks.*,nodes.host.hypervisor_config.esx.moid,nodes.vm.*,nodes.networks.network.moid returned in 722ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:13,499|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.deploy:get_deploy] executing
2023-01-16 15:01:13,501|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449564633016: <CoroutineRecord (controllers.deploy:get_deploy) [ACTIVE]>}]
2023-01-16 15:01:13,505| INFO| 2687|12041|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/deploy?fields=instance_types.*
2023-01-16 15:01:13,517| INFO| 2687|12041|restapi.py|263|GET http://127.0.0.1:8080/api/v3/deploy?fields=instance_types.* returned in 15ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:13,753|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.storage:get_storage_pools] executing
2023-01-16 15:01:13,755|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449564633016: <CoroutineRecord (controllers.storage:get_storage_pools) [ACTIVE]>}]
2023-01-16 15:01:13,758| INFO| 2687|12042|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/clusters/e0d06a86-95ab-11ed-bd72-000c29e96265/nodes/e0d1009a-95ab-11ed-bd72-000c29e96265/storage/pools?fields=pool.moid,pool.device_type
2023-01-16 15:01:13,868|DEBUG| 2687|12042|logger.py|132|client_api_helper.py|240:get_storage_pools| get_storage_pools executing
2023-01-16 15:01:13,885|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.storage:get_storage_pools] executing
2023-01-16 15:01:13,887|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449564633016: <CoroutineRecord (controllers.storage:get_storage_pools) [ACTIVE]>, 140449557918040: <CoroutineRecord (controllers.storage:get_storage_pools) [ACTIVE]>}]
2023-01-16 15:01:13,890| INFO| 2687|12043|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/clusters/e0d06a86-95ab-11ed-bd72-000c29e96265/nodes/e0d228bc-95ab-11ed-bd72-000c29e96265/storage/pools?fields=pool.moid,pool.device_type
2023-01-16 15:01:13,996|DEBUG| 2687|12043|logger.py|132|client_api_helper.py|240:get_storage_pools| get_storage_pools executing
2023-01-16 15:01:14,027|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.cluster:get_clusters] executing
2023-01-16 15:01:14,029|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449564633016: <CoroutineRecord (controllers.storage:get_storage_pools) [ACTIVE]>, 140449557918040: <CoroutineRecord (controllers.storage:get_storage_pools) [ACTIVE]>, 140449557919400: <CoroutineRecord (controllers.cluster:get_clusters) [ACTIVE]>}]
2023-01-16 15:01:14,033| INFO| 2687|12044|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/clusters?fields=id,name
2023-01-16 15:01:14,087| INFO| 2687|12044|restapi.py|263|GET http://127.0.0.1:8080/api/v3/clusters?fields=id,name returned in 58ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:14,363|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.mediator:get_mediator] executing
2023-01-16 15:01:14,365|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449564633016: <CoroutineRecord (controllers.storage:get_storage_pools) [ACTIVE]>, 140449557918040: <CoroutineRecord (controllers.storage:get_storage_pools) [ACTIVE]>, 140449557919400: <CoroutineRecord (controllers.mediator:get_mediator) [ACTIVE]>}]
2023-01-16 15:01:14,378| INFO| 2687|12045|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/mediators/31558f42-8dd0-11ed-bc2c-000c29e96265?fields=id,ip,iscsi_targets.*,iscsi_targets.connections.*
2023-01-16 15:01:14,517|DEBUG| 2687|12045|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1384","write_io_kb":"2060"},{"ip":"10.228.160.231","read_io_kb":"1202","write_io_kb":"1716"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:13 2023","lun0_last_modify":"Mon Jan 16 15:01:14 2023","lun1_last_access":"Mon Jan 16 15:01:14 2023","lun1_last_modify":"Mon Jan 16 15:01:12 2023"}]})
2023-01-16 15:01:14,601|DEBUG| 2687|12045|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:14,688|DEBUG| 2687|12045|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:14,773|DEBUG| 2687|12045|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:14,857|DEBUG| 2687|12045|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:15,156|DEBUG| 2687|12042|rest.py|173|response body: [{"allocation":3767592,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":25058513,"status":"Online"},{"allocation":1991372,"capacity":2072320,"device_type":"SSD","location_type":"external","name":"Datastore_006","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)"],"pool_type":"VMFS-6","pool_uid":"datastore-2417","provisioned_space":1993808,"status":"Online"},{"allocation":1115085,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_05","pdisks":["NETAPP Fibre Channel Disk (naa.600a098038303042542b51447a38305a)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b50636a)"],"pool_type":"VMFS-6","pool_uid":"datastore-2421","provisioned_space":1684248,"status":"Online"}]
2023-01-16 15:01:15,168| INFO| 2687|12042|restapi.py|263|GET http://127.0.0.1:8080/api/v3/clusters/e0d06a86-95ab-11ed-bd72-000c29e96265/nodes/e0d1009a-95ab-11ed-bd72-000c29e96265/storage/pools?fields=pool.moid,pool.device_type returned in 1413ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:15,189|DEBUG| 2687|12043|rest.py|173|response body: [{"allocation":3767592,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":25058513,"status":"Online"},{"allocation":1991372,"capacity":2072320,"device_type":"SSD","location_type":"external","name":"Datastore_006","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)"],"pool_type":"VMFS-6","pool_uid":"datastore-2417","provisioned_space":1993808,"status":"Online"},{"allocation":1392512,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_06","pdisks":["NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)"],"pool_type":"VMFS-6","pool_uid":"datastore-2422","provisioned_space":2024717,"status":"Online"},{"allocation":1457,"capacity":1048320,"device_type":"SSD","location_type":"external","name":"Datastoree_100","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a47)"],"pool_type":"VMFS-6","pool_uid":"datastore-2438","provisioned_space":1457,"status":"Online"}]
2023-01-16 15:01:15,200| INFO| 2687|12043|restapi.py|263|GET http://127.0.0.1:8080/api/v3/clusters/e0d06a86-95ab-11ed-bd72-000c29e96265/nodes/e0d228bc-95ab-11ed-bd72-000c29e96265/storage/pools?fields=pool.moid,pool.device_type returned in 1313ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:15,208|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.storage:get_host_storage] executing
2023-01-16 15:01:15,210|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449557919400: <CoroutineRecord (controllers.mediator:get_mediator) [ACTIVE]>, 140449557919128: <CoroutineRecord (controllers.storage:get_host_storage) [ACTIVE]>}]
2023-01-16 15:01:15,213| INFO| 2687|12046|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/hosts/1cf1e9ac-8fdc-11ed-bc2c-000c29e96265/storage?fields=pools.*,disks.*,disks.supports_sw_raid
2023-01-16 15:01:15,223|DEBUG| 2687|12046|logger.py|132|client_api_helper.py|262:get_host_pdisks| get_host_pdisks executing
2023-01-16 15:01:15,229|DEBUG| 2687|0|coroutine.py|177|RESTAPI [controllers.storage:get_host_storage] executing
2023-01-16 15:01:15,231|DEBUG| 2687|0|coroutine.py|190|Registered Myself: current tasks [{140449557918856: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>, 140449557919400: <CoroutineRecord (controllers.mediator:get_mediator) [ACTIVE]>, 140449557919128: <CoroutineRecord (controllers.storage:get_host_storage) [ACTIVE]>, 140449557918584: <CoroutineRecord (controllers.storage:get_host_storage) [ACTIVE]>}]
2023-01-16 15:01:15,234| INFO| 2687|12047|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/hosts/27de4694-8fdc-11ed-bc2c-000c29e96265/storage?fields=pools.*,disks.*,disks.supports_sw_raid
2023-01-16 15:01:15,242|DEBUG| 2687|12047|logger.py|132|client_api_helper.py|262:get_host_pdisks| get_host_pdisks executing
2023-01-16 15:01:16,486| INFO| 2687|12045|restapi.py|263|GET http://127.0.0.1:8080/api/v3/mediators/31558f42-8dd0-11ed-bc2c-000c29e96265?fields=id,ip,iscsi_targets.*,iscsi_targets.connections.* returned in 2120ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:16,583|DEBUG| 2687|12022|rest.py|173|response body: {"available_pdisks":[{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b75","total_diskspace":1048593,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a47","total_diskspace":1048593,"used_by":"Datastoree_100"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656932","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a0980383036554e2b514565396e70","total_diskspace":3145728,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38302f","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}],"available_pools":[{"allocation":3767592,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":25058513,"status":"Online"},{"allocation":1991372,"capacity":2072320,"device_type":"SSD","location_type":"external","name":"Datastore_006","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)"],"pool_type":"VMFS-6","pool_uid":"datastore-2417","provisioned_space":1993808,"status":"Online"},{"allocation":1392512,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_06","pdisks":["NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)"],"pool_type":"VMFS-6","pool_uid":"datastore-2422","provisioned_space":2024717,"status":"Online"},{"allocation":1457,"capacity":1048320,"device_type":"SSD","location_type":"external","name":"Datastoree_100","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a47)"],"pool_type":"VMFS-6","pool_uid":"datastore-2438","provisioned_space":1457,"status":"Online"}],"available_portgroups":[{"name":"VM Network","network_uid":"network-15","vlanid":"0","vswitch":"vSwitch0"},{"name":"ONTAP_Management","network_uid":"network-39","vlanid":"0","vswitch":"vSwitch1"},{"name":"ONTAP_Internal","network_uid":"network-40","vlanid":"0","vswitch":"vSwitch1"},{"name":"ONTAP_External","network_uid":"network-41","vlanid":"2480","vswitch":"vSwitch1"}],"available_vms":[],"available_vswitches":[{"associated_adapter":["vmnic0"],"available_ports":7918,"mtu":"1500","name":"vSwitch0","portgroups":["VM Network","Management Network"],"total_ports":7936,"type":"Standard vSwitch"},{"associated_adapter":["vmnic3","vmnic2"],"available_ports":7918,"mtu":"9000","name":"vSwitch1","portgroups":["ONTAP_Management","ONTAP_Internal","ONTAP_External"],"total_ports":7936,"type":"Standard vSwitch"}],"host_configuration":{"host_hardware_config":{"cores_in_use":3,"cpu_model":"Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz","cpu_speed_mhz":2394,"hardware_vendor":"Cisco Systems Inc","mem_size_mb":130960,"mem_usage_mb":19341,"num_cpu_cores":20,"num_hbas":2,"num_nics":4,"number_cpus":40,"product_version":"VMware ESXi 7.0.0"},"hypervisor_info":{"esx":{"ESX_license":"VMware vSphere 7 Enterprise Plus","SDK_version":"6.7.0","api_version":"7.0.0.0","bios_release_date":"2018-07-11T00:00:00Z","bios_version":"B200M4.4.0.1c.0.0711181418","firewall_defaults_in":"blocked","firewall_defaults_out":"blocked","host":"sdot-b200-006.gdl.englab.netapp.com","hyperthreading":"active","hypervisor":"VMware ESXi 7.0.0 build-15843807","hypervisor_build":"15843807","hypervisor_name":"VMware ESXi","hypervisor_version":"7.0.0","model":"UCSB-B200-M4","moid":"host-2415:3025b4b7-b510-4852-98ed-38668b194b85","power_mgmt_policy":"Balanced","power_mgmt_technology":"ACPI P-states","reboot_required":"No","server":"sdot-b200-010a.gdl.englab.netapp.com","user":"administrator@vsphere.local","uuid":"00000000-0000-0000-0000-0025b5270060"}},"network_adapters":[{"associated_vswitch":"vSwitch0","enabled":true,"mac":"00:25:b5:27:00:61","name":"vmnic0","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"None","enabled":true,"mac":"00:25:b5:27:00:62","name":"vmnic1","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"vSwitch1","enabled":true,"mac":"00:25:b5:27:00:63","name":"vmnic2","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"vSwitch1","enabled":true,"mac":"00:25:b5:27:00:64","name":"vmnic3","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"}],"storage_adapters":[{"description":"(0000:08:00.0) Cisco UCS VIC Fnic Controller","logical_name":"vmhba0","product":"Cisco UCS VIC Fnic Controller"},{"description":"(0000:0f:00.0) Cisco UCS VIC Fnic Controller","logical_name":"vmhba1","product":"Cisco UCS VIC Fnic Controller"}]}}
2023-01-16 15:01:16,595|DEBUG| 2687|12022|logger.py|132|host.py|645:get_host_support| {'storage_pools': [{'allocation': 3767592, 'capacity': 16934502, 'device_type': 'N/A', 'location_type': 'external', 'name': 'sdot_nfs_ds1', 'pdisks': ['N/A'], 'pool_type': 'NFS', 'pool_uid': 'datastore-2250', 'provisioned_space': 25058513, 'status': 'Online'}, {'allocation': 1991372, 'capacity': 2072320, 'device_type': 'SSD', 'location_type': 'external', 'name': 'Datastore_006', 'pdisks': ['NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)'], 'pool_type': 'VMFS-6', 'pool_uid': 'datastore-2417', 'provisioned_space': 1993808, 'status': 'Online'}, {'allocation': 1392512, 'capacity': 4193792, 'device_type': 'SSD', 'location_type': 'external', 'name': 'Datastore_06', 'pdisks': ['NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)', 'NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)'], 'pool_type': 'VMFS-6', 'pool_uid': 'datastore-2422', 'provisioned_space': 2024717, 'status': 'Online'}, {'allocation': 1457, 'capacity': 1048320, 'device_type': 'SSD', 'location_type': 'external', 'name': 'Datastoree_100', 'pdisks': ['NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a47)'], 'pool_type': 'VMFS-6', 'pool_uid': 'datastore-2438', 'provisioned_space': 1457, 'status': 'Online'}], 'storage_pdisks': [{'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a09805176304d6d5d4b384b506b75', 'total_diskspace': 1048593, 'used_by': 'Datastore_06'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724e7a46', 'total_diskspace': 1048593, 'used_by': 'Datastore_006'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724e7a47', 'total_diskspace': 1048593, 'used_by': 'Datastoree_100'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763671763f4b395a656932', 'total_diskspace': 20480, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f3545', 'total_diskspace': 512078, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f3547', 'total_diskspace': 512078, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f3548', 'total_diskspace': 512078, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f354a', 'total_diskspace': 512078, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a0980383036554e2b514565396e70', 'total_diskspace': 3145728, 'used_by': 'Datastore_06'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098038303042542b51447a38302f', 'total_diskspace': 10240, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763039375d4b39724f352d', 'total_diskspace': 512078, 'used_by': ''}], 'network_portgroups': [{'name': 'VM Network', 'network_uid': 'network-15', 'vlanid': '0', 'vswitch': 'vSwitch0'}, {'name': 'ONTAP_Management', 'network_uid': 'network-39', 'vlanid': '0', 'vswitch': 'vSwitch1'}, {'name': 'ONTAP_Internal', 'network_uid': 'network-40', 'vlanid': '0', 'vswitch': 'vSwitch1'}, {'name': 'ONTAP_External', 'network_uid': 'network-41', 'vlanid': '2480', 'vswitch': 'vSwitch1'}], 'vswitch_config': [{'associated_adapter': ['vmnic0'], 'available_ports': 7918, 'mtu': '1500', 'name': 'vSwitch0', 'portgroups': ['VM Network', 'Management Network'], 'total_ports': 7936, 'type': 'Standard vSwitch'}, {'associated_adapter': ['vmnic3', 'vmnic2'], 'available_ports': 7918, 'mtu': '9000', 'name': 'vSwitch1', 'portgroups': ['ONTAP_Management', 'ONTAP_Internal', 'ONTAP_External'], 'total_ports': 7936, 'type': 'Standard vSwitch'}], 'physical_config': {'network_adapters': [{'associated_vswitch': 'vSwitch0', 'enabled': True, 'mac': '00:25:b5:27:00:61', 'name': 'vmnic0', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'None', 'enabled': True, 'mac': '00:25:b5:27:00:62', 'name': 'vmnic1', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'vSwitch1', 'enabled': True, 'mac': '00:25:b5:27:00:63', 'name': 'vmnic2', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'vSwitch1', 'enabled': True, 'mac': '00:25:b5:27:00:64', 'name': 'vmnic3', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}], 'storage_adapters': [{'description': '(0000:08:00.0) Cisco UCS VIC Fnic Controller', 'logical_name': 'vmhba0', 'product': 'Cisco UCS VIC Fnic Controller'}, {'description': '(0000:0f:00.0) Cisco UCS VIC Fnic Controller', 'logical_name': 'vmhba1', 'product': 'Cisco UCS VIC Fnic Controller'}], 'host_hardware_config': {'cores_in_use': 3, 'cpu_model': 'Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz', 'cpu_speed_mhz': 2394, 'hardware_vendor': 'Cisco Systems Inc', 'mem_size_mb': 130960, 'mem_usage_mb': 19341, 'num_cpu_cores': 20, 'num_hbas': 2, 'num_nics': 4, 'number_cpus': 40, 'product_version': 'VMware ESXi 7.0.0'}, 'host_software_config': {}, 'hypervisor_config': {'esx': {'esx_license': 'VMware vSphere 7 Enterprise Plus', 'sdk_version': '6.7.0', 'api_version': '7.0.0.0', 'bios_release_date': '2018-07-11T00:00:00Z', 'bios_version': 'B200M4.4.0.1c.0.0711181418', 'firewall_defaults_in': 'blocked', 'firewall_defaults_out': 'blocked', 'host': 'sdot-b200-006.gdl.englab.netapp.com', 'hyperthreading': 'active', 'hypervisor': 'VMware ESXi 7.0.0 build-15843807', 'hypervisor_build': '15843807', 'hypervisor_name': 'VMware ESXi', 'hypervisor_version': '7.0.0', 'model': 'UCSB-B200-M4', 'moid': 'host-2415:3025b4b7-b510-4852-98ed-38668b194b85', 'power_mgmt_policy': 'Balanced', 'power_mgmt_technology': 'ACPI P-states', 'reboot_required': 'No', 'server': 'sdot-b200-010a.gdl.englab.netapp.com', 'user': 'administrator@vsphere.local', 'uuid': '00000000-0000-0000-0000-0025b5270060'}}}}
2023-01-16 15:01:16,765|DEBUG| 2687|12022|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1388","write_io_kb":"2072"},{"ip":"10.228.160.231","read_io_kb":"1206","write_io_kb":"1724"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:15 2023","lun0_last_modify":"Mon Jan 16 15:01:15 2023","lun1_last_access":"Mon Jan 16 15:01:14 2023","lun1_last_modify":"Mon Jan 16 15:01:14 2023"}]})
2023-01-16 15:01:16,841|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:16,918|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:17,002|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:17,082|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:17,282|DEBUG| 2687|12022|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1392","write_io_kb":"2072"},{"ip":"10.228.160.231","read_io_kb":"1210","write_io_kb":"1724"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:15 2023","lun0_last_modify":"Mon Jan 16 15:01:15 2023","lun1_last_access":"Mon Jan 16 15:01:17 2023","lun1_last_modify":"Mon Jan 16 15:01:14 2023"}]})
2023-01-16 15:01:17,362|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:17,438|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:17,526|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:17,598|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:18,294|DEBUG| 2687|12022|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1392","write_io_kb":"2080"},{"ip":"10.228.160.231","read_io_kb":"1223","write_io_kb":"1736"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:17 2023","lun0_last_modify":"Mon Jan 16 15:01:18 2023","lun1_last_access":"Mon Jan 16 15:01:17 2023","lun1_last_modify":"Mon Jan 16 15:01:17 2023"}]})
2023-01-16 15:01:18,365|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:18,419|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:18,469|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:18,521|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:18,786|DEBUG| 2687|12047|rest.py|173|response body: [{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b75","total_diskspace":1048593,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a47","total_diskspace":1048593,"used_by":"Datastoree_100"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656932","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a0980383036554e2b514565396e70","total_diskspace":3145728,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38302f","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}]
2023-01-16 15:01:19,622|DEBUG| 2687|12047|logger.py|132|client_api_helper.py|240:get_storage_pools| get_storage_pools executing
2023-01-16 15:01:19,626|DEBUG| 2687|12046|rest.py|173|response body: [{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b74","total_diskspace":1048593,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656931","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b50636a","total_diskspace":1048593,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a383059","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38305a","total_diskspace":3145728,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}]
2023-01-16 15:01:20,466|DEBUG| 2687|12046|logger.py|132|client_api_helper.py|240:get_storage_pools| get_storage_pools executing
2023-01-16 15:01:20,630|DEBUG| 2687|12022|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1396","write_io_kb":"2092"},{"ip":"10.228.160.231","read_io_kb":"1231","write_io_kb":"1740"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:19 2023","lun0_last_modify":"Mon Jan 16 15:01:19 2023","lun1_last_access":"Mon Jan 16 15:01:20 2023","lun1_last_modify":"Mon Jan 16 15:01:19 2023"}]})
2023-01-16 15:01:20,692|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:20,757|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:20,818|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:20,899|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:21,462|DEBUG| 2687|12046|rest.py|173|response body: [{"allocation":3767592,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":25058513,"status":"Online"},{"allocation":1991372,"capacity":2072320,"device_type":"SSD","location_type":"external","name":"Datastore_006","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)"],"pool_type":"VMFS-6","pool_uid":"datastore-2417","provisioned_space":1993808,"status":"Online"},{"allocation":1115085,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_05","pdisks":["NETAPP Fibre Channel Disk (naa.600a098038303042542b51447a38305a)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b50636a)"],"pool_type":"VMFS-6","pool_uid":"datastore-2421","provisioned_space":1684248,"status":"Online"}]
2023-01-16 15:01:21,474| INFO| 2687|12046|restapi.py|263|GET http://127.0.0.1:8080/api/v3/hosts/1cf1e9ac-8fdc-11ed-bc2c-000c29e96265/storage?fields=pools.*,disks.*,disks.supports_sw_raid returned in 6264ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:21,499|DEBUG| 2687|12047|rest.py|173|response body: [{"allocation":3767592,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":25058513,"status":"Online"},{"allocation":1991372,"capacity":2072320,"device_type":"SSD","location_type":"external","name":"Datastore_006","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)"],"pool_type":"VMFS-6","pool_uid":"datastore-2417","provisioned_space":1993808,"status":"Online"},{"allocation":1392512,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_06","pdisks":["NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)"],"pool_type":"VMFS-6","pool_uid":"datastore-2422","provisioned_space":2024717,"status":"Online"},{"allocation":1457,"capacity":1048320,"device_type":"SSD","location_type":"external","name":"Datastoree_100","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a47)"],"pool_type":"VMFS-6","pool_uid":"datastore-2438","provisioned_space":1457,"status":"Online"}]
2023-01-16 15:01:21,514| INFO| 2687|12047|restapi.py|263|GET http://127.0.0.1:8080/api/v3/hosts/27de4694-8fdc-11ed-bc2c-000c29e96265/storage?fields=pools.*,disks.*,disks.supports_sw_raid returned in 6282ms with status HTTPStatus.OK (BA)
2023-01-16 15:01:21,574|DEBUG| 2687|12022|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":4,"NumberOfTargetsWith0ActiveConnections":3,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":0,"Connections":[],"Name":"select000001","lun0_last_access":"Mon Jan  9 09:31:11 2023","lun0_last_modify":"Mon Jan  9 09:31:11 2023","lun1_last_access":"Mon Jan  9 09:31:10 2023","lun1_last_modify":"Mon Jan  9 09:32:02 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000002","lun0_last_access":"Tue Jan 10 09:14:26 2023","lun0_last_modify":"Tue Jan 10 09:14:25 2023","lun1_last_access":"Tue Jan 10 09:14:25 2023","lun1_last_modify":"Tue Jan 10 09:15:06 2023"},{"ActiveConnections":0,"Connections":[],"Name":"select000000","lun0_last_access":"Mon Jan 16 14:38:43 2023","lun0_last_modify":"Mon Jan 16 14:39:22 2023","lun1_last_access":"Mon Jan 16 14:38:44 2023","lun1_last_modify":"Mon Jan 16 14:38:43 2023"},{"ActiveConnections":2,"Connections":[{"ip":"10.228.160.229","read_io_kb":"1400","write_io_kb":"2092"},{"ip":"10.228.160.231","read_io_kb":"1231","write_io_kb":"1748"}],"Name":"select000003","lun0_last_access":"Mon Jan 16 15:01:20 2023","lun0_last_modify":"Mon Jan 16 15:01:20 2023","lun1_last_access":"Mon Jan 16 15:01:20 2023","lun1_last_modify":"Mon Jan 16 15:01:19 2023"}]})
2023-01-16 15:01:21,614|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000001), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev2-0","path":"\/mnt\/iscsi_space\/mb.select000001.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev2-1","path":"\/mnt\/iscsi_space\/mb.select000001.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000001","stack":"scst"})
2023-01-16 15:01:21,660|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000002), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev4-0","path":"\/mnt\/iscsi_space\/mb.select000002.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev4-1","path":"\/mnt\/iscsi_space\/mb.select000002.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000002","stack":"scst"})
2023-01-16 15:01:21,704|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev5-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev5-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
2023-01-16 15:01:21,749|DEBUG| 2687|12022|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000003), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev6-0","path":"\/mnt\/iscsi_space\/mb.select000003.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev6-1","path":"\/mnt\/iscsi_space\/mb.select000003.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000003","stack":"scst"})
2023-01-16 15:01:23,664|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|262:get_host_pdisks| get_host_pdisks executing
2023-01-16 15:01:25,819|DEBUG| 2687|12022|rest.py|173|response body: [{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b74","total_diskspace":1048593,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656931","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b50636a","total_diskspace":1048593,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a383059","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38305a","total_diskspace":3145728,"used_by":"Datastore_05"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}]
2023-01-16 15:01:26,593|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|262:get_host_pdisks| get_host_pdisks executing
2023-01-16 15:01:28,696|DEBUG| 2687|12022|rest.py|173|response body: [{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a09805176304d6d5d4b384b506b75","total_diskspace":1048593,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a46","total_diskspace":1048593,"used_by":"Datastore_006"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724e7a47","total_diskspace":1048593,"used_by":"Datastoree_100"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a656932","total_diskspace":20480,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3545","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3547","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f3548","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f354a","total_diskspace":512078,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a0980383036554e2b514565396e70","total_diskspace":3145728,"used_by":"Datastore_06"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098038303042542b51447a38302f","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763039375d4b39724f352d","total_diskspace":512078,"used_by":""}]
2023-01-16 15:01:29,507|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|240:get_storage_pools| get_storage_pools executing
2023-01-16 15:01:30,105|DEBUG| 2687|12022|rest.py|173|response body: [{"allocation":3767592,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":25058513,"status":"Online"},{"allocation":1991372,"capacity":2072320,"device_type":"SSD","location_type":"external","name":"Datastore_006","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)"],"pool_type":"VMFS-6","pool_uid":"datastore-2417","provisioned_space":1993808,"status":"Online"},{"allocation":1115085,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_05","pdisks":["NETAPP Fibre Channel Disk (naa.600a098038303042542b51447a38305a)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b50636a)"],"pool_type":"VMFS-6","pool_uid":"datastore-2421","provisioned_space":1684248,"status":"Online"}]
2023-01-16 15:01:30,967|DEBUG| 2687|12022|logger.py|132|client_api_helper.py|240:get_storage_pools| get_storage_pools executing
2023-01-16 15:01:31,591|DEBUG| 2687|12022|rest.py|173|response body: [{"allocation":3767592,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":25058513,"status":"Online"},{"allocation":1991372,"capacity":2072320,"device_type":"SSD","location_type":"external","name":"Datastore_006","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a46)"],"pool_type":"VMFS-6","pool_uid":"datastore-2417","provisioned_space":1993808,"status":"Online"},{"allocation":1392512,"capacity":4193792,"device_type":"SSD","location_type":"external","name":"Datastore_06","pdisks":["NETAPP Fibre Channel Disk (naa.600a0980383036554e2b514565396e70)","NETAPP Fibre Channel Disk (naa.600a09805176304d6d5d4b384b506b75)"],"pool_type":"VMFS-6","pool_uid":"datastore-2422","provisioned_space":2024717,"status":"Online"},{"allocation":1457,"capacity":1048320,"device_type":"SSD","location_type":"external","name":"Datastoree_100","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763039375d4b39724e7a47)"],"pool_type":"VMFS-6","pool_uid":"datastore-2438","provisioned_space":1457,"status":"Online"}]
2023-01-16 15:01:32,457| INFO| 2687|12022|logger.py|132|autosupport.py|634:collect_and_transmit| Triggered CLUSTER_DEPLOY ASUP with seq_num 7
2023-01-16 15:01:32,459| INFO| 2687|12022|logger.py|132|autosupport.py|70:invoke_autosupport_async| Collection complete for asup(7), waiting for archival
2023-01-16 15:01:42,476| INFO| 2687|12022|logger.py|132|autosupport.py|680:wait_for_archive_completion| Archive complete for asup(7)
2023-01-16 15:01:42,491| INFO| 2687|12022|logger.py|132|event.py|134:__init__| <Event(id=444, request_id='12022', time='2023-01-16 15:01:42.480905', evtype='AsupInvokeSuccessful', category='asup', level='Info', detail='Invocation of "CLUSTER_DEPLOY" AutoSupport successful. seq_num:7 user_msg:')>
2023-01-16 15:01:42,493|DEBUG| 2687|12022|job.py|364|Updated <Job (CLUSTER_CREATE) success [id:a68744e8-95ac-11ed-a141-000c29e96265 rid:12022 mod:2023-01-16 15:01:42.492942]> [failed:False] Invocation of "CLUSTER_DEPLOY" AutoSupport successful. seq_num:7 user_msg:
