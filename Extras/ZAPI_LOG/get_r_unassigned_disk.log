  "ip": {
            "address": "10.228.161.50"
          },
          "_links": {
            "self": {
              "href": "/api/network/ip/interfaces/23d0c43b-b694-11ed-bc3d-00a0b89dc092"
            }
          }
        }
      ],
      "cluster_interfaces": [
        {
          "uuid": "23d0838a-b694-11ed-bc3d-00a0b89dc092",
          "name": "UA-HW_CLST-02_clus1",
          "ip": {
            "address": "169.254.59.177"
          },
          "_links": {
            "self": {
              "href": "/api/network/ip/interfaces/23d0838a-b694-11ed-bc3d-00a0b89dc092"
            }
          }
        },
        {
          "uuid": "23d0ae37-b694-11ed-bc3d-00a0b89dc092",
          "name": "UA-HW_CLST-02_clus2",
          "ip": {
            "address": "169.254.59.226"
          },
          "_links": {
            "self": {
              "href": "/api/network/ip/interfaces/23d0ae37-b694-11ed-bc3d-00a0b89dc092"
            }
          }
        }
      ],
      "storage_configuration": "unknown",
      "controller": {
        "board": "Virtual Machine",
        "memory_size": 16384000000,
        "over_temperature": "normal",
        "failed_fan": {
          "count": 0,
          "message": {
            "message": "There are no failed fans.",
            "code": "5505031"
          }
        },
        "failed_power_supply": {
          "count": 0,
          "message": {
            "message": "There are no failed power supplies.",
            "code": "5505032"
          }
        },
        "cpu": {
          "count": 4
        }
      },
      "ha": {
        "enabled": true,
        "auto_giveback": true,
        "partners": [
          {
            "uuid": "8566194a-b692-11ed-b76c-000c290ef0a1",
            "name": "UA-HW_CLST-01",
            "_links": {
              "self": {
                "href": "/api/cluster/nodes/8566194a-b692-11ed-b76c-000c290ef0a1"
              }
            }
          }
        ],
        "giveback": {
          "state": "nothing_to_giveback",
          "status": [
            {
              "state": "nothing_to_giveback",
              "aggregate": {
                "name": ""
              }
            }
          ]
        },
        "takeover": {
          "state": "not_attempted"
        },
        "interconnect": {
          "adapter": "MVIA-RDMA",
          "state": "up"
        },
        "ports": [
          {
            "number": 0,
            "state": "active"
          }
        ]
      },
      "service_processor": {
        "state": "node_offline",
        "ssh_info": {
          "allowed_addresses": [
            "0.0.0.0/0",
            "::/0"
          ]
        },
        "api_service": {
          "enabled": true,
          "limit_access": true,
          "port": 50000
        }
      },
      "nvram": {
        "id": 2443885343,
        "battery_state": "battery_ok"
      },
      "hw_assist": {
        "status": {
          "enabled": false,
          "local": {
            "state": "inactive",
            "ip": "0.0.0.0",
            "port": 0
          },
          "partner": {
            "state": "inactive",
            "ip": "0.0.0.0",
            "port": 0
          }
        }
      },
      "_links": {
        "self": {
          "href": "/api/cluster/nodes/85696334-b692-11ed-b76c-000c290ef0a1"
        }
      }
    }
  ],
  "num_records": 2,
  "_links": {
    "self": {
      "href": "/api/cluster/nodes/?fields=%2A"
    }
  }
}
2023-02-27 11:48:10,816|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|100:r_system_node_get_iter| Cluster [UA-HW_CLST]: "GET https://10.228.162.67:443/api/cluster/nodes/?fields=*" node_info: [{'product-version': 'NetApp Release Lighthouse__9.13.1: Mon Feb 20 05:44:10 UTC 2023', 'node-nvram-id': 2443885363, 'nvram-battery-status': 'battery_ok', 'env-failed-fan-count': 0, 'env-failed-fan-message': 'There are no failed fans.', 'env-failed-power-supply-count': 0, 'env-failed-power-supply-message': 'There are no failed power supplies.', 'env-over-temperature': 'false', 'node-uuid': '8566194a-b692-11ed-b76c-000c290ef0a1', 'node': 'UA-HW_CLST-01', 'node-serial-number': '99887766554433221151', 'node-location': None, 'node-owner': None, 'node-model': 'FDvM300', 'node-system-id': '2443885363', 'node-uptime': 450, 'is-node-healthy': 'true', 'node-storage-configuration': 'unknown'}, {'product-version': 'NetApp Release Lighthouse__9.13.1: Mon Feb 20 05:44:10 UTC 2023', 'node-nvram-id': 2443885343, 'nvram-battery-status': 'battery_ok', 'env-failed-fan-count': 0, 'env-failed-fan-message': 'There are no failed fans.', 'env-failed-power-supply-count': 0, 'env-failed-power-supply-message': 'There are no failed power supplies.', 'env-over-temperature': 'false', 'node-uuid': '85696334-b692-11ed-b76c-000c290ef0a1', 'node': 'UA-HW_CLST-02', 'node-serial-number': '99887766554433221131', 'node-location': None, 'node-owner': None, 'node-model': 'FDvM300', 'node-system-id': '2443885343', 'node-uptime': 515, 'is-node-healthy': 'true', 'node-storage-configuration': 'unknown'}]
2023-02-27 11:48:10,849| INFO|25504|6063|logger.py|132|cluster.py|2701:_configure_disks| Cluster [UA-HW_CLST]: configuring unassigned disks on nodes (UA-HW_CLST-01 and UA-HW_CLST-02)
2023-02-27 11:51:16,792|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1684:r_get_unassigned_disks| Cluster [UA-HW_CLST]: "GET https://10.228.162.67:443/api/storage/disks/?fields=*&container_type=unassigned&node.name=UA-HW_CLST-01" response: {
  "records": [
  ],
  "num_records": 0,
  "_links": {
    "self": {
      "href": "/api/storage/disks/?fields=%2A&container_type=unassigned&node.name=UA-HW_CLST-01"
    }
  }
}
2023-02-27 11:51:46,883|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1688:r_get_unassigned_disks| Cluster [UA-HW_CLST]: "GET https://10.228.162.67:443/api/storage/disks/?fields=*&container_type=unassigned&node.name=UA-HW_CLST-01" storage unassigned disks : []
2023-02-27 11:52:23,472|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1684:r_get_unassigned_disks| Cluster [UA-HW_CLST]: "GET https://10.228.162.67:443/api/storage/disks/?fields=*&container_type=unassigned&node.name=UA-HW_CLST-02" response: {
  "records": [
  ],
  "num_records": 0,
  "_links": {
    "self": {
      "href": "/api/storage/disks/?fields=%2A&container_type=unassigned&node.name=UA-HW_CLST-02"
    }
  }
}
2023-02-27 11:52:25,587|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1688:r_get_unassigned_disks| Cluster [UA-HW_CLST]: "GET https://10.228.162.67:443/api/storage/disks/?fields=*&container_type=unassigned&node.name=UA-HW_CLST-02" storage unassigned disks : []
2023-02-27 11:52:30,908| INFO|25504|6063|logger.py|132|cluster.py|2903:_assign_disks| Cluster [UA-HW_CLST]: number of unassigned disks on node (UA-HW_CLST-01): [0]
2023-02-27 11:52:30,910| INFO|25504|6063|logger.py|132|cluster.py|2905:_assign_disks| Cluster [UA-HW_CLST]: number of unassigned disks on node (UA-HW_CLST-02): [0]
2023-02-27 11:52:30,923| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=640, request_id='6063', time='2023-02-27 11:52:30.914015', evtype='ClusterCreateDiskAssignmentCompleted', category='cluster', level='Info', detail='ONTAP disk assignments completed successfully.')>
2023-02-27 11:52:30,924|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:30.924371]> [failed:False] ONTAP disk assignments completed successfully.
2023-02-27 11:52:30,934| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=641, request_id='6063', time='2023-02-27 11:52:30.926317', evtype='ONTAPClusterCreateCompleted', category='cluster', level='Info', detail='ONTAP cluster create complete. All nodes successfully joined cluster "UA-HW_CLST".')>
2023-02-27 11:52:30,934|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:30.934720]> [failed:False] ONTAP cluster create complete. All nodes successfully joined cluster "UA-HW_CLST".
2023-02-27 11:52:30,935| INFO|25504|6063|logger.py|132|cluster_tasks.py|288:create_cluster| Cluster [UA-HW_CLST]: checking for any nodes that moved to a new host...
2023-02-27 11:52:30,964|DEBUG|25504|6063|logger.py|132|client_api_helper.py|137:is_vm_on_host| is_vm_on_host executing
2023-02-27 11:52:30,965|DEBUG|25504|6063|logger.py|132|client_api_helper.py|173:vm_get| vm_get executing
2023-02-27 11:52:31,358|DEBUG|25504|6063|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-017.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"85647626-b692-11ed-b76c-000c290ef0a1","node_uuid":"8566194a-b692-11ed-b76c-000c290ef0a1"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"UA-HW_CLST-01","obj_id":"vm-12007","state":"poweredon","vm_uid":"50166a67-75d6-b15f-436a-cac4e803004b"}
2023-02-27 11:52:31,385|DEBUG|25504|6063|logger.py|132|client_api_helper.py|137:is_vm_on_host| is_vm_on_host executing
2023-02-27 11:52:31,386|DEBUG|25504|6063|logger.py|132|client_api_helper.py|173:vm_get| vm_get executing
2023-02-27 11:52:31,741|DEBUG|25504|6063|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-018.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"bootargs":"wafl-enable-sidl?=true|bootarg.template.use_node_uuids=true|bootarg.iscsi_mediator_vsid=-1","cluster_uuid":"85647626-b692-11ed-b76c-000c290ef0a1","node_uuid":"85696334-b692-11ed-b76c-000c290ef0a1"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"UA-HW_CLST-02","obj_id":"vm-12006","state":"poweredon","vm_uid":"50165ed1-9bf8-ba14-4068-fd8edbfcaf30"}
2023-02-27 11:52:31,742| INFO|25504|6063|logger.py|132|cluster.py|3006:configure_new_cluster| Cluster [UA-HW_CLST]: setting up new cluster ...
2023-02-27 11:52:31,753| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=642, request_id='6063', time='2023-02-27 11:52:31.744761', evtype='ONTAPClusterSetupInProgress', category='cluster', level='Info', detail='New cluster "UA-HW_CLST" setup in progress.')>
2023-02-27 11:52:31,753|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:31.753747]> [failed:False] New cluster "UA-HW_CLST" setup in progress.
2023-02-27 11:52:31,856|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|742:r_set_dns| Cluster [UA-HW_CLST]: "POST https://10.228.162.67:443/api/name-services/dns/" response: {
}
2023-02-27 11:52:31,857| INFO|25504|6063|logger.py|132|cluster.py|2979:_set_dns_config| Cluster [UA-HW_CLST]: completed set dns info (['10.224.223.135'], ['gdl.englab.netapp.com'])
2023-02-27 11:52:31,858|DEBUG|25504|6063|logger.py|132|cluster.py|3014:configure_new_cluster| DNS setup succeeded
2023-02-27 11:52:32,003|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|945:r_set_ntp| Cluster [UA-HW_CLST]: "POST https://10.228.162.67:443/api/cluster/ntp/servers/" response: {
  "job": {
    "uuid": "37c96ce2-b695-11ed-bc3d-00a0b89dc092",
    "_links": {
      "self": {
        "href": "/api/cluster/jobs/37c96ce2-b695-11ed-bc3d-00a0b89dc092"
      }
    }
  }
}
2023-02-27 11:52:32,003|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|948:r_set_ntp| Cluster [UA-HW_CLST]: "POST https://10.228.162.67:443/api/cluster/ntp/servers/" ntp created:True
2023-02-27 11:52:32,004| INFO|25504|6063|logger.py|132|cluster.py|2997:_set_ntp_config| Cluster [UA-HW_CLST]: set ntp server (10.60.248.183)
2023-02-27 11:52:32,005|DEBUG|25504|6063|logger.py|132|cluster.py|3026:configure_new_cluster| NTP setup succeeded
2023-02-27 11:52:32,011|DEBUG|25504|6063|logger.py|132|cluster.py|3039:configure_new_cluster| Split broadcast domain config skipped
2023-02-27 11:52:32,270|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1207:r_get_license_v2_capacity_info| Cluster [UA-HW_CLST]: "GET https://10.228.162.67:443/api/cluster/licensing/licenses/?fields=*&name=select" response: {
  "records": [
    {
      "name": "select",
      "scope": "node",
      "state": "compliant",
      "description": "ONTAP Select License",
      "licenses": [
        {
          "owner": "UA-HW_CLST-02",
          "serial_number": "99887766554433221131",
          "active": true,
          "evaluation": true,
          "expiry_time": "2023-05-28T11:44:44Z",
          "compliance": {
            "state": "compliant"
          },
          "capacity": {
            "used_size": 0
          }
        },
        {
          "owner": "UA-HW_CLST-01",
          "serial_number": "99887766554433221151",
          "active": true,
          "evaluation": true,
          "expiry_time": "2023-05-28T11:43:03Z",
          "compliance": {
            "state": "compliant"
          },
          "capacity": {
            "used_size": 0
          }
        }
      ],
      "_links": {
        "self": {
          "href": "/api/cluster/licensing/licenses/select"
        }
      }
    }
  ],
  "num_records": 1,
  "_links": {
    "self": {
      "href": "/api/cluster/licensing/licenses/?fields=%2A&name=select"
    }
  }
}
2023-02-27 11:52:32,271|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1228:r_get_license_v2_capacity_info| Cluster [UA-HW_CLST]: "GET https://10.228.162.67:443/api/cluster/licensing/licenses/?fields=*&name=select" license_info: [{'current-capacity': 0, 'expiration-time': 1685274284.0, 'max-capacity': None, 'owner': 'UA-HW_CLST-02', 'package': 'select', 'reported-state': 'evaluation', 'serial-number': '99887766554433221131'}, {'current-capacity': 0, 'expiration-time': 1685274183.0, 'max-capacity': None, 'owner': 'UA-HW_CLST-01', 'package': 'select', 'reported-state': 'evaluation', 'serial-number': '99887766554433221151'}]
2023-02-27 11:52:32,439|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1207:r_get_license_v2_capacity_info| Cluster [UA-HW_CLST]: "GET https://10.228.162.67:443/api/cluster/licensing/licenses/?fields=*&name=select" response: {
  "records": [
    {
      "name": "select",
      "scope": "node",
      "state": "compliant",
      "description": "ONTAP Select License",
      "licenses": [
        {
          "owner": "UA-HW_CLST-02",
          "serial_number": "99887766554433221131",
          "active": true,
          "evaluation": true,
          "expiry_time": "2023-05-28T11:44:44Z",
          "compliance": {
            "state": "compliant"
          },
          "capacity": {
            "used_size": 0
          }
        },
        {
          "owner": "UA-HW_CLST-01",
          "serial_number": "99887766554433221151",
          "active": true,
          "evaluation": true,
          "expiry_time": "2023-05-28T11:43:03Z",
          "compliance": {
            "state": "compliant"
          },
          "capacity": {
            "used_size": 0
          }
        }
      ],
      "_links": {
        "self": {
          "href": "/api/cluster/licensing/licenses/select"
        }
      }
    }
  ],
  "num_records": 1,
  "_links": {
    "self": {
      "href": "/api/cluster/licensing/licenses/?fields=%2A&name=select"
    }
  }
}
2023-02-27 11:52:32,440|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1228:r_get_license_v2_capacity_info| Cluster [UA-HW_CLST]: "GET https://10.228.162.67:443/api/cluster/licensing/licenses/?fields=*&name=select" license_info: [{'current-capacity': 0, 'expiration-time': 1685274284.0, 'max-capacity': None, 'owner': 'UA-HW_CLST-02', 'package': 'select', 'reported-state': 'evaluation', 'serial-number': '99887766554433221131'}, {'current-capacity': 0, 'expiration-time': 1685274183.0, 'max-capacity': None, 'owner': 'UA-HW_CLST-01', 'package': 'select', 'reported-state': 'evaluation', 'serial-number': '99887766554433221151'}]
2023-02-27 11:52:32,565|ERROR|25504|6063|logger.py|132|ClusterRestApis.py|1148:r_create_cluster_application_record| Cluster [UA-HW_CLST] "POST None" cluster_application_record exception: 'cluster-notes'
2023-02-27 11:52:33,465|DEBUG|25504|6063|logger.py|132|ASUP.py|50:get_asup_config| asup config to send to ontap: ({'asup_enabled': 'true', 'local_collection': 'true', 'destination_url': 'https://testbed.netapp.com/put/AsupPut', 'proxy_url': ''})
2023-02-27 11:52:33,497|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1334:r_set_asup_config| Cluster [UA-HW_CLST]: "PATCH https://10.228.161.228:443/api/support/autosupport/" response: {
}
2023-02-27 11:52:33,498| INFO|25504|6063|logger.py|132|ClusterRestApis.py|1339:r_set_asup_config| Cluster [UA-HW_CLST]: asup config set on node UA-HW_CLST-01 in cluster
2023-02-27 11:52:33,673|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1334:r_set_asup_config| Cluster [UA-HW_CLST]: "PATCH https://10.228.161.50:443/api/support/autosupport/" response: {
}
2023-02-27 11:52:33,675| INFO|25504|6063|logger.py|132|ClusterRestApis.py|1339:r_set_asup_config| Cluster [UA-HW_CLST]: asup config set on node UA-HW_CLST-02 in cluster
2023-02-27 11:52:33,676| INFO|25504|6063|logger.py|132|ClusterRestApis.py|1346:r_set_asup_config| Cluster [UA-HW_CLST]: asup config set on all nodes in cluster
2023-02-27 11:52:33,708|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1518:r_trigger_ontap_asup| Cluster [UA-HW_CLST]: "POST https://10.228.162.67:443/api/support/autosupport/messages/" response: {
}
2023-02-27 11:52:33,722| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=643, request_id='6063', time='2023-02-27 11:52:33.712675', evtype='ClusterONTAPAsupSent', category='cluster', level='Info', detail='Cluster ONTAP AutoSupport request triggered.')>
2023-02-27 11:52:33,723|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:33.723359]> [failed:False] Cluster ONTAP AutoSupport request triggered.
2023-02-27 11:52:33,802| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=644, request_id='6063', time='2023-02-27 11:52:33.794969', evtype='ClusterOFFTAPAsupSent', category='cluster', level='Info', detail='Cluster ONTAP Select Deploy AutoSupport request triggered, seq_num:10.')>
2023-02-27 11:52:33,802|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:33.802636]> [failed:False] Cluster ONTAP Select Deploy AutoSupport request triggered, seq_num:10.
2023-02-27 11:52:33,831|DEBUG|25504|6063|coroutine.py|190|Registered Myself: current tasks [{139753708700264: <CoroutineRecord (asynchronous.cluster_tasks:create_cluster) [ACTIVE]>, 139753708702440: <CoroutineRecord (resources.autosupport:invoke_autosupport_async) [ACTIVE]>}]
2023-02-27 11:52:33,935|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:33.935409]> [failed:False] Host "sdot-b200-017.gdl.englab.netapp.com" loading cache
2023-02-27 11:52:34,207| INFO|25504|6063|logger.py|132|ZAPI.py|241:invoke|
API: "system-cli"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-02-27 11:52:34,209|DEBUG|25504|6063|logger.py|132|ClusterZapis.py|908:z_system_cli_sidl_enable_at_aggr_create_on| Cluster [UA-HW_CLST]: System CLI, SIDL enable at aggregate creation is ON
2023-02-27 11:52:34,222| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=645, request_id='6063', time='2023-02-27 11:52:34.213444', evtype='SIDLAggrCreateEnabledOnNode', category='node', level='Info', detail='SIDL has been enabled on node "UA-HW_CLST-01".')>
2023-02-27 11:52:34,223|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:34.223634]> [failed:False] SIDL has been enabled on node "UA-HW_CLST-01".
2023-02-27 11:52:34,463| INFO|25504|6063|logger.py|132|ZAPI.py|241:invoke|
API: "system-cli"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-02-27 11:52:34,465|DEBUG|25504|6063|logger.py|132|ClusterZapis.py|908:z_system_cli_sidl_enable_at_aggr_create_on| Cluster [UA-HW_CLST]: System CLI, SIDL enable at aggregate creation is ON
2023-02-27 11:52:34,479| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=646, request_id='6063', time='2023-02-27 11:52:34.470023', evtype='SIDLAggrCreateEnabledOnNode', category='node', level='Info', detail='SIDL has been enabled on node "UA-HW_CLST-02".')>
2023-02-27 11:52:34,480|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:34.480447]> [failed:False] SIDL has been enabled on node "UA-HW_CLST-02".
2023-02-27 11:52:34,569|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1173:r_set_cluster_admin_password| Cluster [UA-HW_CLST]: "POST https://10.228.162.67:443/api/security/authentication/password" response: {
}
2023-02-27 11:52:34,571|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1177:r_set_cluster_admin_password| Cluster [UA-HW_CLST]: "POST https://10.228.162.67:443/api/security/authentication/password", password_changed: True
2023-02-27 11:52:34,573| INFO|25504|6063|logger.py|132|cluster.py|3327:_set_new_password| Cluster [UA-HW_CLST]: set admin_password initiated
2023-02-27 11:52:34,605|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1103:r_system_get_version| Cluster [UA-HW_CLST]: GET https://10.228.162.67:443/api/cluster/ response: {
  "version": {
    "full": "NetApp Release Lighthouse__9.13.1: Mon Feb 20 05:44:10 UTC 2023",
    "generation": 9,
    "major": 13,
    "minor": 1
  },
  "_links": {
    "self": {
      "href": "/api/cluster/"
    }
  }
}
2023-02-27 11:52:34,607|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1111:r_system_get_version| Cluster [UA-HW_CLST]: GET https://10.228.162.67:443/api/cluster/ version_info: {'generation': '9', 'major': '13', 'minor': '1'}
2023-02-27 11:52:34,620| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=647, request_id='6063', time='2023-02-27 11:52:34.610762', evtype='ONTAPAdminPasswordResetCompleted', category='cluster', level='Info', detail='ONTAP default password reset complete.')>
2023-02-27 11:52:34,620|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:34.620876]> [failed:False] ONTAP default password reset complete.
2023-02-27 11:52:34,633| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=648, request_id='6063', time='2023-02-27 11:52:34.627049', evtype='ONTAPClusterSetupCompleted', category='cluster', level='Info', detail='New cluster "UA-HW_CLST" setup completed successfully.')>
2023-02-27 11:52:34,634|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:34.634529]> [failed:False] New cluster "UA-HW_CLST" setup completed successfully.
2023-02-27 11:52:34,635| INFO|25504|6063|logger.py|132|cluster.py|3131:configure_new_cluster| Cluster [UA-HW_CLST]: finished setup of new cluster, returning True
2023-02-27 11:52:34,636| INFO|25504|6063|logger.py|132|cluster_tasks.py|298:create_cluster| Cluster [UA-HW_CLST]: cluster created and setup successfully
2023-02-27 11:52:34,660| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=649, request_id='6063', time='2023-02-27 11:52:34.652541', evtype='FetchingONTAPDiskNames', category='node', level='Info', detail='Fetching ONTAP disk names for node "UA-HW_CLST-01"')>
2023-02-27 11:52:34,660|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:34.660801]> [failed:False] Fetching ONTAP disk names for node "UA-HW_CLST-01"
2023-02-27 11:52:34,669|DEBUG|25504|6063|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-02-27 11:52:34,978|DEBUG|25504|6063|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"UA-HW_CLST-01.vmdk","pool":"sdot_nfs_ds1","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"UA-HW_CLST-01_1.vmdk","pool":"sdot_nfs_ds1","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"UA-HW_CLST-01_2.vmdk","pool":"sdot_nfs_ds1","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"UA-HW_CLST-01_3.vmdk","pool":"sdot_nfs_ds1","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"UA-HW_CLST-01_4.vmdk","pool":"sdot_nfs_ds1","total_diskspace":69632},{"capacity":51200,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"1","unit":"0"},"name":"UA-HW_CLST-01_5.vmdk","pool":"sdot_nfs_ds1","total_diskspace":51200},{"capacity":51200,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"1","unit":"0"},"name":"UA-HW_CLST-01_6.vmdk","pool":"sdot_nfs_ds1","total_diskspace":51200}]
2023-02-27 11:52:34,981|DEBUG|25504|6063|logger.py|132|node.py|5329:get_ontap_disk_names| Disks to find: ['UA-HW_CLST-01_5.vmdk', 'UA-HW_CLST-01_6.vmdk']
2023-02-27 11:52:35,087|DEBUG|25504|6063|logger.py|132|ZAPI.py|241:invoke|
API: "storage-disk-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-02-27 11:52:35,087|DEBUG|25504|6063|logger.py|132|node.py|5335:get_ontap_disk_names| unsupported_disks list: []
2023-02-27 11:52:35,088|DEBUG|25504|6063|logger.py|132|node.py|5338:get_ontap_disk_names| Disks to be attached_after_checking ['UA-HW_CLST-01_5.vmdk', 'UA-HW_CLST-01_6.vmdk']
2023-02-27 11:52:35,089|DEBUG|25504|6063|logger.py|132|node.py|5246:_get_ontap_disk_info| vmdisk-target-addr is unknown
2023-02-27 11:52:35,091|DEBUG|25504|6063|logger.py|132|node.py|5283:_get_ontap_disk_info| Matched disk NET-1.2 with diskpathname 0b.9
2023-02-27 11:52:35,092|DEBUG|25504|6063|logger.py|132|node.py|5246:_get_ontap_disk_info| vmdisk-target-addr is unknown
2023-02-27 11:52:35,093|DEBUG|25504|6063|logger.py|132|node.py|5283:_get_ontap_disk_info| Matched disk NET-1.3 with diskpathname 0b.8
2023-02-27 11:52:35,094|DEBUG|25504|6063|logger.py|132|node.py|5371:set_data_disk_ontap_disk_names| ONTAP disks: {'UA-HW_CLST-01_5.vmdk': {'ontap_name': 'NET-1.3', 'found': True, 'more_disk_info': {'diskpath_name': '0b.8', 'container_type': 'unassigned', 'capacity': 53687087104}}, 'UA-HW_CLST-01_6.vmdk': {'ontap_name': 'NET-1.2', 'found': True, 'more_disk_info': {'diskpath_name': '0b.9', 'container_type': 'unassigned', 'capacity': 53687087104}}}
2023-02-27 11:52:35,122| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=650, request_id='6063', time='2023-02-27 11:52:35.115479', evtype='FetchingONTAPDiskNames', category='node', level='Info', detail='Fetching ONTAP disk names for node "UA-HW_CLST-02"')>
2023-02-27 11:52:35,123|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:35.123382]> [failed:False] Fetching ONTAP disk names for node "UA-HW_CLST-02"
2023-02-27 11:52:35,131|DEBUG|25504|6063|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-02-27 11:52:35,403|DEBUG|25504|6063|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"UA-HW_CLST-02.vmdk","pool":"sdot_nfs_ds1","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"UA-HW_CLST-02_1.vmdk","pool":"sdot_nfs_ds1","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"UA-HW_CLST-02_2.vmdk","pool":"sdot_nfs_ds1","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"UA-HW_CLST-02_3.vmdk","pool":"sdot_nfs_ds1","total_diskspace":69632},{"capacity":69632,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"UA-HW_CLST-02_4.vmdk","pool":"sdot_nfs_ds1","total_diskspace":69632},{"capacity":51200,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"1","unit":"0"},"name":"UA-HW_CLST-02_5.vmdk","pool":"sdot_nfs_ds1","total_diskspace":51200},{"capacity":51200,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"1","unit":"0"},"name":"UA-HW_CLST-02_6.vmdk","pool":"sdot_nfs_ds1","total_diskspace":51200}]
2023-02-27 11:52:35,406|DEBUG|25504|6063|logger.py|132|node.py|5329:get_ontap_disk_names| Disks to find: ['UA-HW_CLST-02_5.vmdk', 'UA-HW_CLST-02_6.vmdk']
2023-02-27 11:52:35,494|DEBUG|25504|6063|logger.py|132|ZAPI.py|241:invoke|
API: "storage-disk-get-iter"
User: "admin"
Request:
<suppressed/>
Response:
<suppressed/>
2023-02-27 11:52:35,496|DEBUG|25504|6063|logger.py|132|node.py|5335:get_ontap_disk_names| unsupported_disks list: []
2023-02-27 11:52:35,497|DEBUG|25504|6063|logger.py|132|node.py|5338:get_ontap_disk_names| Disks to be attached_after_checking ['UA-HW_CLST-02_5.vmdk', 'UA-HW_CLST-02_6.vmdk']
2023-02-27 11:52:35,499|DEBUG|25504|6063|logger.py|132|node.py|5246:_get_ontap_disk_info| vmdisk-target-addr is unknown
2023-02-27 11:52:35,501|DEBUG|25504|6063|logger.py|132|node.py|5283:_get_ontap_disk_info| Matched disk NET-3.2 with diskpathname 0b.9
2023-02-27 11:52:35,502|DEBUG|25504|6063|logger.py|132|node.py|5246:_get_ontap_disk_info| vmdisk-target-addr is unknown
2023-02-27 11:52:35,504|DEBUG|25504|6063|logger.py|132|node.py|5283:_get_ontap_disk_info| Matched disk NET-3.3 with diskpathname 0b.8
2023-02-27 11:52:35,506|DEBUG|25504|6063|logger.py|132|node.py|5371:set_data_disk_ontap_disk_names| ONTAP disks: {'UA-HW_CLST-02_5.vmdk': {'ontap_name': 'NET-3.3', 'found': True, 'more_disk_info': {'diskpath_name': '0b.8', 'container_type': 'unassigned', 'capacity': 53687087104}}, 'UA-HW_CLST-02_6.vmdk': {'ontap_name': 'NET-3.2', 'found': True, 'more_disk_info': {'diskpath_name': '0b.9', 'container_type': 'unassigned', 'capacity': 53687087104}}}
2023-02-27 11:52:35,545|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1103:r_system_get_version| Cluster [UA-HW_CLST]: GET https://10.228.162.67:443/api/cluster/ response: {
  "version": {
    "full": "NetApp Release Lighthouse__9.13.1: Mon Feb 20 05:44:10 UTC 2023",
    "generation": 9,
    "major": 13,
    "minor": 1
  },
  "_links": {
    "self": {
      "href": "/api/cluster/"
    }
  }
}
2023-02-27 11:52:35,548|DEBUG|25504|6063|logger.py|132|ClusterRestApis.py|1111:r_system_get_version| Cluster [UA-HW_CLST]: GET https://10.228.162.67:443/api/cluster/ version_info: {'generation': '9', 'major': '13', 'minor': '1'}
2023-02-27 11:52:35,562| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=651, request_id='6063', time='2023-02-27 11:52:35.554268', evtype='ClusterVersionRefreshed', category='cluster', level='Info', detail='ONTAP version changed from "R9.13.1xN_230220_0000-vidconsole" to "9.13.1-vidconsole".')>
2023-02-27 11:52:35,563|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:35.563856]> [failed:False] ONTAP version changed from "R9.13.1xN_230220_0000-vidconsole" to "9.13.1-vidconsole".
2023-02-27 11:52:35,580| INFO|25504|6063|logger.py|132|event.py|134:__init__| <Event(id=652, request_id='6063', time='2023-02-27 11:52:35.573219', evtype='ClusterDeployCompleted', category='cluster', level='Info', detail='Cluster "UA-HW_CLST" is deployed and ready for use.')>
2023-02-27 11:52:35,581|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) running [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:35.581782]> [failed:False] Cluster "UA-HW_CLST" is deployed and ready for use.
2023-02-27 11:52:41,364|DEBUG|25504|6063|rest.py|173|response body: {"available_pdisks":[{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a652f63","total_diskspace":512078,"used_by":"Datastore-test"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a652f64","total_diskspace":512078,"used_by":"datastore-v"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a652f65","total_diskspace":819252,"used_by":"Datastore-test"},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a663545","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a663549","total_diskspace":3145728,"used_by":"sdot-b200-017-hdd"}],"available_pools":[{"allocation":25264,"capacity":3145472,"device_type":"SSD","location_type":"external","name":"sdot-b200-017-hdd","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a663549)"],"pool_type":"VMFS-5","pool_uid":"datastore-11358","provisioned_space":56586,"status":"Online"},{"allocation":103847,"capacity":512000,"device_type":"SSD","location_type":"external","name":"datastore-v","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a652f64)"],"pool_type":"VMFS-6","pool_uid":"datastore-11359","provisioned_space":103847,"status":"Online"},{"allocation":24128,"capacity":1331200,"device_type":"SSD","location_type":"external","name":"Datastore-test","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a652f65)","NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a652f63)"],"pool_type":"VMFS-6","pool_uid":"datastore-11360","provisioned_space":48867,"status":"Online"},{"allocation":2691372,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":15378762,"status":"Online"}],"available_portgroups":[{"name":"VM Network","network_uid":"network-15","vlanid":"0","vswitch":"vSwitch0"},{"name":"ONTAP_INTERNAL","network_uid":"network-1049","vlanid":"0","vswitch":"vSwitch1"},{"name":"ONTAP_EXTERNAL","network_uid":"network-1048","vlanid":"2480","vswitch":"vSwitch1"},{"name":"ONTAP_MANAGEMENT","network_uid":"network-1047","vlanid":"0","vswitch":"vSwitch1"}],"available_vms":[],"available_vswitches":[{"associated_adapter":["vmnic0"],"available_ports":7657,"mtu":"1500","name":"vSwitch0","portgroups":["VM Network","Management Network"],"total_ports":7680,"type":"Standard vSwitch"},{"associated_adapter":["vmnic2","vmnic3"],"available_ports":7657,"mtu":"9000","name":"vSwitch1","portgroups":["ONTAP_INTERNAL","ONTAP_EXTERNAL","ONTAP_MANAGEMENT"],"total_ports":7680,"type":"Standard vSwitch"}],"host_configuration":{"host_hardware_config":{"cores_in_use":4,"cpu_model":"Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz","cpu_speed_mhz":2394,"hardware_vendor":"Cisco Systems Inc","mem_size_mb":130960,"mem_usage_mb":48732,"num_cpu_cores":20,"num_hbas":2,"num_nics":4,"number_cpus":40,"product_version":"VMware ESXi 7.0.3"},"hypervisor_info":{"esx":{"ESX_license":"vSphere 7 Enterprise Plus","SDK_version":"6.7.0","api_version":"7.0.3.0","bios_release_date":"2018-07-11T00:00:00Z","bios_version":"B200M4.4.0.1c.0.0711181418","firewall_defaults_in":"blocked","firewall_defaults_out":"blocked","host":"sdot-b200-017.gdl.englab.netapp.com","hyperthreading":"active","hypervisor":"VMware ESXi 7.0.3 build-19193900","hypervisor_build":"19193900","hypervisor_name":"VMware ESXi","hypervisor_version":"7.0.3","model":"UCSB-B200-M4","moid":"host-11357:3025b4b7-b510-4852-98ed-38668b194b85","power_mgmt_policy":"Balanced","power_mgmt_technology":"ACPI P-states","reboot_required":"No","server":"sdot-b200-010a.gdl.englab.netapp.com","user":"administrator@vsphere.local","uuid":"00000000-0000-0000-0000-0025b5270110"}},"network_adapters":[{"associated_vswitch":"vSwitch0","enabled":true,"mac":"00:25:b5:27:01:11","name":"vmnic0","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"None","enabled":true,"mac":"00:25:b5:27:01:12","name":"vmnic1","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"vSwitch1","enabled":true,"mac":"00:25:b5:27:01:13","name":"vmnic2","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"vSwitch1","enabled":true,"mac":"00:25:b5:27:01:14","name":"vmnic3","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"}],"storage_adapters":[{"description":"(0000:08:00.0) Cisco UCS VIC Fnic Controller","logical_name":"vmhba0","product":"Cisco UCS VIC Fnic Controller"},{"description":"(0000:0f:00.0) Cisco UCS VIC Fnic Controller","logical_name":"vmhba1","product":"Cisco UCS VIC Fnic Controller"}]}}
2023-02-27 11:52:41,373|DEBUG|25504|6063|logger.py|132|host.py|645:get_host_support| {'storage_pools': [{'allocation': 25264, 'capacity': 3145472, 'device_type': 'SSD', 'location_type': 'external', 'name': 'sdot-b200-017-hdd', 'pdisks': ['NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a663549)'], 'pool_type': 'VMFS-5', 'pool_uid': 'datastore-11358', 'provisioned_space': 56586, 'status': 'Online'}, {'allocation': 103847, 'capacity': 512000, 'device_type': 'SSD', 'location_type': 'external', 'name': 'datastore-v', 'pdisks': ['NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a652f64)'], 'pool_type': 'VMFS-6', 'pool_uid': 'datastore-11359', 'provisioned_space': 103847, 'status': 'Online'}, {'allocation': 24128, 'capacity': 1331200, 'device_type': 'SSD', 'location_type': 'external', 'name': 'Datastore-test', 'pdisks': ['NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a652f65)', 'NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a652f63)'], 'pool_type': 'VMFS-6', 'pool_uid': 'datastore-11360', 'provisioned_space': 48867, 'status': 'Online'}, {'allocation': 2691372, 'capacity': 16934502, 'device_type': 'N/A', 'location_type': 'external', 'name': 'sdot_nfs_ds1', 'pdisks': ['N/A'], 'pool_type': 'NFS', 'pool_uid': 'datastore-2250', 'provisioned_space': 15378762, 'status': 'Online'}], 'storage_pdisks': [{'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763671763f4b395a652f63', 'total_diskspace': 512078, 'used_by': 'Datastore-test'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763671763f4b395a652f64', 'total_diskspace': 512078, 'used_by': 'datastore-v'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763671763f4b395a652f65', 'total_diskspace': 819252, 'used_by': 'Datastore-test'}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763671763f4b395a663545', 'total_diskspace': 10240, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763671763f4b395a663549', 'total_diskspace': 3145728, 'used_by': 'sdot-b200-017-hdd'}], 'network_portgroups': [{'name': 'VM Network', 'network_uid': 'network-15', 'vlanid': '0', 'vswitch': 'vSwitch0'}, {'name': 'ONTAP_INTERNAL', 'network_uid': 'network-1049', 'vlanid': '0', 'vswitch': 'vSwitch1'}, {'name': 'ONTAP_EXTERNAL', 'network_uid': 'network-1048', 'vlanid': '2480', 'vswitch': 'vSwitch1'}, {'name': 'ONTAP_MANAGEMENT', 'network_uid': 'network-1047', 'vlanid': '0', 'vswitch': 'vSwitch1'}], 'vswitch_config': [{'associated_adapter': ['vmnic0'], 'available_ports': 7657, 'mtu': '1500', 'name': 'vSwitch0', 'portgroups': ['VM Network', 'Management Network'], 'total_ports': 7680, 'type': 'Standard vSwitch'}, {'associated_adapter': ['vmnic2', 'vmnic3'], 'available_ports': 7657, 'mtu': '9000', 'name': 'vSwitch1', 'portgroups': ['ONTAP_INTERNAL', 'ONTAP_EXTERNAL', 'ONTAP_MANAGEMENT'], 'total_ports': 7680, 'type': 'Standard vSwitch'}], 'physical_config': {'network_adapters': [{'associated_vswitch': 'vSwitch0', 'enabled': True, 'mac': '00:25:b5:27:01:11', 'name': 'vmnic0', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'None', 'enabled': True, 'mac': '00:25:b5:27:01:12', 'name': 'vmnic1', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'vSwitch1', 'enabled': True, 'mac': '00:25:b5:27:01:13', 'name': 'vmnic2', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'vSwitch1', 'enabled': True, 'mac': '00:25:b5:27:01:14', 'name': 'vmnic3', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}], 'storage_adapters': [{'description': '(0000:08:00.0) Cisco UCS VIC Fnic Controller', 'logical_name': 'vmhba0', 'product': 'Cisco UCS VIC Fnic Controller'}, {'description': '(0000:0f:00.0) Cisco UCS VIC Fnic Controller', 'logical_name': 'vmhba1', 'product': 'Cisco UCS VIC Fnic Controller'}], 'host_hardware_config': {'cores_in_use': 4, 'cpu_model': 'Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz', 'cpu_speed_mhz': 2394, 'hardware_vendor': 'Cisco Systems Inc', 'mem_size_mb': 130960, 'mem_usage_mb': 48732, 'num_cpu_cores': 20, 'num_hbas': 2, 'num_nics': 4, 'number_cpus': 40, 'product_version': 'VMware ESXi 7.0.3'}, 'host_software_config': {}, 'hypervisor_config': {'esx': {'esx_license': 'vSphere 7 Enterprise Plus', 'sdk_version': '6.7.0', 'api_version': '7.0.3.0', 'bios_release_date': '2018-07-11T00:00:00Z', 'bios_version': 'B200M4.4.0.1c.0.0711181418', 'firewall_defaults_in': 'blocked', 'firewall_defaults_out': 'blocked', 'host': 'sdot-b200-017.gdl.englab.netapp.com', 'hyperthreading': 'active', 'hypervisor': 'VMware ESXi 7.0.3 build-19193900', 'hypervisor_build': '19193900', 'hypervisor_name': 'VMware ESXi', 'hypervisor_version': '7.0.3', 'model': 'UCSB-B200-M4', 'moid': 'host-11357:3025b4b7-b510-4852-98ed-38668b194b85', 'power_mgmt_policy': 'Balanced', 'power_mgmt_technology': 'ACPI P-states', 'reboot_required': 'No', 'server': 'sdot-b200-010a.gdl.englab.netapp.com', 'user': 'administrator@vsphere.local', 'uuid': '00000000-0000-0000-0000-0025b5270110'}}}}
2023-02-27 11:52:41,397|DEBUG|25504|6063|job.py|364|Updated <Job (CLUSTER_CREATE) success [id:027ddf4e-b693-11ed-b76c-000c290ef0a1 rid:6063 mod:2023-02-27 11:52:41.397456]> [failed:False] Host "sdot-b200-018.gdl.englab.netapp.com" loading cache
2023-02-27 11:52:47,946|DEBUG|25504|6063|rest.py|173|response body: {"available_pdisks":[{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a663544","total_diskspace":10240,"used_by":""},{"adapter_name":"vmhba1","description":"NETAPP Fibre Channel Disk ","device_type":"unknown","logical_name":"","name":"naa.600a098051763671763f4b395a663547","total_diskspace":3145728,"used_by":"sdot-b200-018-hdd"}],"available_pools":[{"allocation":2829062,"capacity":3145472,"device_type":"SSD","location_type":"external","name":"sdot-b200-018-hdd","pdisks":["NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a663547)"],"pool_type":"VMFS-5","pool_uid":"datastore-11394","provisioned_space":3009776,"status":"Online"},{"allocation":2691372,"capacity":16934502,"device_type":"N\/A","location_type":"external","name":"sdot_nfs_ds1","pdisks":["N\/A"],"pool_type":"NFS","pool_uid":"datastore-2250","provisioned_space":15378762,"status":"Online"}],"available_portgroups":[{"name":"VM Network","network_uid":"network-15","vlanid":"0","vswitch":"vSwitch0"},{"name":"ONTAP_INTERNAL","network_uid":"network-1049","vlanid":"0","vswitch":"vSwitch1"},{"name":"ONTAP_EXTERNAL","network_uid":"network-1048","vlanid":"2480","vswitch":"vSwitch1"},{"name":"ONTAP_MANAGEMENT","network_uid":"network-1047","vlanid":"0","vswitch":"vSwitch1"}],"available_vms":[],"available_vswitches":[{"associated_adapter":["vmnic0"],"available_ports":7658,"mtu":"1500","name":"vSwitch0","portgroups":["VM Network","Management Network"],"total_ports":7680,"type":"Standard vSwitch"},{"associated_adapter":["vmnic3","vmnic2"],"available_ports":7658,"mtu":"9000","name":"vSwitch1","portgroups":["ONTAP_INTERNAL","ONTAP_EXTERNAL","ONTAP_MANAGEMENT"],"total_ports":7680,"type":"Standard vSwitch"}],"host_configuration":{"host_hardware_config":{"cores_in_use":6,"cpu_model":"Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz","cpu_speed_mhz":2394,"hardware_vendor":"Cisco Systems Inc","mem_size_mb":130960,"mem_usage_mb":69314,"num_cpu_cores":20,"num_hbas":2,"num_nics":4,"number_cpus":40,"product_version":"VMware ESXi 7.0.3"},"hypervisor_info":{"esx":{"ESX_license":"vSphere 7 Enterprise Plus","SDK_version":"6.7.0","api_version":"7.0.3.0","bios_release_date":"2018-07-11T00:00:00Z","bios_version":"B200M4.4.0.1c.0.0711181418","firewall_defaults_in":"blocked","firewall_defaults_out":"blocked","host":"sdot-b200-018.gdl.englab.netapp.com","hyperthreading":"active","hypervisor":"VMware ESXi 7.0.3 build-19193900","hypervisor_build":"19193900","hypervisor_name":"VMware ESXi","hypervisor_version":"7.0.3","model":"UCSB-B200-M4","moid":"host-11393:3025b4b7-b510-4852-98ed-38668b194b85","power_mgmt_policy":"Balanced","power_mgmt_technology":"ACPI P-states","reboot_required":"No","server":"sdot-b200-010a.gdl.englab.netapp.com","user":"administrator@vsphere.local","uuid":"00000000-0000-0000-0000-0025b5270120"}},"network_adapters":[{"associated_vswitch":"vSwitch0","enabled":true,"mac":"00:25:b5:27:01:21","name":"vmnic0","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"None","enabled":true,"mac":"00:25:b5:27:01:22","name":"vmnic1","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"vSwitch1","enabled":true,"mac":"00:25:b5:27:01:23","name":"vmnic2","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"},{"associated_vswitch":"vSwitch1","enabled":true,"mac":"00:25:b5:27:01:24","name":"vmnic3","speed":"20000","vendor":"Cisco Systems Inc Cisco VIC Ethernet NIC"}],"storage_adapters":[{"description":"(0000:08:00.0) Cisco UCS VIC Fnic Controller","logical_name":"vmhba0","product":"Cisco UCS VIC Fnic Controller"},{"description":"(0000:0f:00.0) Cisco UCS VIC Fnic Controller","logical_name":"vmhba1","product":"Cisco UCS VIC Fnic Controller"}]}}
2023-02-27 11:52:47,956|DEBUG|25504|6063|logger.py|132|host.py|645:get_host_support| {'storage_pools': [{'allocation': 2829062, 'capacity': 3145472, 'device_type': 'SSD', 'location_type': 'external', 'name': 'sdot-b200-018-hdd', 'pdisks': ['NETAPP Fibre Channel Disk (naa.600a098051763671763f4b395a663547)'], 'pool_type': 'VMFS-5', 'pool_uid': 'datastore-11394', 'provisioned_space': 3009776, 'status': 'Online'}, {'allocation': 2691372, 'capacity': 16934502, 'device_type': 'N/A', 'location_type': 'external', 'name': 'sdot_nfs_ds1', 'pdisks': ['N/A'], 'pool_type': 'NFS', 'pool_uid': 'datastore-2250', 'provisioned_space': 15378762, 'status': 'Online'}], 'storage_pdisks': [{'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763671763f4b395a663544', 'total_diskspace': 10240, 'used_by': ''}, {'adapter_name': 'vmhba1', 'description': 'NETAPP Fibre Channel Disk ', 'device_type': 'unknown', 'logical_name': '', 'name': 'naa.600a098051763671763f4b395a663547', 'total_diskspace': 3145728, 'used_by': 'sdot-b200-018-hdd'}], 'network_portgroups': [{'name': 'VM Network', 'network_uid': 'network-15', 'vlanid': '0', 'vswitch': 'vSwitch0'}, {'name': 'ONTAP_INTERNAL', 'network_uid': 'network-1049', 'vlanid': '0', 'vswitch': 'vSwitch1'}, {'name': 'ONTAP_EXTERNAL', 'network_uid': 'network-1048', 'vlanid': '2480', 'vswitch': 'vSwitch1'}, {'name': 'ONTAP_MANAGEMENT', 'network_uid': 'network-1047', 'vlanid': '0', 'vswitch': 'vSwitch1'}], 'vswitch_config': [{'associated_adapter': ['vmnic0'], 'available_ports': 7658, 'mtu': '1500', 'name': 'vSwitch0', 'portgroups': ['VM Network', 'Management Network'], 'total_ports': 7680, 'type': 'Standard vSwitch'}, {'associated_adapter': ['vmnic3', 'vmnic2'], 'available_ports': 7658, 'mtu': '9000', 'name': 'vSwitch1', 'portgroups': ['ONTAP_INTERNAL', 'ONTAP_EXTERNAL', 'ONTAP_MANAGEMENT'], 'total_ports': 7680, 'type': 'Standard vSwitch'}], 'physical_config': {'network_adapters': [{'associated_vswitch': 'vSwitch0', 'enabled': True, 'mac': '00:25:b5:27:01:21', 'name': 'vmnic0', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'None', 'enabled': True, 'mac': '00:25:b5:27:01:22', 'name': 'vmnic1', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'vSwitch1', 'enabled': True, 'mac': '00:25:b5:27:01:23', 'name': 'vmnic2', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}, {'associated_vswitch': 'vSwitch1', 'enabled': True, 'mac': '00:25:b5:27:01:24', 'name': 'vmnic3', 'speed': '20000', 'vendor': 'Cisco Systems Inc Cisco VIC Ethernet NIC'}], 'storage_adapters': [{'description': '(0000:08:00.0) Cisco UCS VIC Fnic Controller', 'logical_name': 'vmhba0', 'product': 'Cisco UCS VIC Fnic Controller'}, {'description': '(0000:0f:00.0) Cisco UCS VIC Fnic Controller', 'logical_name': 'vmhba1', 'product': 'Cisco UCS VIC Fnic Controller'}], 'host_hardware_config': {'cores_in_use': 6, 'cpu_model': 'Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz', 'cpu_speed_mhz': 2394, 'hardware_vendor': 'Cisco Systems Inc', 'mem_size_mb': 130960, 'mem_usage_mb': 69314, 'num_cpu_cores': 20, 'num_hbas': 2, 'num_nics': 4, 'number_cpus': 40, 'product_version': 'VMware ESXi 7.0.3'}, 'host_software_config': {}, 'hypervisor_config': {'esx': {'esx_license': 'vSphere 7 Enterprise Plus', 'sdk_version': '6.7.0', 'api_version': '7.0.3.0', 'bios_release_date': '2018-07-11T00:00:00Z', 'bios_version': 'B200M4.4.0.1c.0.0711181418', 'firewall_defaults_in': 'blocked', 'firewall_defaults_out': 'blocked', 'host': 'sdot-b200-018.gdl.englab.netapp.com', 'hyperthreading': 'active', 'hypervisor': 'VMware ESXi 7.0.3 build-19193900', 'hypervisor_build': '19193900', 'hypervisor_name': 'VMware ESXi', 'hypervisor_version': '7.0.3', 'model': 'UCSB-B200-M4', 'moid': 'host-11393:3025b4b7-b510-4852-98ed-38668b194b85', 'power_mgmt_policy': 'Balanced', 'power_mgmt_technology': 'ACPI P-states', 'reboot_required': 'No', 'server': 'sdot-b200-010a.gdl.englab.netapp.com', 'user': 'administrator@vsphere.local', 'uuid': '00000000-0000-0000-0000-0025b5270120'}}}}
2023-02-27 11:52:48,083|DEBUG|25504|6063|logger.py|132|Mediator.py|41:get_all_iscsi_tgts_status| url (http://localhost:3080/targets/status), output ({"NumberOfTargets":1,"NumberOfTargetsWith0ActiveConnections":0,"NumberOfTargetsWith1ActiveConnections":0,"NumberOfTargetsWith2ActiveConnections":1,"Targets":[{"ActiveConnections":2,"Connections":[{"ip":"10.228.161.50","read_io_kb":"1386","write_io_kb":"2180"},{"ip":"10.228.161.228","read_io_kb":"1500","write_io_kb":"2500"}],"Name":"select000000","lun0_last_access":"Mon Feb 27 11:52:48 2023","lun0_last_modify":"Mon Feb 27 11:52:46 2023","lun1_last_access":"Mon Feb 27 11:52:47 2023","lun1_last_modify":"Mon Feb 27 11:52:46 2023"}]})
2023-02-27 11:52:48,133|DEBUG|25504|6063|logger.py|132|Mediator.py|102:get_iscsi_tgt| url (http://localhost:3080/targets/select000000), output ({"INITIATORs":["iqn.1994-09.org.freebsd:*"],"LUNs":{"0":{"dev":"dev1-0","path":"\/mnt\/iscsi_space\/mb.select000000.0","scsiid":"lun0","scsisn":"lun0","type":"fileio"},"1":{"dev":"dev1-1","path":"\/mnt\/iscsi_space\/mb.select000000.1","scsiid":"lun1","scsisn":"lun1","type":"fileio"}},"iqn":"iqn.2012-05.local:mailbox.target.select000000","stack":"scst"})
