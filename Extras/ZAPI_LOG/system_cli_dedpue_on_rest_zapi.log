}
2023-02-16 04:30:38,663|DEBUG|31806|45545|logger.py|132|ClusterRestApis.py|629:r_get_cluster_identity| Cluster [single_node_cluster1]: "GET https://10.228.161.170:443/api/cluster/?fields=**" cluster_identity info:{'cluster-name': 'single_node_cluster1', 'cluster-uuid': 'd0124614-963d-11ed-b252-000c29c2b6e0', 'cluster-location': None, 'cluster-contact': None}
2023-02-16 04:30:38,668| INFO|31806|45545|logger.py|132|cluster.py|4001:patch_cluster_identity| Cluster [single_node_cluster1]: cluster's node count validation successful
2023-02-16 04:30:38,671| INFO|31806|45545|logger.py|132|cluster.py|4012:patch_cluster_identity| Cluster [single_node_cluster1]: cluster identity refreshed
2023-02-16 04:30:43,553|DEBUG|31806|0|coroutine.py|177|RESTAPI [controllers.job:get_job] executing
2023-02-16 04:30:43,554|DEBUG|31806|0|coroutine.py|190|Registered Myself: current tasks [{140668997928344: <CoroutineRecord (asynchronous.cluster_tasks:refresh_cluster) [ACTIVE]>, 140669008550904: <CoroutineRecord (controllers.job:get_job) [ACTIVE]>}]
2023-02-16 04:30:43,559| INFO|31806|45553|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/jobs/a8d19a78-adb2-11ed-9a00-000c29c2b6e0?fields=*,last_modified,message
2023-02-16 04:30:43,561| INFO|31806|45553|restapi.py|263|GET http://127.0.0.1:8080/api/v3/jobs/a8d19a78-adb2-11ed-9a00-000c29c2b6e0?fields=*,last_modified,message returned in 6ms with status HTTPStatus.OK (BA)
2023-02-16 04:30:43,644|DEBUG|31806|45545|logger.py|132|ClusterRestApis.py|1475:r_system_cli_version| Cluster [single_node_cluster1]: "GET https://10.228.161.170:443/api/private/cli" response: {
  "output": "1 entry was acted on.

Node: single_node_cluster1-01
NetApp Release devN_220829_0200: Mon Aug 29 02:30:41 EDT 2022   <1>

"
}
2023-02-16 04:30:43,645|DEBUG|31806|45545|logger.py|132|ClusterRestApis.py|1479:r_system_cli_version| Cluster [single_node_cluster1]: "GET https://10.228.161.170:443/api/private/cli" system CLI node version: 1 entry was acted on.

Node: single_node_cluster1-01
NetApp Release devN_220829_0200: Mon Aug 29 02:30:41 EDT 2022   <1>


2023-02-16 04:30:49,846|DEBUG|31806|0|coroutine.py|177|RESTAPI [controllers.job:get_job] executing
2023-02-16 04:30:49,847|DEBUG|31806|0|coroutine.py|190|Registered Myself: current tasks [{140668997928344: <CoroutineRecord (asynchronous.cluster_tasks:refresh_cluster) [ACTIVE]>, 140668997927256: <CoroutineRecord (controllers.job:get_job) [ACTIVE]>}]
2023-02-16 04:30:49,855| INFO|31806|45554|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/jobs/a8d19a78-adb2-11ed-9a00-000c29c2b6e0?fields=*,last_modified,message
2023-02-16 04:30:49,857| INFO|31806|45554|restapi.py|263|GET http://127.0.0.1:8080/api/v3/jobs/a8d19a78-adb2-11ed-9a00-000c29c2b6e0?fields=*,last_modified,message returned in 9ms with status HTTPStatus.OK (BA)
2023-02-16 04:30:57,519|DEBUG|31806|45545|logger.py|132|ClusterRestApis.py|1555:r_system_cli_inline_dedupe_on| Cluster [single_node_cluster1]: "GET https://10.228.161.170:443/api/private/cli" response: {
  "output": "

"
}
2023-02-16 04:30:58,744|DEBUG|31806|45545|logger.py|132|ClusterRestApis.py|1558:r_system_cli_inline_dedupe_on| Cluster [single_node_cluster1]: "GET https://10.228.161.170:443/api/private/cli" System CLI, Inline Dedupe and Inline Compression is ON
2023-02-16 04:31:21,927|DEBUG|31806|0|coroutine.py|190|Registered Myself: current tasks [{140668997928344: <CoroutineRecord (asynchronous.cluster_tasks:refresh_cluster) [ACTIVE]>, 140668997928616: <CoroutineRecord (asynchronous.crontab:create_mediator) [ACTIVE]>}]
2023-02-16 04:31:21,935|DEBUG|31806|0|coroutine.py|177|RESTAPI [controllers.job:get_job] executing
2023-02-16 04:31:21,936|DEBUG|31806|0|coroutine.py|190|Registered Myself: current tasks [{140668997928344: <CoroutineRecord (asynchronous.cluster_tasks:refresh_cluster) [ACTIVE]>, 140668997928616: <CoroutineRecord (controllers.job:get_job) [ACTIVE]>}]
2023-02-16 04:31:21,940| INFO|31806|45555|restapi.py|231|Processing GET http://127.0.0.1:8080/api/v3/jobs/a8d19a78-adb2-11ed-9a00-000c29c2b6e0?fields=*,last_modified,message
2023-02-16 04:31:21,942| INFO|31806|45555|restapi.py|263|GET http://127.0.0.1:8080/api/v3/jobs/a8d19a78-adb2-11ed-9a00-000c29c2b6e0?fields=*,last_modified,message returned in 6ms with status HTTPStatus.OK (BA)
2023-02-16 04:31:22,132| INFO|31806|45545|logger.py|132|ZAPI.py|241:invoke|
API: "system-cli"
User: "admin"
Request:
<?xml version="1.0" ?>
<!DOCTYPE netapp
  SYSTEM 'file:/etc/netapp_filer.dtd'>
<netapp nmsdk_language="Python" nmsdk_platform="Debian GNU/Linux 10
 \l unknown" nmsdk_version="5.4" version="1.40" xmlns="http://www.netapp.com/filer/admin">
        <system-cli>
                <priv>diagnostic</priv>
                <args>
                        <arg>system</arg>
                        <arg>node</arg>
                        <arg>run</arg>
                        <arg>-node</arg>
                        <arg>single_node_cluster1-01</arg>
                        <arg>options</arg>
                        <arg>sis.idedup_allow_non_aff_hya</arg>
                        <arg>on</arg>
                </args>
        </system-cli>
</netapp>

Response:
<?xml version="1.0" ?>
<!DOCTYPE netapp
  SYSTEM 'file:/etc/netapp_gx.dtd'>
<netapp version="1.230" xmlns="http://www.netapp.com/filer/admin">


        <results status="passed">
                <cli-output>

</cli-output>
                <cli-result-value>1</cli-result-value>
        </results>
</netapp>

2023-02-16 04:31:51,617|DEBUG|31806|45545|logger.py|132|ClusterZapis.py|865:z_system_cli_inline_dedupe_on| Cluster [single_node_cluster1]: System CLI, Inline Dedupe and Inline Compression is ON
2023-02-16 04:32:29,544|DEBUG|31806|45545|logger.py|132|ClusterRestApis.py|1103:r_system_get_version| Cluster [single_node_cluster1]: GET https://10.228.161.170:443/api/cluster/ response: {
  "version": {
    "full": "NetApp Release Lighthouse__9.13.0: Mon Aug 29 06:30:41 UTC 2022",
    "generation": 9,
    "major": 13,
    "minor": 0
  },
  "_links": {
    "self": {
      "href": "/api/cluster/"
    }
  }
}
2023-02-16 04:32:29,546|DEBUG|31806|45545|logger.py|132|ClusterRestApis.py|1111:r_system_get_version| Cluster [single_node_cluster1]: GET https://10.228.161.170:443/api/cluster/ version_info: {'generation': '9', 'major': '13', 'minor': '0'}
2023-02-16 04:32:29,560|DEBUG|31806|45545|fault.py|147|Unable to find a registered fault for zapi.system_node_get_iter.
2023-02-16 04:32:29,598|DEBUG|31806|45545|logger.py|132|ZAPI.py|241:invoke|
API: "system-node-get-iter"
User: "admin"
Request:
<?xml version="1.0" ?>
<!DOCTYPE netapp
  SYSTEM 'file:/etc/netapp_filer.dtd'>
<netapp nmsdk_language="Python" nmsdk_platform="Debian GNU/Linux 10
 \l unknown" nmsdk_version="5.4" version="1.40" xmlns="http://www.netapp.com/filer/admin">
        <system-node-get-iter/>
</netapp>

Response:
<?xml version="1.0" ?>
<!DOCTYPE netapp
  SYSTEM 'file:/etc/netapp_gx.dtd'>
<netapp version="1.230" xmlns="http://www.netapp.com/filer/admin">


        <results status="passed">
                <attributes-list>
                        <node-details-info>
                                <cpu-busytime>1923</cpu-busytime>
                                <env-failed-fan-count>0</env-failed-fan-count>
                                <env-failed-fan-message>There are no failed fans.</env-failed-fan-message>
                                <env-failed-power-supply-count>0</env-failed-power-supply-count>
                                <env-failed-power-supply-message>There are no failed power supplies.</env-failed-power-supply-message>
                                <env-over-temperature>false</env-over-temperature>
                                <is-all-flash-optimized>false</is-all-flash-optimized>
                                <is-all-flash-select-optimized>false</is-all-flash-select-optimized>
                                <is-capacity-optimized>false</is-capacity-optimized>
                                <is-cloud-optimized>false</is-cloud-optimized>
                                <is-diff-svcs>false</is-diff-svcs>
                                <is-epsilon-node>true</is-epsilon-node>
                                <is-node-cluster-eligible>true</is-node-cluster-eligible>
                                <is-node-healthy>true</is-node-healthy>
                                <is-perf-optimized>false</is-perf-optimized>
                                <maximum-aggregate-size>219902325555200</maximum-aggregate-size>
                                <maximum-number-of-volumes>1000</maximum-number-of-volumes>
                                <maximum-volume-size>329853488332800</maximum-volume-size>
                                <node>single_node_cluster1-01</node>
                                <node-location/>
                                <node-model>FDvM300</node-model>
                                <node-nvram-id>2443885357</node-nvram-id>
                                <node-owner/>
                                <node-serial-number>99887766554433221145</node-serial-number>
                                <node-storage-configuration>unknown</node-storage-configuration>
                                <node-system-id>2443885357</node-system-id>
                                <node-uptime>56348</node-uptime>
                                <node-uuid>d01393c0-963d-11ed-b252-000c29c2b6e0</node-uuid>
                                <node-vendor>NetApp</node-vendor>
                                <nvram-battery-status>battery_ok</nvram-battery-status>
                                <product-version>NetApp Release Lighthouse__9.13.0: Mon Aug 29 06:30:41 UTC 2022</product-version>
                                <sas2-sas3-mixed-stack-support>none</sas2-sas3-mixed-stack-support>
                                <vmhost-info>
                                        <vm-uuid>421663f0-8114-1385-7706-e35d37ec942f</vm-uuid>
                                        <vmhost-error>Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the &quot;system node virtual-machine hypervisor modify-credentials&quot; command.</vmhost-error>
                                        <vmhost-hardware-vendor>VMware, Inc.</vmhost-hardware-vendor>
                                        <vmhost-model>VMware Virtual Platform</vmhost-model>
                                        <vmhost-software-vendor>NetApp</vmhost-software-vendor>
                                </vmhost-info>
                        </node-details-info>
                </attributes-list>
                <num-records>1</num-records>
        </results>
</netapp>

2023-02-16 04:32:29,599|DEBUG|31806|45545|logger.py|132|ClusterZapis.py|245:z_system_node_get_iter| Cluster [single_node_cluster1]: got system node get iter: [[{'cpu-busytime': '1923', 'env-failed-fan-count': '0', 'env-failed-fan-message': 'There are no failed fans.', 'env-failed-power-supply-count': '0', 'env-failed-power-supply-message': 'There are no failed power supplies.', 'env-over-temperature': 'false', 'is-all-flash-optimized': 'false', 'is-all-flash-select-optimized': 'false', 'is-capacity-optimized': 'false', 'is-cloud-optimized': 'false', 'is-diff-svcs': 'false', 'is-epsilon-node': 'true', 'is-node-cluster-eligible': 'true', 'is-node-healthy': 'true', 'is-perf-optimized': 'false', 'maximum-aggregate-size': '219902325555200', 'maximum-number-of-volumes': '1000', 'maximum-volume-size': '329853488332800', 'node': 'single_node_cluster1-01', 'node-location': None, 'node-model': 'FDvM300', 'node-nvram-id': '2443885357', 'node-owner': None, 'node-serial-number': '99887766554433221145', 'node-storage-configuration': 'unknown', 'node-system-id': '2443885357', 'node-uptime': '56348', 'node-uuid': 'd01393c0-963d-11ed-b252-000c29c2b6e0', 'node-vendor': 'NetApp', 'nvram-battery-status': 'battery_ok', 'product-version': 'NetApp Release Lighthouse__9.13.0: Mon Aug 29 06:30:41 UTC 2022', 'sas2-sas3-mixed-stack-support': 'none', 'vmhost-info': {'vm-uuid': '421663f0-8114-1385-7706-e35d37ec942f', 'vmhost-error': 'Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the "system node virtual-machine hypervisor modify-credentials" command.', 'vmhost-hardware-vendor': 'VMware, Inc.', 'vmhost-model': 'VMware Virtual Platform', 'vmhost-software-vendor': 'NetApp'}}]]
2023-02-16 04:32:29,605| INFO|31806|45545|logger.py|132|node.py|3494:patch_node_identity| Node [single_node_cluster1-01] Cluster [single_node_cluster1]: node identity refreshed
2023-02-16 04:32:29,668|DEBUG|31806|45545|logger.py|132|ZAPI.py|241:invoke|
API: "net-interface-get-iter"
User: "admin"
Request:
<?xml version="1.0" ?>
<!DOCTYPE netapp
  SYSTEM 'file:/etc/netapp_filer.dtd'>
<netapp nmsdk_language="Python" nmsdk_platform="Debian GNU/Linux 10
 \l unknown" nmsdk_version="5.4" version="1.40" xmlns="http://www.netapp.com/filer/admin">
        <net-interface-get-iter>
                <query>
                        <net-interface-info>
                                <role>node_mgmt</role>
                                <current-node>single_node_cluster1-01</current-node>
                        </net-interface-info>
                </query>
        </net-interface-get-iter>
</netapp>

Response:
<?xml version="1.0" ?>
<!DOCTYPE netapp
  SYSTEM 'file:/etc/netapp_gx.dtd'>
<netapp version="1.230" xmlns="http://www.netapp.com/filer/admin">


        <results status="passed">
                <attributes-list>
                        <net-interface-info>
                                <address>10.228.161.198</address>
                                <address-family>ipv4</address-family>
                                <administrative-status>up</administrative-status>
                                <current-node>single_node_cluster1-01</current-node>
                                <current-port>e0a</current-port>
                                <data-protocols>
                                        <data-protocol>none</data-protocol>
                                </data-protocols>
                                <dns-domain-name>none</dns-domain-name>
                                <failover-group>Default</failover-group>
                                <failover-policy>local_only</failover-policy>
                                <firewall-policy>mgmt</firewall-policy>
                                <home-node>single_node_cluster1-01</home-node>
                                <home-port>e0a</home-port>
                                <interface-name>single_node_cluster1-01_mgmt1</interface-name>
                                <ipspace>Default</ipspace>
                                <is-auto-revert>true</is-auto-revert>
                                <is-home>true</is-home>
                                <is-vip>false</is-vip>
                                <lif-uuid>82ea662f-963f-11ed-9230-00a0b89403a8</lif-uuid>
                                <listen-for-dns-query>false</listen-for-dns-query>
                                <netmask>255.255.252.0</netmask>
                                <netmask-length>22</netmask-length>
                                <operational-status>up</operational-status>
                                <role>node_mgmt</role>
                                <service-names>
                                        <lif-service-name>management_core</lif-service-name>
                                        <lif-service-name>management_autosupport</lif-service-name>
                                        <lif-service-name>management_ssh</lif-service-name>
                                        <lif-service-name>management_https</lif-service-name>
                                        <lif-service-name>management_ems</lif-service-name>
                                        <lif-service-name>management_ntp_client</lif-service-name>
                                        <lif-service-name>management_dns_client</lif-service-name>
                                        <lif-service-name>management_ad_client</lif-service-name>
                                        <lif-service-name>management_ldap_client</lif-service-name>
                                        <lif-service-name>management_nis_client</lif-service-name>
                                        <lif-service-name>management_http</lif-service-name>
                                        <lif-service-name>backup_ndmp_control</lif-service-name>
                                        <lif-service-name>management_snmp_server</lif-service-name>
                                        <lif-service-name>management_ntp_server</lif-service-name>
                                        <lif-service-name>management_log_forwarding</lif-service-name>
                                </service-names>
                                <service-policy>default-management</service-policy>
                                <use-failover-group>unused</use-failover-group>
                                <vserver>single_node_cluster1</vserver>
                        </net-interface-info>
                </attributes-list>
                <num-records>1</num-records>
        </results>
</netapp>

2023-02-16 04:32:29,670| INFO|31806|45545|logger.py|132|node.py|3400:patch_node_management_network| Node [single_node_cluster1-01] Cluster [single_node_cluster1]: node management network information refreshed
2023-02-16 04:32:29,677|DEBUG|31806|45545|logger.py|132|client_api_helper.py|221:get_vm_info| get_vm_info executing
2023-02-16 04:32:29,981|DEBUG|31806|45545|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-008.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"cluster_uuid":"d0124614-963d-11ed-b252-000c29c2b6e0","node_uuid":"d01393c0-963d-11ed-b252-000c29c2b6e0"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"single_node_cluster1-01","obj_id":"vm-11047","state":"poweredon","vm_uid":"50160e79-1635-0b14-362e-17ef3eba8b9b"}
2023-02-16 04:32:29,986| INFO|31806|45545|logger.py|132|node.py|3582:refresh_vm_name| Node [single_node_cluster1-01] Cluster [single_node_cluster1]: Node vm_name information refresh successful.
2023-02-16 04:32:29,989|DEBUG|31806|45545|logger.py|132|client_api_helper.py|221:get_vm_info| get_vm_info executing
2023-02-16 04:32:30,277|DEBUG|31806|45545|rest.py|173|response body: {"cpus":"4","host_id":"sdot-b200-008.gdl.englab.netapp.com","hw_version":"vmx-14","memory":"16384","metadata":{"cluster_uuid":"d0124614-963d-11ed-b252-000c29c2b6e0","node_uuid":"d01393c0-963d-11ed-b252-000c29c2b6e0"},"mgmt_server_uid":"3025b4b7-b510-4852-98ed-38668b194b85","name":"single_node_cluster1-01","obj_id":"vm-11047","state":"poweredon","vm_uid":"50160e79-1635-0b14-362e-17ef3eba8b9b"}
2023-02-16 04:32:30,280| INFO|31806|45545|logger.py|132|node.py|3615:refresh_state| Node [single_node_cluster1-01] Cluster [single_node_cluster1]: Node state information refresh successful.
2023-02-16 04:32:30,281|DEBUG|31806|45545|logger.py|132|client_api_helper.py|558:vm_get_vm_vnics| vm_get_vm_vnics executing
2023-02-16 04:32:30,636|DEBUG|31806|45545|rest.py|173|response body: [{"mac":"00:A0:B8:94:03:A8","model":"VirtualVmxnet3","name":"Network adapter 1","network_name":"ONTAP-Management"},{"mac":"00:A0:B8:94:C8:AB","model":"VirtualVmxnet3","name":"Network adapter 2","network_name":"ONTAP-External"},{"mac":"00:A0:B8:94:1B:9A","model":"VirtualVmxnet3","name":"Network adapter 3","network_name":"ONTAP-External"}]
2023-02-16 04:32:30,648| INFO|31806|45545|logger.py|132|node.py|3681:refresh_network_info| Node [single_node_cluster1-01] Cluster [single_node_cluster1]: Network information refresh successful.
2023-02-16 04:32:30,649|DEBUG|31806|45545|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-02-16 04:32:30,903|DEBUG|31806|45545|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01_1.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"single_node_cluster1-01_2.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01_3.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":69632},{"capacity":51200,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01_4.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":51200}]
2023-02-16 04:32:30,916| INFO|31806|45545|logger.py|132|node.py|3821:refresh_storage_pool_info| Node [single_node_cluster1-01] Cluster [single_node_cluster1]: Node storage pool information refresh successful.
2023-02-16 04:32:30,924|DEBUG|31806|45545|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-02-16 04:32:31,237|DEBUG|31806|45545|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01_1.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"single_node_cluster1-01_2.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01_3.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":69632},{"capacity":51200,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01_4.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":51200}]
2023-02-16 04:32:31,244|DEBUG|31806|45545|logger.py|132|client_api_helper.py|570:vm_get_vm_vdisks| vm_get_vm_vdisks executing
2023-02-16 04:32:31,534|DEBUG|31806|45545|rest.py|173|response body: [{"capacity":7742,"controller_id":"SCSI controller 0","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"0","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":7742},{"capacity":122880,"controller_id":"SCSI controller 1","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"1","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01_1.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":122880},{"capacity":4096,"controller_id":"NVME controller 0","controller_type":"VirtualNVMEController","disk_addr":{"bus":null,"controller":null,"target":"0","unit":"0"},"name":"single_node_cluster1-01_2.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":4096},{"capacity":69632,"controller_id":"SCSI controller 2","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"2","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01_3.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":69632},{"capacity":51200,"controller_id":"SCSI controller 3","controller_type":"VirtualLsiLogicSASController","disk_addr":{"bus":"3","controller":"7","target":"0","unit":"0"},"name":"single_node_cluster1-01_4.vmdk","pool":"sdot-b200-008-hdd","total_diskspace":51200}]
2023-02-16 04:32:31,538|DEBUG|31806|45545|logger.py|132|node.py|5319:get_ontap_disk_names| Disks to find: ['single_node_cluster1-01.vmdk', 'single_node_cluster1-01_1.vmdk', 'single_node_cluster1-01_2.vmdk', 'single_node_cluster1-01_3.vmdk', 'single_node_cluster1-01_4.vmdk']
2023-02-16 04:32:31,604|DEBUG|31806|45545|logger.py|132|ZAPI.py|241:invoke|
API: "storage-disk-get-iter"
User: "admin"
Request:
<?xml version="1.0" ?>
<!DOCTYPE netapp
  SYSTEM 'file:/etc/netapp_filer.dtd'>
<netapp nmsdk_language="Python" nmsdk_platform="Debian GNU/Linux 10
 \l unknown" nmsdk_version="5.4" version="1.40" xmlns="http://www.netapp.com/filer/admin">
        <storage-disk-get-iter/>
</netapp>

Response:
<?xml version="1.0" ?>
<!DOCTYPE netapp
  SYSTEM 'file:/etc/netapp_gx.dtd'>
<netapp version="1.230" xmlns="http://www.netapp.com/filer/admin">


        <results status="passed">
                <attributes-list>
                        <storage-disk-info>
                                <disk-inventory-info>
                                        <bytes-per-sector>512</bytes-per-sector>
                                        <capacity-sectors>142606328</capacity-sectors>
                                        <checksum-compatibility>advanced_zoned</checksum-compatibility>
                                        <disk-class>solid-state</disk-class>
                                        <disk-cluster-name>NET-1.2</disk-cluster-name>
                                        <disk-type>SSD</disk-type>
                                        <disk-uid>4E455441:50502020:302D3273:2F304463:2F322D76:66517835:666B5A32:00000000:00000000:00000000</disk-uid>
                                        <firmware-revision>0001</firmware-revision>
                                        <grown-defect-list-count>0</grown-defect-list-count>
                                        <health-monitor-time-interval>0</health-monitor-time-interval>
                                        <import-in-progress>false</import-in-progress>
                                        <is-dynamically-qualified>false</is-dynamically-qualified>
                                        <is-foreign>false</is-foreign>
                                        <is-multidisk-carrier>false</is-multidisk-carrier>
                                        <is-shared>false</is-shared>
                                        <location>single_node_cluster1-01</location>
                                        <location-id>2443885357</location-id>
                                        <model>PHA-DISK</model>
                                        <reservation-key>0x0</reservation-key>
                                        <reservation-type>none</reservation-type>
                                        <right-size-sectors>142593536</right-size-sectors>
                                        <serial-number>0-2s/0Dc/2-vfQx5fkZ2</serial-number>
                                        <vendor>NETAPP</vendor>
                                        <virtual-machine-disk-info>
                                                <vmhost-error>Failed to connnect to the vSphere server. Reason: Either the server hostname or IP address is not set, or there are network issues. Correct the vSphere credentials with the &quot;system node virtual-machine hypervisor modify-credentials&quot; command.</vmhost-error>
                                        </virtual-machine-disk-info>
                                        <vmdisk-target-address>Scsi bus=2, target=0, lun=0</vmdisk-target-address>
                                </disk-inventory-info>
                                <disk-metrocluster-info>
                                        <is-local-attach>true</is-local-attach>
                                </disk-metrocluster-info>
                                <disk-name>NET-1.2</disk-name>
                                <disk-ownership-info>
                                        <disk-uid>4E455441:50502020:302D3273:2F304463:2F322D76:66517835:666B5A32:00000000:00000000:00000000</disk-uid>
                                        <home-node-id>2443885357</home-node-id>
                                        <home-node-name>single_node_cluster1-01</home-node-name>
                                        <is-failed>false</is-failed>
                                        <owner-node-id>2443885357</owner-node-id>
                                        <owner-node-name>single_node_cluster1-01</owner-node-name>
                                        <pool>0</pool>
                                        <reserved-by-node-id>0</reserved-by-node-id>
                                </disk-ownership-info>
                                <disk-paths>
                                        <disk-path-info>
                                                <array-name>NETAPP_PHA_1</array-name>
                                                <disk-name>single_node_cluster1-01:0b.2</disk-name>
                                                <disk-port>A</disk-port>
                                                <disk-port-name>SA:A</disk-port-name>
                                                <disk-uid>4E455441:50502020:302D3273:2F304463:2F322D76:66517835:666B5A32:00000000:00000000:00000000</disk-uid>
                                                <initiator-io-kbps>198</initiator-io-kbps>
                                                <initiator-iops>3</initiator-iops>
                                                <initiator-lun-in-use-count>0</initiator-lun-in-use-count>
                                                <initiator-port>0b</initiator-port>
                                                <initiator-port-speed>-</initiator-port-speed>
                                                <initiator-side-switch-port>N/A</initiator-side-switch-port>
                                                <lun-io-kbps>0</lun-io-kbps>
                                                <lun-iops>0</lun-iops>
                                                <lun-number>0</lun-number>
                                                <lun-path-use-state>INU</lun-path-use-state>
                                                <node>single_node_cluster1-01</node>
                                                <path-io-kbps>0</path-io-kbps>
                                                <path-iops>0</path-iops>
                                                <path-link-errors>0</path-link-errors>
                                                <path-lun-in-use-count>0</path-lun-in-use-count>
                                                <path-quality>0</path-quality>
                                                <preferred-target-port>false</preferred-target-port>
                                                <target-io-kbps>198</target-io-kbps>
                                                <target-iops>3</target-iops>
                                                <target-iqn>N/A</target-iqn>
                                                <target-lun-in-use-count>0</target-lun-in-use-count>
                                                <target-port-access-state>AO</target-port-access-state>
                                                <target-side-switch-port>N/A</target-side-switch-port>
                                                <target-wwnn>53059d50444f5476</target-wwnn>
                                                <target-wwpn>53059d50444f5476</target-wwpn>
                                                <tpgn>89</tpgn>
                                                <vmdisk-device-id>2</vmdisk-device-id>
                                        </disk-path-info>
                                </disk-paths>
                                <disk-raid-info>
                                        <active-node-name>single_node_cluster1-01</active-node-name>
                                        <container-type>spare</container-type>
                                        <disk-shared-info>
                                                <is-sparecore>false</is-sparecore>
                                        </disk-shared-info>
                                        <disk-spare-info>
                                                <is-media-scrubbing>true</is-media-scrubbing>
                                                <is-offline>false</is-offline>
                                                <is-sparecore>false</is-sparecore>
                                                <is-zeroed>false</is-zeroed>
                                                <is-zeroing>false</is-zeroing>
                                        </disk-spare-info>
                                        <disk-uid>4E455441:50502020:302D3273:2F304463:2F322D76:66517835:666B5A32:00000000:00000000:00000000</disk-uid>
                                        <effective-disk-type>SSD</effective-disk-type>
                                        <physical-blocks>17825791</physical-blocks>
                                        <position>present</position>
                                        <spare-pool>spare</spare-pool>
                                        <standard-disk-type>ssd</standard-disk-type>
                                        <used-blocks>17824192</used-blocks>
                                </disk-raid-info>
                                <disk-stats-info>
                                        <average-latency>2443885357</average-latency>
                                        <bytes-per-sector>512</bytes-per-sector>
                                        <disk-io-kbps>0</disk-io-kbps>
                                        <disk-iops>0</disk-iops>
                                        <disk-uid>4E455441:50502020:302D3273:2F304463:2F322D76:66517835:666B5A32:00000000:00000000:00000000</disk-uid>
                                        <path-error-count>0</path-error-count>
                                        <power-on-time-interval>0</power-on-time-interval>
                                        <sectors-read>0</sectors-read>
                                        <sectors-written>0</sectors-written>
                                </disk-stats-info>
                                <disk-uid>4E455441:50502020:302D3273:2F304463:2F322D76:66517835:666B5A32:00000000:00000000:00000000</disk-uid>
                        </storage-disk-info>
                        <storage-disk-info>
                                <disk-inventory-info>
                                        <bytes-per-sector>512</bytes-per-sector>
                                        <capacity-sectors>104857592</capacity-sectors>
                                        <checksum-compatibility>advanced_zoned</checksum-compatibility>
                                        <disk-class>solid-state</disk-class>
                                        <disk-cluster-name>NET-1.1</disk-cluster-name>
                                        <disk-type>SSD</disk-type>
                                        <disk-uid>4E455441:50502020:302D3273:2F304463:51387830:356D344E:7A564A33:00000000:00000000:00000000</disk-uid>
                                        <firmware-revision>0001</firmware-revision>
                                        <grown-defect-list-count>0</grown-defect-list-count>
                                        <health-monitor-time-interval>0</health-monitor-time-interval>
                                        <import-in-progress>false</import-in-progress>
                                        <is-dynamically-qualified>false</is-dynamically-qualified>
                                        <is-foreign>false</is-foreign>
                                        <is-multidisk-carrier>false</is-multidisk-carrier>
                                        <is-shared>false</is-shared>
